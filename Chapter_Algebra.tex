%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Calcul Algébrique}\label{calculus}
	\lettrine[lines=4]{\color{BrickRed}D}ans la section d'Arithmétique de ce site, nous avons beaucoup écrit sur différents théorèmes utilisant les nombres abstraits afin de généraliser l'étendue de la validité de ces derniers. Nous avons cependant peu abordé la façon dont nous devions manipuler ces nombres abstraits. C'est ce que nous allons voir maintenant.
	
	Comme vous le savez peut-être déjà, le nombre peut être envisagé en faisant abstraction de la nature des objets qui constituent le groupement qu'il caractérise et ainsi qu'à la façon de codifier (chiffre arabe, romain, ou autre système...). Nous disons alors que le nombre est un "\NewTerm{nombre abstrait}\index{nombre abstrait}" et lorsque nous manipulons ces types d'objets nous disons que nous faisons du "calcul 	algébrique" ou encore du "\NewTerm{calcul littéral}".
	
	\textbf{Définition (\#\mydef):} Le "\NewTerm{calcul littéral}\index{calcul litt\'eral}" consiste à calculer avec des variables (c'est-à-dire avec des lettres) comme on le ferait avec des nombres.
	
	Pour les mathématiciens il n'est souvent pas avantageux de travailler avec des valeurs numériques (1,2,3...) car ils représentent uniquement des cas particuliers. Ce que cherchent les physiciens, ingénieurs ainsi que les mathématiciens, ce sont des relations applicables universellement dans un cadre le plus général possible.
	
	Ces nombres abstraits appelés aujourd'hui communément "\NewTerm{variable}\index{variables}" sont très souvent représentés par l'alphabet latin (pour lequel les premières lettres de l'alphabet latin $a, b, c, \ldots$ désignent souvent les nombres connus, et les dernières $x, y, z, \ldots$ les nombres inconnus), l'alphabet grec (aussi beaucoup utilisé pour représenter des opérateurs mathématiques plus ou moins complexes) et l'alphabet hébraïque (dans une moindre mesure).
	
	Bien que ces symboles puissent représenter n'importe quel nombre, il en existe cependant quelques-uns aussi bien en physique ou en mathématique qui peuvent représenter des constantes dites "\NewTerm{Universelles}\index{constantes universelles}" telles que la vitesse de la lumière $c$, la constante gravitationnelle $G$, la valeur de $\pi$, le nombre d'Euler $e$, etc.

	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Il semblerait que les lettres pour représenter les nombres ont été employées pour la première fois par Viète au milieu du 16ème siècle.
	\end{tcolorbox}	

	Une variable est donc susceptible de prendre des valeurs numériques différentes. L'ensemble de ces valeurs peut varier suivant le caractère du problème considéré.
	
	Rappels (nous avions déjà défini cela dans le chapitre traitant des Nombres dans la section d'Arithmétique):

	\begin{enumerate}
		\item[R1.] Nous appelons "\NewTerm{domaine de définition}\index{domaine de d\'efinition}" d'une variable, l'ensemble des valeurs numériques qu'elle est susceptible de prendre entre deux bornes, ou dans un ensemble (tel que $\mathbb{N}, \mathbb{R},\mathbb{R}^+,$ etc.).
	
	Soit $a$ et $b$ deux nombres tel que $a<b$. Alors:
		
		\item[R2.] Nous appelons "\NewTerm{intervalle fermé d'extrémités $a$ et $b$}\index{intervalle ferm\'e}", l'ensemble de tous les nombres $x$ compris entre ces deux valeurs et nous le désignons de la façon suivante:
		
		
		\item[R3.] Nous appelons "\NewTerm{intervalle ouvert d'extrémités $a$ et $b$}\index{intervalle ouvert}", l'ensemble de tous les nombres $x$ compris entre ces deux valeurs non comprises et nous le désignons de la façon suivante:
		
		
		\item[R4.] Nous appelons "\NewTerm{intervalle fermé à gauche, ouvert à droite}\index{intervalle semi-ouvert}" la relation suivante:
		
		
		\item[R5.] Nous appelons "\NewTerm{intervalle ouvert à gauche, fermé à droite}" la relation suivante:
		
	\end{enumerate}

	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Si la variable $x$ peut prendre toutes les valeurs négatives et positives possibles nous écrirons dès lors: $\left] -\infty,+\infty \right[$ où le symbole "$\infty$" signifie "infini". Evidemment il peut y avoir des combinaisons d'intervalles ouvert et infini à droite, fermé et limité à gauche et réciproquement.
	\end{tcolorbox}	

	\textbf{Définition (\#\mydef):} Nous appelons "\NewTerm{voisinage de $a$}\index{voisinage (analyse fonctionnelle)}", tout intervalle ouvert de $\mathbb{R}$ contenant $a$ (c'est un concept simple que nous reprendrons pour définir ce qu'est une fonction continue). Ainsi:
	
	est un voisinage de $a$.

	\subsection{Équations et inéquations}
	L'algèbre élémentaire consiste à partir des définitions de l'addition, soustraction, multiplication, et puissance et de leurs propriétés (associativité, distributivité, commutativité, élément neutre, inverse,...) - ce qui constitue selon l'ensemble sur lequel nous travaillons un corps ou un groupe commutatif abélien ou non (\SeeChapter{cf. chapitre Théorie des Ensembles \pageref{structures}}) - à manipuler selon un but fixé des "\NewTerm{équations algébriques}\index{\'equations alg\'ebriques}" mettant en relation des variables et constantes.

	Nous allons définir de suite après ce qu'est une équation et une inéquation mais nous souhaitons d'abord définir certaines de leurs propriétés:

%	Soit $A$ et $B$ deux polynômes (ou monômes) quelconques - voir définitions un peu plus loin - les expressions:
	
	Vérifient les propriétés suivantes :
	\begin{enumerate}
		\item[P1.] Nous pouvons toujours ajouter ou ôter aux deux membres d'une inéquation ou équation un même polynôme en obtenant une inéquation ou équation équivalente (c'est à dire avec les mêmes solutions ou réductions). Nous disons alors que l'égalité ou l'inégalité restent "vraies" par l'opération d'addition ou de soustraction membre à membre.
		
		\item[P2.] Si nous multiplions ou si nous divisons les deux membres d'une équation ou inéquation par un même nombre positif nous obtenons également une inéquation ou équation équivalente (nous avons déjà vu cela). Nous disons alors que l'égalité ou l'inégalité reste "vraie" par l'opération de multiplication ou division membre à membre.
		
		\item[P3.] Si nous multiplions ou si nous divisons les deux membres d'une inéquation par un même nombre négatif et si nous inversons le sens de l'inégalité, nous obtenons alors une inéquation ou équation équivalente.
	\end{enumerate}
	
	\subsubsection{Équations}
	\textbf{Définition (\#\mydef):} Une "\NewTerm{équation}\index{\'equation}" est une relation d'égalité entre des valeurs toutes abstraites (autrement dit: deux expressions algébriques) ou non toutes abstraites (dès lors nous parlons d'équations à une inconnue, deux inconnues, trois inconnues, ... ) reliées entre elles par des opérateurs divers.

	La maîtrise parfaite de l'algèbre élémentaire est fondamentale en physique-mathématique et dans l'industrie!!! Comme il existe une infinité de types d'équations, nous ne les présenterons pas ici. C'est le rôle de l'enseignant/formateur dans les classes d'entraîner le cerveau de son auditoire pendant plusieurs années (2 à 3 ans en moyenne) à résoudre énormément de configurations différentes d'équations algébriques (exposées sous forme de problèmes de tous les jours, géométriques ou purement mathématiques) et ce afin que les élèves manipulent ces dernières sans erreurs en suivant un raisonnement logique et rigoureux (ce n'est qu'en forgeant que l'on devient forgeron...)!!!

	En d'autres termes: Un professeur/formateur et un établissement ad hoc sont irremplaçables pour
acquérir un savoir et avoir un retour d'expérience!!!

	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
Nous avons tenté, ci-dessous, de faire une généralisation simpliste des règles de base de l'algèbre élémentaire. Cette généralisation sera d'autant plus simple à comprendre que le lecteur aura l'habitude de manipuler des quantités abstraites.
	\end{tcolorbox}	

	Ainsi, soit $a, b, c, d, e, ..., x, y$ des nombres abstraits pouvant prendre n'importe quelle valeur numérique (nous restons dans le cadre des nombres classiques scolaires et industriels...).

	Soit $\Xi$ la lettre majuscule grecque se prononçant "Xi") représentant un ou plusieurs nombres abstraits (variables) opérants entre eux d'une façon quelconque tel que nous ayons des monômes (un seul nombre abstrait) ou polynômes (poly = plusieurs) algébriques différents distinguables ou non (nous faisons donc ici une sorte d'abstrait de l'abstraction ou si vous préférez une variable de plusieurs variables).

	Propriétés (il s'agit en fait plus d'exemples que de propriétés...):
	\begin{enumerate}
		\item[P1.] Nous aurons toujours $\Xi=\Xi$ si et seulement si le terme $\Xi$ à gauche de l'égalité est le même terme $\Xi$  que celui qui est à droite de l'égalité. Si cette condition est satisfaite nous avons alors :
		
		Sinon:
		
		où nous excluons donc les cas où tous les $\Xi$ sont identiques entre eux (sinon nous revenons à P1).
		
		\item[P2.] Nous avons, si $\Xi\neq 0$:
		
		qui vérifie la symbolique de l'équation $\Xi=\Xi$ dans le cas seulement où les éléments sont identiques entre eux (nous excluons bien évidemment le cas avec dénominateur nul).
	
		\item[P3.] \label{power rules calculations}Si tous les $\Xi$ sont strictement identiques, alors:
				
		Sinon nous avons:
		
		qui ne peut s'écrire sous forme condensée simple. Il peut aussi arriver que:
		
		avec le $\Xi$ à droite de l'égalité identique à aucun, un ou encore plusieurs $\Xi$ du membre gauche de l'égalité.

		\item[P4.] Nous pouvons avoir:
		
		sans que nécessairement les exposants du numérateur ou dénominateur soient égaux (nous excluons le dénominateur nul).

		Sinon nous pouvons avoir:
		
		Mais n'oubliez pas (\SeeChapter{voir section Opérateurs page \pageref {division}}) que dans le cas général où le numérateur et le dénominateur ne sont pas égaux que:
		
		
		\item[P5.] On a si tous les dénominateurs sont strictement égaux :
		
		mais il n'est cependant pas impossible d'avoir :
		
		avec le $\Xi$ à droite de l'égalité identique à un ou plusieurs membres de gauche de l'égalité ou encore il est tout à fait possible d'avoir :
		

		\item[P6.] Laissons $\star$ représenter exclusivement le symbole d'addition ou de soustraction, nous avons alors (à une variation de signe donné):
		
		si tous les $\Xi$ sont identiques les uns par rapport aux autres ou si la combinaison d'un nombre indéterminé de $\Xi$ est égal aux $\Xi$ sur la droite de l'égalité.

		Sinon on aura (c'est-à-dire que le résultat donnera n'importe quel monôme ou polynôme indéterminé) :
		

		\item[P7.] On a si tous les $\Xi$ (les bases) levés à la puissance sont strictement identiques :
		
	\end{enumerate}
	si et seulement si les bases $\Xi$ sont égales (ou pourraient être décomposables pour être égales) et les puissances $\Xi$ ne sont pas nécessairement égales.

	A partir de la connaissance des exemples de ces $7$ règles/propriétés de base, nous pouvons résoudre, simplifier ou montrer qu'une équation simple a des solutions ou non relativement à un problème ou un énoncé donné.

	Ainsi, étant donné un opérande ou une séquence d'opérations quelconques sur une ou plusieurs abstractions d'abstraits et parmi toutes, une (ou plusieurs) dont les valeurs numériques est ou sont inconnues (les autres sont connues), nous pourrions alors être en mesure de prouver qu'une relation (énoncé) comme :
	
	a des solutions existantes ou non (c'est-à-dire : est Vrai ou Faux).
	
	Dans le cas d'une équation avec valeur absolue (\SeeChapter{voir section Opérateurs page \pageref{absolute value}}) du type :
	
	avec le second membre étant strictement positif (sinon les relations précédentes seraient un non-sens) cela équivaut bien sûr à partir de la définition de la valeur absolue à écrire :
	

	\begin{tcolorbox}[title=Remarques,colframe=black,arc=10pt]
	\textbf{R1.} La présence de la valeur absolue dans une équation algébrique dans laquelle on cherche des solutions double souvent le nombre de solutions.\\
	
	\textbf{R2.} Une équation est nommée "\NewTerm{\'equation conditionnelle}\index{\'equation conditionnelle}" lorsqu'il y a des nombres dans l'ensemble de définition qui ne sont pas des solutions (ce qui est en fait le cas le plus courant). Inversement, si un nombre quelconque de l'ensemble de définition est la solution de l'équation, alors l'équation est nommée "\NewTerm{équation d'identité}\index{\'equation d'identit\'e}".
	\end{tcolorbox}	
	
	Nous devons parfois résoudre un "\NewTerm{système d'équations}\index{syst\'eme d'\'equations}" (voir par exemple les systèmes d'équations linéaires à la page \pageref{linear systems of equations}).
	\begin{itemize}
		\item Qu'est-ce que c'est ? : C'est un ensemble d'au moins deux équations à résoudre (la résolution n'est pas toujours équivalente à celle consistant à simplifier une expression!).

		\item Quelle est la spécificité d'un tel système ? : Toutes les solutions du système sont l'intersection de toutes les solutions des équations à résoudre (pour des exemples détaillés voir les sections Algèbre Linéaire et Méthodes Numériques). 

		\item Quelle sont les applications pratiques ? : C'est sans quasiment infini (voir les différents chapitres du livre), ces systèmes résolvent des problèmes impliquant des applications des mathématiques à d'autres domaines (finance, ingénierie, recherche opérationnelle, etc.).
	\end{itemize}
	En raison de la variété illimitée des applications, il est difficile d'établir des règles générales précises pour les solutions et les procédures existantes à suivre ne seront certainement utiles que si le problème peut être formulé en équations et les procédures suivantes peuvent aider un peu au moins à éviter certaines erreurs :
	\begin{enumerate}
		\item Si nous avons un l'énoncé d'un problème déjà sous forme écrite, nous le relisons plusieurs fois attentivement et nous considérons les faits donnés et la quantité d'inconnues à trouver et leur domaine de définition (résumer l'énoncé sur une feuille de papier ou ailleurs est souvent utile pour les gros problèmes!).
		
		\item Sélectionnez les lettres qui représentent les quantités inconnues. C'est une des étapes décisives dans la recherche de solutions. Les phrases contenant des mots tels que : trouver, quoi, comment, où, quand devraient vous aider à identifier la quantité inconnue.
		
		\item Faites un dessin (dans votre tête ou sur papier) avec des légendes. C'est à coup sûr possible la plupart du temps uniquement avec des problèmes ayant $1$, $2$ ou $3$ inconnues.
		
		\item Énumérez les faits connus et les relations concernant les quantités inconnues. Une relation peut être décrite par une équation dans laquelle apparaissent dans un ou les deux côtés du signe égale des déclarations (affirmations) écrites avec des phrases normales.
		
		\item Après l'étape précédente, écrivez une ou plusieurs équations qui décrivent avec précision ce qui a été dit avec des phrases.
		
		\item Résoudre l'équation ou le système d'équations formulés dans les étapes précédentes en utilisant les multiples techniques heuristiques existantes.
		
		\item Vérifier les solutions obtenues à l'étape précédente en se référant à l'énoncé initial du problème (vérifier que la solution est cohérente avec les conditions de l'énoncé).
	\end{enumerate}
	Certaines méthodes de résolutions de systèmes d'équations sont traitées en détail dans la section de Méthodes Numériques et également dans la section Algèbre Linéaire (vous y comprendrez donc mieux la procédure ci-dessus).
	 
	 \subsubsection{Inéquations}
	 Précédemment nous avons vu qu'une équation était composée d'une égalité de divers calculs avec des termes différents (avec au moins une "inconnue" ou un "nombre abstrait") et que :
 	\begin{enumerate}
 	  \item "Résoudre une équation" est un processus consistant à calculer la valeur de l'inconnue pour satisfaire l'égalité (quand une solution existe !)
 	  
 	  \item "Simplifier une équation" est le processus consistant à minimiser mathématiquement le nombre de termes (factoriser ou éliminer...)
 	  
 	  \item "Développer une équation" est le processus consistant à distribuer tous les termes.
 	\end{enumerate}
 	Pourquoi faut-il rappeler la définition d'une équation ? Tout simplement parce que pour l'inégalité, c'est presque le même processus intellectuel ! La différence ? Si l'équation est une égalité, l'inégalité est une inégalité (...) : Comme l'équation, l'inégalité est composée de divers calculs avec des termes différents interconnectés par des opérateurs quelconques avec au moins une inconnue.
	
	Principales différences entre les équations d'égalité et d'inégalité :
	
	\begin{enumerate}
		\item Égalité : Symbolisée par le symbole $=$
		
		\item Inégalité : Symbolisée par les relations d'ordre $<, \leq, \geq, >$
	\end{enumerate}
	
	Lorsque nous résolvons une inégalité, notre inconnue peut avoir une plage de valeurs qui satisfont l'inégalité. On dit alors que la solution de l'inégalité est un "\NewTerm{ensemble de valeurs}\index{ensemble de valeurs}". C'est la différence fondamentale entre l'égalité (\underline{plusieurs} solutions) et l'inégalité (\underline{plage} de solutions) !
	
	Faisons un rafraichissement sur les signes que l'on peut rencontrer dans une inégalité :
	\begin{itemize}
		\item $<$: Doit être lu "\NewTerm{strictement inférieur à}\index{symbole!strictement inf\'erieur \`a}" ou "\NewTerm{strictement plus petit que}\index{symbole!strictement plus petit que}". Dans ce cas, la valeur numérique cible n'est pas incluse dans la plage et nous pouvons alors représenter la plage (intervalle) avec un crochet gauche ouvert $] ...$ ou un crochet droit ouvert $... [$ à côté de la valeur cible.
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Exemple:}\\\\
		Écrire $x<5$ signifie $x \in ]-\infty,5[$ et $x<-5$ signifie $x \in ]-\infty,-5[$ .
		\end{tcolorbox}
		
		\item $>$: Doit être lu "\NewTerm{strictement sup\'erieur \`a}\index{symbole!strictement plus grand que}" ou "\NewTerm{strictement plus grand que}\index{symbole!strictement plus grand que}". Dans ce cas, la valeur cible numérique n'est pas non plus incluse dans la plage (intervalle) et nous pouvons alors représenter la plage avec un crochet gauche ouvert $] ...$ ou un crochet droit $... [$ à côté de la valeur cible.
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemple:}\\\\
		Écrire $x>5$ signifie $x \in ]5,+\infty[$ et $x>-5$ signifie $x \in ]-5,+\infty[$ .
		\end{tcolorbox}
		
		\item $\leq$: Doit être lu "\NewTerm{inférieur ou égal à}\index{symbole!inf\'erieur ou \'egal \`a}" ou "\NewTerm{plus petit ou égal à}\index{symbole!plus petit ou \'egal \`a}". Dans ce cas, la valeur cible numérique est dans la plage (intervalle) et nous pouvons alors représenter la plage avec un crochet gauche fermé $[...$ ou crochet droit $...]$.
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemple:}\\\\
		Écrire $x\leq 5$ signifie $x \in ]-\infty,5]$ et $x<-5$ signifie $x \in ]-\infty,-5]$ .
		\end{tcolorbox}
		
		\item $\geq$: Doit être lu "\NewTerm{supérieur ou égal à}\index{symbol!sup\'erieur ou \'egal \`a}" ou "\NewTerm{plus grand ou égal à}\index{symbole!plus grand ou \'egal \`a}". Dans ce cas, la valeur cible numérique est dans la plage (intervalle) et nous pouvons alors représenter la plage avec un crochet gauche fermé $[...$ ou crochet droit $...]$.
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemple:}\\\\
		Écrire $x\geq 5$ signifie $x \in [5,+\infty[$ et $x>-5$ signifie $x \in [-5,+\infty[$.
		\end{tcolorbox}
	
	\end{itemize}
	L'objectif des inégalités est la plupart du temps (hors but esthétique) d'avoir au moins une valeur numérique qui définit le domaine de solution (de tous les termes abstraits - variables - de l'inégalité) qui satisfait l'inégalité.
	
	Il existe de nombreuses façons de représenter les domaines de définition des variables qui satisfont une inégalité. Nous allons voir à travers de petits exemples quelles sont quelques-unes des possibilités les plus courantes :
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Exemple:}\\
	Soit une inégalité linéaire (du premier degré) sur $x$ avec une seule inconnue à laquelle on impose une contrainte particulière arbitraire par exemple (bien entendu, l'expression peut contenir plus de termes...) :
	
	où nous imaginons que cette inéquation résulte déjà d'une simplification de termes qui étaient superflus. Résoudre l'inégalité revient à rechercher les valeurs de $x$ inférieures à $2$. Bien sûr, il n'y a pas qu'une solution dans $\mathbb{R}$ mais un ensemble (intervalle) de solutions et c'est le principe des inégalités !\\
	
	Pour résoudre l'inégalité, on observe d'abord le type d'inégalité imposé ("strict" ou "égal"). Ensuite, dans les classes de lycées (et pas seulement parfois...) on représente l'ensemble $\mathbb{R}$ traditionnellement par un tableau tel que :
	\begin{table}[H]
		\centering
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|r|}
		\hline 
		{\cellcolor{black!30}$-\infty$} & {\cellcolor{black!30}$0$} & {\cellcolor{black!30}$+\infty$}\\ 
		\hline 
		............... & .......|....... & ............... \\  
		\hline 
		\end{tabular} 
		\caption{Tableau de résolution des inégalités}
	\end{table}
	Nous savons intuitivement que la solution de notre inégalité comprend toutes les valeurs inférieures à $2$ ($2$ étant lui-même exclu des solutions) jusqu'à $-\infty$. Ensuite, nous écrivons cet intervalle ou domaine comme suit:
	
	On peut alors représenter sous forme de tableau l'ensemble des solutions (cela aide à comprendre et prépare l'étudiant à la résolution de systèmes d'équations et d'inéquations et à l'étude des variations de fonctions). Pour cela, on reprend le gabarit du tableau précédent et on y place notre valeur cible (on n'en a qu'une dans cet exemple particulier mais parfois il peut y en avoir plusieurs car il y a une singularité ou des racines pour certaines valeurs du domaine de définition), soit la valeur $2$ :
	\begin{table}[H]
		\begin{center}
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|r|}
		\hline 
		{\cellcolor{black!30}$-\infty$} & {\cellcolor{black!30}$0$} & {\cellcolor{black!30}$2$} & {\cellcolor{black!30}$+\infty$}\\ 
		\hline 
		............... & .......|....... & .......[....... & ............... \\  
		\hline 
		\end{tabular} 
		\end{center}
		\caption{Table augmentée pour la résolution des inégalités}
	\end{table}
	et enfin, on délimite avec de la couleur (...) l'ensemble des solutions de $-\infty$ à $+2$ exclus :
	\begin{table}[H]
		\begin{center}
		\definecolor{gris}{gray}{0.85}
		\begin{tabular}{|l|c|c|r|}
		\hline 
		{\cellcolor{black!30}$-\infty$} & {\cellcolor{black!30}$0$} & {\cellcolor{black!30}$2$} & {\cellcolor{black!30}$+\infty$}\\ 
		\hline 
		{\cellcolor{green!30}...............} & {\cellcolor{green!30}.......|.......}  & {\cellcolor{green!30}.......[.......} & ............... \\  
		\hline 
		\end{tabular} 
		\end{center}
		\caption{Tableau en surbrillance pour la résolution des inégalités}
	\end{table}
	A la valeur $2$, on n'oublie pas de marquer le signe $....[$ pour montrer que cette valeur est exclue des solutions. Et c'est tout, et le concept peut être extrapolé à des inégalités beaucoup plus complexes!
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remarques,colframe=black,arc=10pt]
	\textbf{R1.} Parfois au lieu de représenter des tableaux comme nous l'avons fait plus haut, certains professeurs (c'est un choix tout à fait artistique) demandent à leurs élèves d'ombrer les cases du tableau et de dessiner des petits cercles à l'intérieur..., ou ils utilisent aussi de petites flèches, ou dessinent le graphique de l'inégalité (cette dernière méthode est certes esthétique mais prend du temps dans des cas complexes mais un exemple est donné plus bas...).\\
	
	\textbf{R2.} Dans le cadre des inégalités de degré supérieur à $1$, il faut (voir plus loin ce que cela veut dire exactement) d'abord aussi déterminer les racines de l'inégalité qui déterminent les intervalles puis par tâtonnements, déterminer quels intervalles sont à rejeter ou à conserver.
	\end{tcolorbox}
	On peut aussi (tout comme les équations) parfois avoir à résoudre un "système d'inéquations". Qu'est-ce que c'est ? : C'est un ensemble d'au moins deux inégalités à résoudre. La particularité du système ? : L'ensemble des solutions du système est l'intersection de toutes les solutions de chaque inégalité.
	
	Par exemple le système suivant de trois inégalités :
	
	Autrement dit, la méthode est la même que la précédente, à la différence près que notre tableau (représentant les domaines de solutions) comportera une ligne supplémentaire par inéquation supplémentaire dans le système plus une ligne de synthèse qui est la projection des domaines de solutions possibles du système.
	
	Ainsi, un système à $n$ inéquations aura un tableau récapitulatif à $n+1$ lignes.
	
	Mathématiquement, les domaines (car il peut y en avoir plusieurs qui sont disjoints) peuvent s'écrire comme un ensemble de domaines:
	
	Les systèmes d'inéquations sont très fréquents dans beaucoup de problèmes de la mathématique, physique, économétrie, etc... Il est donc important de s'entraîner à les résoudre pendant vos études avec l'aide de votre professeur..
	
	Par exemple, voici une représentation possible du domaine de solution du système d'inéquations précédent :
	\begin{center}
	\begin{tikzpicture}

    \draw[gray!50, thin, step=0.5] (-1,-3) grid (5,4);
    \draw[very thick,->] (-1,0) -- (5.2,0) node[right] {$x_1$};
    \draw[very thick,->] (0,-3) -- (0,4.2) node[above] {$x_2$};

    \foreach \x in {-1,...,5} \draw (\x,0.05) -- (\x,-0.05) node[below] {\tiny\x};
    \foreach \y in {-3,...,4} \draw (-0.05,\y) -- (0.05,\y) node[right] {\tiny\y};

    \fill[blue!50!cyan,opacity=0.3] (8/3,1/3) -- (1,2) -- (13/3,11/3) -- cycle;

    \draw (-1,4) -- node[below,sloped] {\tiny$x_1+x_2\geq3$} (5,-2);
    \draw (1,-3) -- (3,1) -- node[below left,sloped] {\tiny$2x_1-x_2\leq5$} (4.5,4);
    \draw (-1,1) -- node[above,sloped] {\tiny$-x_1+2x_2\leq3$} (5,4);

	\end{tikzpicture}
	\end{center}
	
	\subsection{Identités remarquables}\label{calculus remarkable identities}
	Les identités remarquables sont des sortes de relations magiques, qui nous servent le plus souvent pour la factorisation ou la résolution d'équations algébriques.

	Rappelons certaines notions qui ont déjà été vues dans le chapitre de théorie des ensembles de la section d'arithmétique (nous supposons le concept d'élément neutre connu puisque déjà défini):
	\begin{itemize}
		\item Commutativité:
		
		
		\item Associativité:
		
		
		\item Distributivité:
		
	\end{itemize}
	Les mêmes observations sont valables avec l'opération de soustraction bien évidemment dans les domaines de définition adéquats.
	
	\pagebreak
	Nous pouvons vérifier avec des valeurs numériques (en remplaçant chaque nombre abstrait par un nombre choisi au hasard), ou par développement (ce serait mieux, ainsi vous êtes sûr d'avoir compris ce dont quoi nous parlions), que les identités algébriques suivantes sont vérifiées (ce sont les plus connues):
	\begin{enumerate}
		\item Identité du second degré:
		
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.5]{img/algebra/binomial_identity.jpg}
			\caption[]{Visuel de l'identité binomiale au carré (source: Wikipedia, Auteur:Drini)}
		\end{figure}
	
		\item Identité du troisième degré:
		
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.5]{img/algebra/binomial_cubic_identity.jpg}
			\caption[]{Visuel de l'identité binomiale au cube (source: Wikipedia, Auteur:Drini)}
		\end{figure}
	\end{enumerate}
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Nous pouvons très bien poser que $(a+(c+d))^2:=(a+b)^2$ où nous avons bien évidemment posé que  $b:=c+d$ (nous faisons un "abstrait d'abstraction" ou plus couramment: un "changement de variable"). Et dès lors:
	
	\end{tcolorbox}
	Nous pouvons remarquer ainsi qu'en toute généralité, pour calculer le développement de $(a+b)^n$,  nous utilisons le développement de $(a+b)^{n-1}$,  c'est-à-dire calculé avec la valeur précédente de $n$.
	
	Nous pouvons également voir une sorte de motif émerger si nous rassemblons tout cela:
	
	Nous remarquons alors les propriétés suivantes pour $a$ et $b$:
	\begin{enumerate}
		\item  Les puissances de $a$ décroissent de $n$ à $0$ ($a^0=1$, sdonc il n'est pas noté dans le dernier terme).
		
		\item  Les puissances de $b$ croissent de $0$ à $n$ ($a^0=1$, , donc il n'est pas noté dans le premier terme).
		
		\item Dans chaque terme, la somme des puissances de $a$ et $b$ est égale à $n$.
		
		\item Les coefficients multiplicateurs devant chaque terme se calculent en faisant la somme des coefficients multiplicateurs de deux termes du développement obtenu avec la valeur précédente de $n$ (voir la figure ci-dessous). 
	\end{enumerate}
	Les "\NewTerm{coefficients binomiaux}\index{coefficients binomiaux}" peuvent alors être obtenus par construction du "\NewTerm{triangle de Pascal}\index{triangle de Pascal}\label{Pascal's triangle}" ci-dessous:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/pascal_triangle.jpg}
		\caption{Triangle de Pascal}
	\end{figure}
	Où chaque élément est donné par (\SeeChapter{voir section Probabilités page \pageref{combinatorial analysis}}):
	
	avec $n,p\in \mathbb{N}^{+}$.
	\begin{theorem}
	Nous pouvons alors démontrer que:\label{binomial theorem}:
	
	ce qui constitue le fameux "\NewTerm{binôme de Newton}\index{bin\-ome de Newton}" (que nous réutiliserons dans de multiples autres chapitres et sections dans ce livre) aussi parfois nommé "\NewTerm{théorème binomial}\index{th\'eor\'eme binomial}".
	
	Plus explicitement\label{binomial coefficient development}:
	
	aussi appelé "\NewTerm{série binomiale}\index{s\'erie binomiale}".
	\end{theorem}
	\begin{dem}
	Cette relation peut être prouvée tout simplement par induction en supposant vraie la relation précédente et en la calculant pour le rang $1$ :
	
	Nous obtenons alors:
	
	La relation est vraie pour le rang $n + 1$, elle est donc vraie pour tout $n$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem} 
	
	En appliquant deux fois la formule binomiale, on peut obtenir pour le plaisir la formule du trinôme suivante :
	
	Concernant les identités remarquables à valeurs négatives, il n'est pas nécessaire de mémoriser l'emplacement du signe "$-$". Il suffit de faire un changement de variable et une fois le développement fait on remplace la variable en arrière (changement de variable inverse).
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemple:}\\
	
	et ainsi de suite pour toute puissance finie $n$.
	\end{tcolorbox}
	On peut bien sûr mélanger les genres (...) comme (exemple particulièrement célèbre) :
	
	et quelques relations pratiques supplémentaires remarquables qui sont souvent utilisées dans les petites classes pour les exercices :
	
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Lorsque, du côté droit (sous une forme numérique simplifiée) l'enseignant demande à ses élèves comme exercice d'obtenir la factorisation de la gauche de l'égalité, il n'y a pas d'autre moyen que de procéder par tests successifs.
	\end{tcolorbox}
	Pour information, nous obtenons également le développement célèbre suivant qui est immédiatement déductible de ce que nous avons vu auparavant :
	
	qui est valable pour toute valeur de $b$ et est nommée le "\NewTerm{développement binomial}\index{d\'eveloppement binomial}\label{binomial expansion}". 
	
	Pour les petits valeurs de $b$, nous pourrions négliger tous les termes impliquant $b^2$ ou une puissance supérieure de $b$, nous donnant la relation d'approximation :
	
	avec $b\ll 1$.
	
	Bien sûr, il y a encore un beaucoup plus grand nombre de relations utiles (dont une partie découle d'une généralisation de celles présentées ci-dessus) que le lecteur découvrira par ses propres raisonnements et en fonction de sa pratique.
	
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Il est bien sûr possible de multiplier des polynômes entre eux et de distribuer les termes multiplicatifs. Inversement, il est souvent demandé aux élèves des petites classes de faire la procédure inverse ("factoriser" ou "décomposer" un polynôme) afin qu'ils s'habituent à la manipulation des identités remarquables. Décomposer en un produit de facteurs est une opération importante en mathématiques, puisqu'il est ainsi possible de réduire l'étude d'expressions compliquées à l'étude de plusieurs expressions plus simples.
	\end{tcolorbox}
	
	\subsection{Polynômes}\label{polynomial}
	\textbf{Définition (version naïve \#\mydef):} Nous appelons "\NewTerm{polynôme algébrique $P(x)$}\index{polyn\-ome alg\'ebrique univari\'e}" une fonction de degré $n\in \mathbb{N}$ qui s'écrit:
	
	ou de façon plus condensée par:
	
	où le "\NewTerm{facteur dominant}\index{facteur dominant}" d'un polynôme est le coefficient du monôme de plus haut degré $a_n$ et le "\NewTerm{terme dominant}\index{terme dominant}" est simplement $a_nx^n$. Si $a_n=1$ est égal à $1$ nous parlons alors de "\NewTerm{polynôme normalisé}\index{polyn\-ome normalis\'e}".
	 
	 Si l'on considère chaque coefficient du polynôme comme la composante d'un vecteur\label{polynomial vector}, alors on peut manipuler les polynômes comme des espaces vectoriels notés $P_n(\mathbb{R})$ et de dimension $n+1$.
	 
	\begin{tcolorbox}[title=Remarques,colframe=black,arc=10pt]
	\textbf{R1.} L'indice $n$ de $P (x)$ est la plupart du temps omis comme explicitement défini dans l'expression du polynôme lui-même.\\
	
	\textbf{R2.} Le lecteur qui a lu la section de Théorie des Ensembles, se rappellera probablement que l'ensemble de tous les polynômes de degré $n$ ou inférieur forme une structure d'espace vectoriel !
	\end{tcolorbox}
	Nous supposerons sans preuve et évident (car nous sommes des ingénieurs ou physiciens mais pas des mathématiciens...) que si nous notons $\text{deg}(P_n(x))=n$ le degré d'un polynôme, et que $P_n(x) $ et $P_m(x)$ sont tous deux des polynômes, que alors :
	
	et:
	
	
	\textbf{Définition (version théorie des ensembles \#\mydef):} Soit $k$ un anneau (\SeeChapter{voir section Théorie des Ensembles page \pageref{ring}}) et $n\in \mathbb{N}^{*}$, "\NewTerm{l'anneau polynomial}\index{anneau polynomial}\label{polynomial ring}" en $n$ indéterminées (ou variables) $k[X_1,...,X_n]$ est construit à partir d'un polynôme élémentaire, nommé le "\NewTerm{monôme}\index{mon\-ome}" de la forme:
	
	où $\lambda \in k$ est le "\NewTerm{coefficient du monôme}\index{coefficient du mon\-ome}", $m_1,m_2,...,m_n$ sont des entiers positifs non nuls et où $X_1 ^{m_1}...X_m^{m_n}$ forment la "\NewTerm{partie littérale du monôme}\index{partie litt\'erale du mon\-ome}". Ainsi, par construction, un polynôme est une somme d'un nombre fini de monômes nommés les "\NewTerm{termes du polynôme}\index{termes d'un polyn\-ome}".
	
	Ainsi, le cas particulier courant utilisé dans les petites classes et présenté au début est $k[X]$, c'est-à-dire l'anneau de polynômes univariés à coefficients en $k$. En effet, la plupart du temps, les ingénieurs et étudiants travaillent avec des "anneaux de polynômes univariés à coefficient en $\mathbb{R}$" et notés $\mathbb{R}[X]$. Tout élément de $k[X]$ s'écrit donc :
	
	avec $a_i\in k$ (la majorité du temps $a_i\in \mathbb{R}$ ), $i=0...n$ et $n\in \mathbb{N}$.
	\begin{tcolorbox}[title=Remarques,colframe=black,arc=10pt]
	\textbf{R1.} Notez que les puissances de $i$ sont toujours positives ou nulles dans $k[X]$!!!\\
	
	\textbf{R2.} Nous disons que deux polynômes sont "similaires" s'ils ont la même partie littérale.
	\end{tcolorbox}
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/polynomials.jpg}
		\caption[Quelques polynômes tracés avec R.3.2.1]{Quelques polynômes tracés avec R.3.2.1 (voir le livre compagnon sur R)}
	\end{figure}
	Le "\NewTerm{comportement limite}\index{comportement limite}" d'une fonction décrit ce qui arrive à la fonction quand $x\rightarrow \pm\infty$. Le degré d'un polynôme et le signe de son coefficient directeur (ie dominant) dictent son comportement limite. En particulier:
	\begin{itemize}
		\item Si le degré d'un polynôme $f(x)$ est pair et que le coefficient directeur est positif, alors $f(x)\rightarrow +\infty$ quand $x\rightarrow \pm\infty$.
		
		\item Si $f(x)$ est un polynôme de degré pair avec un coefficient directeur négatif, alors $f(x)\rightarrow -\infty$ quand $x\rightarrow \pm\infty$.
		
		\item Si $f(x)$ est un polynôme de degré impair avec un coefficient directeur positif, alors $f(x)\rightarrow -\infty$ quand $x\rightarrow +\infty$ et $f(x)\rightarrow +\infty$ quand $x\rightarrow +\infty$.
 
		\item Si $f(x)$ est un polynôme de degré impair avec un coefficient directeur négatif, alors $f(x)\rightarrow +\infty$ quand $x\rightarrow -\infty$ et $f(x)\rightarrow -\infty$ quand $x\rightarrow +\infty$.
	\end{itemize}
	Ces résultats sont résumés dans le tableau ci-dessous :	
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|c|c|}
		\hline
		\rowcolor[HTML]{C0C0C0} 
		\textbf{Degré du polynôme}     & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0}\textbf{Coefficient directeur}}                                                                                                                                                                                                                                      \\ \hline
		                                      & $+$                                                                                                                                               & $-$                                                                                                                                               \\ \hline
		\cellcolor[HTML]{C0C0C0}\textbf{Pair} & $f(x)\rightarrow +\infty$ quand $x\rightarrow \pm\infty$                                                                                            & $f(x)\rightarrow -\infty$ quand $x\rightarrow \pm\infty$                                                                                             \\ \hline
		\cellcolor[HTML]{C0C0C0}\textbf{Impair}  & \begin{tabular}[c]{@{}c@{}}$f(x)\rightarrow -\infty$ quand $x\rightarrow -\infty$\\ $f(x)\rightarrow -\infty$ quand $x\rightarrow -\infty$\end{tabular} & \begin{tabular}[c]{@{}c@{}}$f(x)\rightarrow +\infty$ quand $x\rightarrow -\infty$\\ $f(x)\rightarrow -\infty$ quand $x\rightarrow +\infty$\end{tabular} \\ \hline
		\end{tabular}
		\caption{Comportement limite des polynômes}
	\end{table}
	\textbf{Définition (\#\mydef):} Nous nommons "\NewTerm{point tournant}\index{point tournant}" un point à partir duquel le graphique change de direction de croissant à décroissant ou de décroissant à croissant (ou autrement dit : où les dérivées sont égales à zéro).
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/algebra/turning_points.jpg}
		\caption{Points tournants explicitement mis en évidence}
	\end{figure}
	La fonction $f$ ci-dessus est un polynôme de degré $4$ et a $3$ points tournants. Le nombre maximum de points de retournement d'une fonction polynomiale est toujours un de moins que le degré de la fonction.
	
	\textbf{Définition (\#\mydef):} Nous nommons "\NewTerm{racine}\index{racine d'un polyn\-ome}" ou "\NewTerm{zéro d'un polynôme (univarié)}\index{z\'ero d'un polyn\-ome univari\'e}", les valeurs $x$ telles que "\NewTerm{l'équation polynomiale}\index{\'equation polynomiale}" $P(x)=0$ est satisfaite à la condition qu'au moins un des $a_n$ avec $n>0$ ne soit pas nul.
	
	Si le polynôme est normalisé\footnote{En effet, si vous développez l'expression donnée, vous verrez que le facteur devant le terme d'ordre le plus élevé $x^n$ est $1$.} et admet une ou plusieurs racines $r_n$ nous pouvons alors évidemment le factoriser comme suit (nous le prouverons rigoureusement plus loin) :
	
	de sorte que lorsque $x$ prend la valeur d'une des racines $r_n$, l'expression ci-dessus est zéro. C'est ce que l'on nomme par convention "\NewTerm{factoriser un polynôme}\index{factoriser un polyn\-ome}".
	
	Si le polynôme n'est pas normalisé alors la factorisation sera évidemment donnée par :
	
	Les identités algébriques sont des formes particulières de fonctions polynomiales. En effet, considérons une constante $c$ et une variable $x$ et :
	
	On voit que si nous posons :
	
	nous retombons sur:
	
	\textbf{Définitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] Un polynôme à une indéterminée est nommé "\NewTerm{polynôme univarié}\index{polyn\-ome univari\'e}", un polynôme avec plus d'une indéterminée est nommé "\NewTerm{polynôme multivarié}\index{polyn\-ome multivari\'e}". Un polynôme à deux indéterminées est nommé "\NewTerm{polynôme bivarié}\index{polyn\-ome bivari\'e}".
		
		Un exemple célèbre en mathématiques pures d'un polynôme multivarié donné à plusieurs reprises dans les cours de premier cycle universitaire est (certains lecteurs le reconnaîtront probablement...):
		
	
		\item[D2.] Dans le cas de polynômes à plus d'une indéterminée, un polynôme est dit "\NewTerm{homogène de degré $n$}\index{polyn\-ome homog\'ene de degr\'e $n$} si tous ses termes non nuls sont de degré $n$ (l'exemple juste au-dessus est un tel polynôme !).
	\end{enumerate}
	
	\pagebreak
	\subsubsection{Relations générales entre racines et coefficients}
	Voyons maintenant une relation intéressante entre les racines d'un polynôme et ses coefficients.

	Pour cela, considérons le polynôme suivant :
	
	Et désignons ses racines par $r_1,r_2,\ldots,r_n$ ($n$ racines ne signifie pas nécessairement $n$ racines distinctes...). Nous avons alors :
	
	Nous en déduisons alors en développant la dernière égalité et en identifiant aux coefficients que :
	
	Nous pouvons le réécrire différemment en définissant les nombres $\sigma_1,\sigma_2,\ldots,\sigma_n$ comme suit :
	
	Inversement si les $r_1,r_2,\ldots,r_n$ satisfont le système précédent, alors ce sont les racines d'un polynôme normalisé de la forme :
	
	Remarquons que pour le fameux cas $n=2$ que nous avons trouvé :
	
	
	
	\subsubsection{Multiplication de polynômes}\label{polynomials multiplication}
	Le produit des deux polynômes $P$ (de degré $n$) et $Q$ (de degré $m$) est trivialement donné par :
	
	Il n'y a pas grand chose à dire de plus concernant la multiplication des polynômes pour les besoins de ce livre.
	
	\subsubsection{Division Euclidienne de Polynômes}\label{polynomials division}
	Plaçons nous à présent dans l'anneau $k[X]$. Si $P(x)\in k[X]$, nous notons $\text{deg}(P)$ le degré du polynôme $P(X)$ à coefficients dans un anneau $k$ (les réels ou les complexes... peu importe!)

	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Par convention:
	
	\end{tcolorbox}
	\begin{theorem}
	Soit:
	
	avec $k,m>0$. Alors il existe deux polynômes uniques $q(X),r(X)\in k[X]$ tels que:
	
	et:
	
	où $q(X)$ est le "\NewTerm{polynôme quotient}\index{polyn\-ome quotient}" et $r(X)$ est le "\NewTerm{polynôme résiduel}\index{polyn\-ome r\'esiduel}".
	\end{theorem}
	\begin{dem}
	Si $u (X) = 0$ le résultat est évident. Supposons $u(X)\neq 0$ et prouvons l'existence par récurrence sur le degré $k$ de $u (X)$.
	
	Si $k = 0$ alors $q (X) = 0$ (puisque $m>0$) et donc $r (X) = u (X)$ fera le travail.
	
	Supposons maintenant ce résultat appliquable pour tout $k\leq n$... (cette supposition est totalement gratuite...):
	
	Soit $u(X)$ de degré $k=n+1$. Si $m>n+1$ alors $q (X) = 0$ et $r (X) = u (X)$ peuvent également faire le travail.
	
	Sinon, si $m\leq n+1$ alors en écrivant ($u_{n+1}$ est le $n+1$-ième coefficient du polynôme $u(X)$ et $v_m$ le $m$-ième coefficient de $v(X)$):
	
	on réduit alors $u(X)$ à un polynôme de degré $\leq n$ puisque $v(X)$ est de degré $m$ (et qu'il existe) !
	
	Effectivement le terme:
	
	supprime (au moins) le terme de plus haut degré $u_{n+1}X^{n+1}$.
	
	Par l'hypothèse d'induction, il existe $f(X)$ et $g(X)$ tels que :
	
	avec $\text{deg}(g)<m$. Donc après réorganisation:
	
	et dès lors:
	
	fait le travail!
	
	Donc par récurrence on voit que la division euclidienne existe dans l'anneau polynomial $k[X]$
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Cette preuve nous a permis dans la section de Théorie des Ensembles de montrer que cet anneau est "principal".
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemple:}\\\\
	Nous ne verrons qu'un seul exemple car l'idée est toujours la même. On veut diviser $x^3+x^2$ par $x-1$, nous obtenons:
		
	\end{tcolorbox}
	
	
	\pagebreak
	\subsubsection{Théorème de factorisation des polynômes}\label{factorization theorem}
	Nous allons maintenant prouver un théorème important qui est en fait illustré à l'origine (entre autres) par les identités remarquables que nous avons vues ci-dessus :
	
	\begin{theorem}
	Si un polynôme $P(X)\in k[\mathbb{K}]$ avec des coefficients dansn $k$ de degré $n\geq 1$ a une racine $x=r$ dans l'anneau $k$, alors on peut factoriser $P(x)$ par $(x - r)$ tel que :
	
	où $Q$ est un polynôme de degré $n-1$ (et peut donc être un simple monôme).
	
	Autrement dit, "\NewTerm{factoriser un polynôme}\index{factorisation d'un polyn\-ome}", c'est l'écrire comme un produit de monômes (dans le cas général : de polynômes). Lorsqu'elle n'est pas seulement appliquée aux polynômes ou aussi simplement aux nombres, la factorisation est une opération qui transforme une somme en un produit!
	\end{theorem}
	\begin{dem}
	L'idée est d'effectuer la division euclidienne de $P(x)$ par $(x-r)$. D'après le théorème précédent, il existe un couple $(Q, R)$ de polynômes tels que :
	
	et d'après le résultat du théorème précédent sur la division euclidienne :
	
	Mais $\text{deg}(x-r)=1$, donc $\text{deg}(R)=0$ (ou $-\infty$ par convention). $R(x)$ est donc une fonction polynomiale constante. De plus, par hypothèse, $r$ est une racine de $P(x)$. On a donc :
	
	Donc $R(r)=0$. Dès lors $R(x)$ est le polynôme zéro et le théorème est pratiquement prouvé. Il reste à prouver que $\text{deg}(Q)=n-1$, ce qui est une conséquence immédiate de la relation :
	
	Ainsi:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	A partir de cette propriété de factoriser un polynôme, nommé parfois "\NewTerm{théorème de factorisation}\index{théorème de factorisation}", on peut donner un avant-goût d'un théorème beaucoup plus important :
	\begin{theorem}
	Montrons que si on a une fonction polynomiale $P(X)\in k[X]$ de degré $n\in \mathbb{N}$ avec des coefficients en $k$, alors elle a au plus un nombre fini $ n$ de racines (certaines pouvant être confondues) dans $k$.
	\end{theorem}

	\begin{dem}
	Premièrement, parce que $P(x)$ a un degré (ordre), $P(x)$ n'est pas une fonction polynomiale nulle. Alors, argumentons par l'absurde :
	
	Si la fonction $P(x)$ a $n$ racines avec $p>n$ (plus de racines que de degrés...), en notant ces racines $r_1,...,r_p$, on a, par le théorème de factorisation précédent (appliqué $p$ fois):
	
	où $Q$ est un polynôme de degré :
	
	Maintenant, puisque par définition un polynôme est un polynôme si et seulement si son degré (ordre) appartient à $\mathbb{N}$, le polynôme $Q$ doit être le polynôme nul tel que :
	
	Il s'ensuit que :
	
	Ceci contredit l'hypothèse initiale selon laquelle $P$ n'est pas le polynôme zéro, d'où :
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\subsubsection{Équation diophantienne}\label{diophantine equation}
	Si l'on généralise la notion de polynôme univarié à plusieurs variables telles que :
	
	alors on nomme "\NewTerm{équation diophantienne}\index{\'equation diophantienne}" une équation de la forme :
	
	où $P$ est un polynôme à coefficients entiers (ou rationnels) dont on cherche les radicaux (racines) strictement dans $\mathbb{N}$ ou $\mathbb{Q}$. Les équations diophantiennes conventionnelles sont par exemple :
	\begin{itemize}
		\item Les équations diophantiennes linéaires :
		
		
		\item Les triplets pythagoriciens :
			
	\end{itemize}
	Pour la preuve générale de cette dernière, le lecteur devra attendre un peu le temps que les rédacteurs du présent livre aient le temps de comprendre la preuve traditionnelle avant de pouvoir la simplifier (...).
	
	\subsubsection{Polynômes et équations univarié du premier ordre}
	Étant donné la fonction linéaire :
	
	Si $a\neq 0$ alors la première l'équation :
	
	a une racine analytique simple donnée trivialement par :
	
	tel que $P_1(r)=0$.
	
	Si $b=0$ ce polynôme est nommé uen "\NewTerm{fonction affine}\index{fonction affine}".
	
	\begin{tcolorbox}[title=Remarques,colframe=black,arc=10pt]
	\textbf{R1.} Si les coefficients du polynôme univarié de degré $1$ sont tous tels que $a,b\in \mathbb{R}$ alors la racine appartient aussi à $\mathbb{R}$.\\
	
	\textbf{R2.} Si un des coefficients du polynôme univarié de degré $1$ appartient à $\mathbb{C}$ alors la racine appartient aussi à $\mathbb{C}$.\\
	
	\textbf{R3.} Si les deux coefficients du polynôme univarié de degré $1$ appartiennent à $\mathbb{C}$ alors la racine appartient aussi à $\mathbb{C}$ ou $\mathbb{R}$.\\
	
	\textbf{R4.} On dit que deux équations polynomiales sont "\NewTerm{équivalents}\index{polyn\-omes \'equivalents}" si elles admettent les mêmes solutions.
	\end{tcolorbox}
	Voici également quelques propriétés pour les polynômes univariés du premier ordre que nous donnons sans démonstration car elles nous semblent très très intuitives (sauf sur demande de lecteurs) :
	\begin{enumerate}
		\item[P1.] Si nous sommons (ou respectivement soustrayons) un même nombre à chaque membre de l'équatoin (à gauche du signe "$=$", et aussi à droite), on obtient une équation qui a les mêmes solutions que l'équation d'origine (et ce quelle que soit est son degré).
	
		\item[P2.] Si nous multiplions (ou divisons respectivement) chaque membre d'une équation (à gauche du signe "$=$", et aussi à droite) par un même nombre non nul, nous obtenons une équation qui a les mêmes solutions que l'équation d'origine (et ce quel que soit son degré).
	\end{enumerate}
	La méthode doit être suffisamment générale pour être appliquée à toutes les équations du même genre, être construite sur les quatre opérations arithmétique de base (addition, soustraction, multiplication et division) et l'extraction de racines. On peut trouver les solutions (racines), d'une équation grâce à ses coefficients, en n'utilisant que les opérations précédentes (c'est-à-dire sous une "\NewTerm{forme analytique}\index{forme analytique}"), on dit alors que le l'équation peut être résolue par "\NewTerm{radicaux}\index{radicaux}".
	
	\paragraph{Résolution par référence circulaire}\mbox{}\\\\
	De nombreux cadres dans les petites, moyennes et entreprises internationales ne savent plus résoudre les équations linéaires du premier degré (et au-delà aussi bien sûr...). Je pense donc qu'il serait intéressant pour l'étudiant de voir comment procède la majorité des gens qui ne sont pas ou plus capables de résoudre de tels problèmes de niveau secondaire (c'est particulièrement intéressant quand vous savez qu'ils sont payés assez cher pour résoudre de tels "problèmes compliqués" selon employeur). La façon la plus simple de voir cela c'est d'apprendre comment je les voirs utiliser le tableur Microsoft Excel. En effet, considérons l'exemple suivant:

	Une entreprise réalise un chiffre d'affaires (revenu) de $1'500$.- et doit payer $1'000$.- de frais fixes. Il a $40\%$ d'impôt payable après déduction. L'entreprise souhaite faire un don (déductible d'impôt) tel que le montant de ce don représente $10\%$ d'impôt après déduction.

	Ceci s'écrit mathématiquement simplement si on note $R$ le chiffre d'affaires, $C$ les frais fixes, $D$ le don, $t$ le taux d'imposition :
	
	Après quelques petits réarrangements triviaux, nous obtenons :
	
	Cela correspond bien à la taxe de $10\%$ après déduction puisque :
	
	Et voici comment les (mauvais) cadres utilisent un tableur comme Microsoft Excel et « résolvent » cela (vous pouvez me faire confiance, je vois vraiment des gens  faire cela en entreprise parce qu'ils ne sont plus capables de résoudre des problèmes niveau lycée sur papier). Ils écrivent d'abord dans le tableur (qui détecte automatiquement l'incohérence de l'utilisateur en l'indiquant par une flèche bleue pour mettre en évidence la "référence circulaire"):
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/circular_reference_microsoft_excel_01.jpg}
		\caption{Résoudre une équation simple par référence circulaire dans Microsoft Excel}
	\end{figure}
	Validé cela donne:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/circular_reference_microsoft_excel_02.jpg}
	\end{figure}
	Pour résoudre cette équation, il faut activer dans les options du logiciel l'autorisation d'utiliser l'itération (la position de cette option dépend de la version du tableur).
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/algebra/circular_reference_microsoft_excel_03.jpg}
		\caption[]{Option de résolution d'équations par récurrenc dans Microsoft Excel 14.0.7183}
	\end{figure}
	Ce qui donne alors:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/circular_reference_microsoft_excel_04.jpg}
	\end{figure}
	Donc, comme on peut le voir, c'est une façon assez peu efficace de résoudre un simple problème de niveau lycée (regrettable pour des gestionnaires ou analystes  qui ont un MBA...!).

	\subsubsection{Polynômes et équations univarisé d'ordre deux}\label{second order polynomials}
	Soit le polynôme univarié suivant à coefficient dans $\mathbb{R}$ (trinôme du second degré) :
	
	Si nous représentons ce polynôme univarié sur le plan, cela nous donne :
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/polynomial_orientation.jpg}
		\caption{Représentation des polynômes en fonction du signe du terme de degré $2$}
	\end{figure}
	Si on prend la dérivée de cette fonction (\SeeChapter{voir section Calcul Différentiel et Intégral page \pageref{differential calculus}}) et qu'on cherche en quel point la dérivée est égale à zéro, on trouvera toujours l'optimum sur le point d'inflexion (\SeeChapter{voir section Calcul Différentiel et Intégral page \pageref{inflection point}}) de la parabole (qui correspond aussi à son axe de symétrie) :
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/polynomial_optimum.jpg}
		\caption{Point d'inflexion de la tangente}
	\end{figure}
	Si $a\neq 0$, nous avons alors:
	
	Nous avons alors une "\NewTerm{racine double}\index{racine double}\label{double root}" (ou "\NewTerm{racine de multiplicité $2$}") que l'on note par :
	
	tel que $P(r_{1,2})=0$ et nous définissons un nouveau terme nommé parfois le "\NewTerm{déterminant du polynôme}" et le plus souvent le "\NewTerm{discriminant du polynôme}\index{discriminant du polyn\-ome}\label{discriminant}":
		
	Finalement\label{second order polynomial roots}:
	
	Si le polynôme univarié du second ordre en $x$ a deux racines, nous pouvons alors le factoriser sous une forme irréductible (suivant le théorème de factorisation démontré précédemment) sous la forme suivante :
	
	Nous prouvons aussi facilement à partir de l'expression de la racine en faisant de l'algèbre simple les "\NewTerm{relations de Viète}\index{relations de Vi\'ete}\label{vieta relations}" (sur demande des lecteurs nous pouvons détailler les développements nécessaires) :
	
	Selon le signe de $2a$ et celui du discriminant $\Delta$, nous avons:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/polynomial_second_order_signature.jpg}
	\end{figure}
	Dès lors:
	\begin{itemize}
		\item Si $\Delta<0$ notre polynôme n'a pas de racines réelles et ne peut pas être factorisé dans une multiplication de monômes avec des facteurs réels ($
\mathbb{R}$) mais avec un facteur complexe ($\mathbb{C}$). Par conséquent (il est recommandé d'avoir lu d'abord la partie sur les Nombres Complexes à la page 	\pageref{complex numbers} dans la section Nombres de ce livre) :
		
		et nous savons qu'on peut écrire n'importe quel nombre complexe sous une forme condensée (formule d'Euler) et comme les racines complexes d'un polynôme du second degré sont conjuguées (on connaît déjà ce jargon) nous avons:
		
		où (rappel) $r$ est le module des racines complexes (module qui est égal pour les deux) et $\varphi$ l'argument des racines complexes (égal en valeur absolue).
		
		\item Si $\Delta=0$ l'équation polynomiale a alors une seule solution qui est évidemment :
		
		
		\item Si $\Delta>0$ l'équation polynomiale a alors deux solutions définies par les relations générales que nous avons déjà données plus haut :
		
	\end{itemize}
	A propos du cas complexe, prenons comme exemple le polynôme quadratique suivant :
	
	qui n'admet que deux racines complexes que sont $\mathrm{i}$ et $-\mathrm{i}$. Dans le plan réel ce polynôme sera représenté avec Maple 4.00b par :
	
	\texttt{>plot(x\string^2+1,x=-5..5);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/polynomial_complex_solutions_in_real_plane.jpg}
		\caption[]{Exemple de tracé d'un polynôme de degré $2$ qui n'admet que des solutions complexes}
	\end{figure}
	où l'on voit bien qu'il n'y a pas de solutions (zéros) réelles. En nous plaçant dans les complexe $\mathbb{C}$, nous avons par contre :
	
	\texttt{>plot3d(abs((re+I*im)\string^2+1),re=-2..2,im=0..2,view=[-2..2,-2..2,0..2],\\
	orientation=[-130,70],contours=50,style=PATCHCONTOUR,axes=frame,\\
	grid=[100,100],numpoints=10000);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/polynomial_complex_solutions_in_complex_plane.jpg}
		\caption[]{Le même polynôme mais en jouant avec la représentation complexe}
	\end{figure}
	où les deux zéros sont visibles sur l'axe imaginaire à $-1$ et $+1$. Évidemment quand c'est la première fois que l'on voit une fonction représentée sur une figure prenant en compte les valeurs complexes on essaie de trouver où est la parabole correspondante au cas purement réel. Pour ce faire, on coupe simplement la surface ci-dessus en deux sur l'axe imaginaire et on obtient alors :
	
	\texttt{>plot3d(abs((re+I*im)\string^2+1),re=-2..2,im=0..2,view=[-2..2,-2..2,0..2],\\
	orientation=[-130,70],contours=50,style=PATCHCONTOUR,axes=frame,\\
	grid=[100,100],numpoints=10000);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/polynomial_complex_solutions_in_complex_plane_cutted.jpg}
		\caption[]{A little zoom on the same polynomial}
	\end{figure}
	où l'on retrouve clairement notre parabole visible sur la surface découpée. On peut donc se demander si les nombres complexes sont une extension naturelle de notre espace conventionnel au-delà de nos sens physiques et de nos appareils de mesure communs... ?!

	\paragraph{Équations Irrationnelles}\mbox{}\\\\
	Le praticien doit toujours prendre l'habitude de vérifier la solution dans l'équation d'origine pour être sûr de la validation du domaine de définition de la fonction. En effet, il existe des solutions à la résolution d'équations qui ne satisfont pas l'équation d'origine et c'est ce que l'on nomme des  "\NewTerm{solutions étrangères}\index{solution polynomiale \'etrang\'ere}" et c'est typiquement le cas des équations irrationnelles.

	\textbf{Définition  (\#\mydef):} Une "\NewTerm{\'equation irrationnelle}\index{\'equation irrationnelle}" est une équation où l'inconnue est sous un radical (c'est-à-dire dans le cas typique : sous une racine carrée).
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemple:}\\\\
	Considérons l'équation suivante :
	
	Pour le résoudre d'abord, nous pouvons redistribuer :
	
	Nous mettons le tout au carré:
	
	Nous simplifions un peu :
	
	et encore:
	
	Nous mettons au carré une deuxième fois:
	
	Nous simplifions une dernière fois:
	
	Nous obtenons deux solutions triviales qui sont $ x_1 = -2 $ et $ x_2 = 2 $. Mais seule la solution $ x_1 = -2 $ satisfait l'équation proposée. En effet, si on met la première solution dans l'équation d'origine nous obtenons :
	
	Mais si nous mettons la deuxième solution:
		
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Nombre d'or}\mbox{}\\\\
	Il existe un polynôme de degré deux dont la solution est fameuse de par le monde. Ce nombre est appelé la "\NewTerm{nombre d'or}\index{nombre d'or}\label{golden ratio}" ou "\NewTerm{divine proportion}\index{divine proportion}" et se retrouve en architecture, esthétique ou encore en phyllotaxie (c'est-à-dire dans la disposition des feuilles autour de la tige des plantes).
	
	Ce nombre vaut: 
	
	Le nombre d'or est aussi un nombre algébrique (tout nombre complexe qui est une racine d'un polynôme univarié non nul avec des coefficients rationnels) et même un entier algébrique (nombre complexe qui est une racine d'un polynôme monique avec des coefficients dans $\mathbb{Z}$) car c'est la solution de :
	
	Historiquement, le lecteur doit savoir que deux quantités $a$ et $b$ sont dites « dans le ratio du nombre d'or $\varphi$ » si leur rapport est le même que le rapport de leur somme à la plus grande des deux quantités :
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/algebra/golden_ratio.jpg}
		\caption{Illustration du nombre d'or}
	\end{figure}
	Une méthode pour trouver la valeur de $\varphi$ consiste à commencer par la fraction de gauche. En simplifiant la fraction et en substituant dans $b/a = 1/\varphi$ :
	
	Dès lors:
	
	Multipliant par $\varphi$ donne:
	
	qui peut être réarrangé en :
	
	En utilisant la formule quadratique, deux solutions sont obtenues :
	
	Parce que $\varphi$ est le rapport entre des quantités positives, $\varphi$ est nécessairement positif :
	
	Introduisons maintenant une relation utile pour lorsque nous étudierons la fonction génératrice ordinaire (voir page \pageref{ordinary generating function}) relativement à la suite de Fibonacci. Rappelons que nous sommes partis de :
	
	we then have:
	
	C'est pourquoi $-1/\varphi$ est nommé le "\NewTerm{conjugué du nombre d'or}" !
	
	
	\subsubsection{Polynômes et équations univarisé d'ordre trois}
	Même s'il est rare de résoudre une telle chose en physique théorique ou en ingénierie, résoudre un polynôme univarié de degré $3$ est assez récréatif et montre un bon exemple d'un raisonnement mathématique déjà mature (nous avons ces développements grâce à Scipione del Ferro et Jérôme Cardan mathématiciens du XVIe siècle...).
	
	Étant donnée l'équation :
	
	avec les coefficients tous dans $\mathbb{R}$ (pour commencer...). Dans un premier temps, le lecteur pourra voir que les raisonnements que nous avons appliqués pour les polynômes de degrés inférieurs à $3$ coincent rapidement (excepté pour des cas particuliers simplistes bien sûr...).

	Nous allons contourner le problème par des changements de variables subtils mais tout à fait justifiés.
	
	Ainsi, rien ne nous empêche de poser que:
	
	et qu'en divisant le polynôme de degré $3$ par $a$ d'écrire:
	
	En regroupant les termes de même ordre:
	
	et posons (rien, mais alors absolument rien ne nous l'interdit):
	
	où (1) est connu si et seulement si $X$ est connu et où $p, q$ sont de toute façon connus.

	Le polynôme\footnote{La première fois que j'ai dû résoudre un tel polynôme, c'était pour calculer la nutation d'un gyroscope et la deuxième fois c'était pour le calcul de l'horizon d'un Trou Noir basé sur la métrique de Schwarzschild avec une constante cosmologique qui était en unités naturelles : $\dfrac{\Lambda}{3}r^3-r-2M=0$}:
	
	étant de degré impair, il admet - comme permet de le constater tout tracé visuel d'un tel polynôme à coefficients réels - au moins une racine réelle, appelée "\NewTerm{racine certaine}\index{racine certaine}" (vérifiez! Vous verrez bien par une représentation graphique d'un polynôme de degré impair que cela est trivial).
	
	Le membre de gauche de cette équation est un "\NewTerm{trinôme monique}\index{trin\-ome monique}" appelé aussi la "\NewTerm{cubique déprimée}\index{cubique d\'eprim\'ee}", car le terme quadratique a un coefficient $0$.
	
	Maintenant, nous faisons un autre changement de variable (nous en avons tout à fait le droit) subtil: 
	
	en imposant la condition que $u,v$ doivent être tels que $3uv=-p$ (rien ne nous empêche d'imposer une telle contrainte) et nous avons alors:
	
	Dès lors nous avons:
	
	Nous pouvons très bien faire une analogie entre les deux relations (1') et (2') et les relations de Viète que nous avions obtenues pour le polynôme de degré 2 qui rappelons-le étaient:
	
	excepté que nous avons maintenant (nous adoptons une autre notation pour ces racines intermédiaires):
	
	à la différence que nous avons maintenant (nous adoptons une autre notation pour ces racines intermédiaires): 
	
	dont $z_1,z_2$sont les racines.
	Cette dernière équation a pour discriminant:
	
	Prenons maintenant le cas par cas:
	\begin{enumerate}
		\item Si , l'$\Delta >0$ en $Z$ admet deux solutions $z_1,z_2$ dont la somme va nous donner indirectement la valeur de $X$ puisque par définition $X=u+v$ et $z_1=u^3$ et $z_2=v^3$. Nous voyons que nous avons tous les ingrédients pour trouver la première racine de l'équation initiale qui sera la racine certaine (ou "\NewTerm{zéro certain}\index{z\'ero certain}"). Ainsi:
		
		comme $\Delta>0$ et que les racines supérieures sont cubiques nous avons nécessairement $X_1\in \mathbb{R}$ si tous les coefficients de l'équation originale sont bien dans $\mathbb{R}$.
		
		\item Si $\Delta=0$, nous le savons, l'équation en $Z$ admet une racine double et puisque le discriminant comporte une puissance carrée de $q$ cela signifie nécessairement que $p$ est négatif.

		Le polynôme $P$ admet donc lui aussi une racine double et de même pour l'équation d'origine. Nous avons vu par ailleurs que pour un polynôme du second degré si le discriminant est nul les racines sont:
		
		alors par analogie:
		
		
		\item Si $\Delta<0$ nous devons à nouveau utiliser les nombres complexes comme nous l'avons fait lors de notre étude du polynôme de degré $2$. Ainsi, nous savons que l'équation en $Z$ admet deux solutions complexes telles que:
		
		et à nouveau comme les racines sont conjuguées nous pouvons écrire sous la forme condensée:
		
		et comme:
		
		nous avons donc:
		
		Comme $u_k,v_k$ sont conjugués, nous avons nécessairement.
	\end{enumerate}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemple:}\\\\
	Considérons l'équation:
	
	Nous avons donc:
	
	et alors:
	
	Nous avons donc:
	
	\end{tcolorbox}
	Les polynômes de degré trois sont donc bien résolubles par radicaux!
	
	Remarquez aussi une autre approche intéressante due à Viète ! On recommence à partir de :
	
	et posons $X = u\cos(\theta)$. L'idée est de choisir $u$ pour faire coïncider l'équation précédente avec l'identité trigonométrique suivante :
	
	Avant de continuer, démontrons cette identité en utilisant les identités trigonométriques prouvées dans la section Trigonométrie:
	
	En fait, en choisissant $u=2\sqrt{-{\dfrac {p}{3}}}$ et en divisant l'équation par $\dfrac{u^{3}}{4}$ nous obtenons:
	
	En comparant avec l'identité ci-dessus, nous obtenons :
	
	et donc les racines de l'équation cubique déprimée sont :
	
	
	\subsubsection{Polynômes et équations univarisé d'ordre quatre}
	L'équation polynomiale univariée à résoudre ici est :
	
	avec $a\neq 0$.
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Nous devons cette méthode de résolution à l'italien Ludovico Ferrari mathématicien italien du 16ème siècle également.
	\end{tcolorbox}
	Quitte à diviser par $a$ nous avons:
	
	Puis, en posant:
	
	l'équation se réduit:
	
	où nous voyons que le coefficient devant le $y^3$ s'annule. Ainsi, tout polynôme du type:
	
	peut être écrit sous la forme suivante:
	
	En posant:
	
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Si $d''=0$, l'équation à résoudre est en réalité une "\NewTerm{équation bicarrée}\index{\'equation bicarr\'ee}". Le changement de variable permet alors de se ramener à une équation polynomiale du deuxième degré (ce que nous savons facilement résoudre).
	\end{tcolorbox}
	Nous introduisons maintenant un paramètre $t$ (que nous choisirons judicieusement par la suite) et nous réécrivons l'équation polynomiale sous la forme suivante:
	
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Si le lecteur développe et distribue tous les termes de la relation précédente il retombera bien évidemment sur $x^4+c''x^2+d''x+e''=0$.
	\end{tcolorbox}
	L'idée sous-jacente est d'essayer de s'assurer que la partie entre crochets de l'expression précédente peut être écrite sous la forme d'un carré :
	
	Car dans ce cas, en utilisant :
	
	Notre équation polynomiale peut alors s'écrire :
	
	et nous n'aurions qu'à résoudre deux équations polynomiales du second degré (ce que nous savons déjà faire).
	
	Mais pour que nous puissions écrire :
	
	l'expression du second degré à gauche de l'égalité ne doit avoir qu'une seule racine. Mais nous avons vu dans notre étude des équations polynomiales du second degré cela signifiait puisque le discriminant est nul :
	
	et que la racine était alors donnée par :
	
	Ce qui correspond dans notre cas à :
	
	et donc que :
	
	avec:
	
	Donc finalement, si $t$ est tel que $4(2t-c'')(t^2-e'')={d''}^2$, alors nous avons :
	
	puisque le théorème fondamental des polynômes nous donne pour un polynôme du deuxième degré n'ayant qu'une seule racine:
	
	Pour conclure, il suffit de voir que trouver un nombre t vérifiant la relation:
	
	est un problème de degré $3$ que nous savons déjà résoudre par la méthode de Cardan.
	
	De telles méthodes générales n'existent plus pour les degrés égaux ou supérieurs à $5$ comme nous le verrons à l'aide de la théorie de Galois (\SeeChapter{voir section Algèbre Ensembliste page \pageref{galois theory}}).
	
	\subsubsection{Polynômes trigonométriques}\label{trigonometric polynomials}
	\textbf{Définition (\#\mydef):} Nous appelons "\NewTerm{polynôme trigonométrique}\index{polynôme trigonométrique}" de degré $N$ toute somme finie du type:
	
	où $c_n\in \mathbb{C}$.

	Un polynôme trigonométrique peut aussi être écrit en utilisant les fonctions trigonométriques usuelles grâce aux transformations suivantes:
	
	Soit en utilisant la formule d'Euler (\SeeChapter{voir section Nombres page \pageref{euler formula}}):
	
	Ce que nous pouvons réécrire aussi sous la forme:
	
	En posant alors:
	
	Il vient:
	
	Nous verrons longuement dans le chapitre des Suites Et Séries comment utiliser ces polynômes dans le cadre de l'étude des séries de Fourier.
	
	\subsubsection{Pôlynomes Cyclotomiques}
	\textbf{Définition (\#\mydef):} Si $n$ est un entier naturel (appartenant donc à $\mathbb{N}$) et $x$ un nombre complexe, nous appelons "\NewTerm{polynôme cyclotomique}\index{polynôme cyclotomique}" ce que nous notons traditionnellement et que nous définissons comme étant le produit de tous les monômes:
	
	où $\alpha$ est une racine primitive $n$-ème de l'unité de $\mathbb{C}$. En d'autres termes:
	
	Pour rappel une racine $n$-ème de l'unité (parfois appelée "\NewTerm{nombre de De Moivre}\index{nombre de De Moivre}") est un nombre complexe dont la puissance $n$-ème vaut $1$.
	
	Ainsi, l'ensemble des racines $n$-èmes de l'unité est l'ensemble:
	
	qui est un groupe cyclique (voir  section de Théorie des Ensebmles et aussi la section d'Algèbre Ensembliste page \pageref{set algebra}).
	
	Nous appelons alors "\NewTerm{racine primitive $n$-ème de l'unité}\index{racine primitive $n$-ème de l'unité}" ou "R.P.N." tout élément de ce groupe l'engendrant.
	
	Les éléments de $G_n$ sont donc du type:
	
	avec $k\in \mathbb{Z}$. Nous écrivons alors l'ensemble des $G_n$ sous la forme:
	
	Un petit exemple de polynôme cyclotomique (plus d'exemples seront donnés plus bas):
	
	avec:
	
	qui sont donc les racines quatrièmes de l'unité (autrement dit chacun de ces nombres mis à la puissance $4$ donne $1$). Elles forment le groupe et celui-ci ne peut-être engendré que par $\mathrm{i}$ et $-\mathrm{i}$ (générateur du groupe selon ce qui a été vu dans le chapitre de Théorie des Ensembles).
	
	Donc un polynôme cyclotomique est le produit de facteurs qui s'écrit:
	
	avec $k\in \{0,\ldots,n-1\}$.
	
	Nous verrons avec les exemples ci-dessous que si $n$ est pair alors :
	
	et si $n$ est impair :
	
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemple:}\\\\
	Pour $n$ jusqu'à $30$, les polynômes cyclotomiques sont :
	
	\end{tcolorbox}
	
	\subsubsection{Polynômes de Legendre}\label{legendre polynomials}
	\textbf{Définition (\#\mydef):} Les polynômes de Legendre sont définis par (il est fortement recommandé de lire les sections de Calcul Différentiel et Intégral page \pageref{differential and integral calculus} et aussi d'Analyse Fonctionnelle \pageref{functional analysis} avant de poursuivre):
	
	où $P_n$ est donc un polynôme de degré $n$. Nous retrouverons ces polynômes dans la résolution d'équations différentielles en physique (propagation de la chaleur, physique quantique, chimie quantique, etc.). Nous retrouvons plus souvent l'écriture équivalente triviale:
	
	Nous nous concentrerons ici uniquement et uniquement sur les propriétés qui sont utilisées actuellement dans les autres sections sur la Physique de ce livre!!!
	
	Démontrons que selon la définition du produit scalaire fonctionnel (voir sections d'Analyse Fonctionelle page \pageref{functional dot product} et Calcul Vectoriel \pageref{dot product}) que les polynômes de Legendre sont orthogonaux. C'est une propriété très importante pour notre étude de la Chimie Quantique (voir page \pageref{quantum chemistry rigid rotator}) plus tard !
	
	\begin{dem}
	Soit $P$ un polynôme de degré $\leq n-1$. Il suffit de montrer que $\langle P_n | P \rangle =0$, c'est-à-dire que $P_n$ est orthogonal à l'espace des polynômes de degré inférieur à $n$. Nous avons en effet:
	
	en intégrant par parties nous obtenons:
	
	
	\begin{tcolorbox}[colback=red!5,borderline={1mm}{2mm}{red!5},arc=0mm,boxrule=0pt]
	\bcbombe Attention!!! Pour le terme nul ci-dessus, seulement le terme $(1-x^2)^n$ y est dérivé. Donc puisque $x$ est au carré, quelque soit la dérivée la valeur sera toujours la même. Ce qui justifie que le terme soit nul.
	\end{tcolorbox}
	
	En continuant de la sorte nous obtenons après $n$ intégrations par parties:
	
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Le terme dérivé est nul puisque le polynôme dérivé est de degré n-1 $n-1$.
	\end{tcolorbox}
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}	
	Voici quelques propriétés utiles pour le chapitre de Chimie Quantique des polynômes de Legendre:
	\begin{enumerate}
		\item[P1.] Nous avons $P_n(1)=1$:
		\begin{dem}
		
		et en utilisant la règle de différenciation de Leibniz pour les produits (\SeeChapter{voir section de Calcul Différentiel et Intégral page \pageref{Leibniz differentiation rule for products}}) nous avons:
		
		Dès lors:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}

		\item[P2.] Nous avons $P_n(-x)=P_n(-x)$ si $n$ est pair:
		\begin{dem}
		Si $n$ est pair:
		
		est une fonction pair et dès lors:
		
		est pair.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}

		\item[P3.] Nous avons $P_n(-x)=-P_n(x)$ si $n$ est impair:
		\begin{dem}
		Si $n$ est impair:
		
		est une fonction impair et dès lors:
		
		est impair.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
	\end{enumerate}
	\begin{theorem}
	Nous allons à présent démontrer la validité de la relation de récurrence suivante pour les $P_n$ (relations que nous utiliserons en physique):
	
	pour $n \geq 1$.
	\end{theorem}
	\begin{dem}
	$xP_n(x)$ est un polynôme de degré $n+1$, il existe dès lors des $a_j\in \mathbb{R}$ tel que ce polynôme peut s'exprimer comme combinaison linéaire de la famille de polynômes constituant la base orthonormale (base qui permet donc d'engendrer $xP_n(x)$):
	
	nous pouvons dès lors écrire:
	
	mais nous choisissons $k\leq n-2$ (parce que est dès lors $xP_k$ est de degré $n-1$):
	
	Dès lors:
	
	c'est-à-dire que nous devons avoir $a_k=0$. Il en suit que:
	
	Par les propriétés des polynômes de Legendre vues précédemment, nous pouvons écrire les égalités:
	
	et:
	
	dès lors:
	
	Le coefficient dominant de  $P_n$est défini (rappelons-le) par le coefficient du monôme du plus grand degré. Ainsi, il est donné par:
	
	Donc:
	
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Le lecteur vérifiera au besoin pour un $n$ donné que: 
	
	\end{tcolorbox}
	La relation:
	
	que nous avons obtenu ci-dessus nous impose que le coefficient dominant du polynôme de la combinaison linéaire soit égal au coefficient dominant du polynôme $xP_n$ (nous avons éliminé le $(-1)^n$ qui se simplifie):
	
	Après simplification, nous obtenons:
	
	et ce qui donne finalement facilement:
	
	La relation:
	
	devient dès lors:
		
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Voici les six premiers polynômes de Legendre:
	
	Les graphiques de ces polynômes (jusqu'à $n = 5$) sont présentés ci-dessous :
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/legendre_polynomials.jpg}
		\caption[Cinq premiers polynômes de Legendre]{Cinq premiers polynômes de Legendre (source: Wikipédia)}
	\end{figure}
	
	\subsubsection{Polynômes de Laguerre}\label{Laguerre polynomials}
	Avant d'aborder les mathématiques des polynômes de Laguerre, le lecteur doit savoir que la grande majorité du texte ci-dessous est un simple copier/coller du contenu de l'excellent manuel \cite{arfken2013mathematical}. Il est d'ailleurs fortement recommandé de lire les sections de Calcul Différentiel et Intégral à la page \pageref{differential and integral calculus} et aussi d'Analyse Fonctionnelle à la page \pageref{analyse fonctionnelle} avant de continuer !
	
	\textbf{Définition (\#\mydef):} "\NewTerm{L'équation différentielle ordinaire de Laguerre}\index{\'equation diff\'erentielle ordinaire de Laguerre}" (qui dérive de l'équation différentielle partielle radiale de Schrödinger pour l'atome d'hydrogène\footnote{Les polynômes de Laguerre apparaissent en mécanique quantique, dans la partie radiale de la solution de l'équation de Schrödinger pour un atome à un électron. Ils décrivent également les fonctions statiques de Wigner des systèmes oscillants en mécanique quantique dans l'espace des phases. Ils entrent en outre dans la mécanique quantique du potentiel de Morse et de l'oscillateur harmonique isotrope 3D.} comme nous le prouverons dans la section de Chimie quantique à la page \pageref{quantum chemistry non-rigid rotator}) est donnée par:
	
	Cette équation n'a de solutions non singulières que si $n$ est un entier non négatif.
	
	Dérivons une solution en utilisant la méthode de Frobenius (\SeeChapter{voir section Séquences et Séries page \pageref{Frobenius method}})! Pour cela nous supposons d'abord que la solution $y$ est donnée sous la forme :
	
	avec ses deux premières dérivées :
	
	et:
	
	En substituant ces expressions dans l'équation différentielle donnée, on obtient :
	
	En multipliant les facteurs par les sommations correspondantes, on obtient :
	
	Maintenant, nous ajustons l'indice des deux premières sommations pour tels que les $x$ aient l'exposant $r+n$ :
	
	Nous évaluons quelques termes pour que toutes les sommations commencent à l'indice $n=0$ et nous obtenons :
	
	En combinant les termes, nous avons :
	
	qui donne, après simplification :
	
	En égalant les coefficients des deux côtés, on obtient :
	
	En divisant la dernière équation par $r+n+1 \neq 0$ et en décalant les indices, on obtient "\NewTerm{l'équation de récurrence}":
	
	Puisque $a_{0} \neq 0$, nous obtenons "\NewTerm{l'équation indicatrice}" de l'équation initiale :
	
	ayant pour racines $r_{1,2}=0$.
	
	La relation de récurrence peut s'écrire :
	
	Dès lors:
	
	Par conséquent, en choisissant $a_{0}=1$, la solution $y=y_{1}$ est égale à :
	
	On voit que cette somme est finie car quelle que soit la valeur de $\lambda$ à un moment donné tout facteur aura $(\lambda-\ldots)$ égal à zéro. Donc, tous les termes s'annuleront. Ainsi la solution $y_1$ a un nombre fini de termes et est un polymonial!
	
	Pour $\lambda=n$, cette solution devient:
	
	On voit que cette solution est polynomiale si $\lambda=n$. Revenons maintenant à :
	
	Cette dernière relation nous amène à définir le polynôme de Laguerre comme :
	
	Cette relation se retrouve souvent dans les livres sous la forme suivante :
	
	On peut alors construire le tableau suivant :
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|}
			\rowcolor[HTML]{C0C0C0}\hline$n$ & \multicolumn{1}{|c|} {$L_{n}(x)$} \\
			\hline 0 & 1 \\
			\hline 1 & $-x+1$ \\
			\hline 2 & $\frac{1}{2}\left(x^{2}-4 x+2\right)$ \\
			\hline 3 & $\frac{1}{6}\left(-x^{3}+9 x^{2}-18 x+6\right)$ \\
			\hline 4 & $\frac{1}{24}\left(x^{4}-16 x^{3}+72 x^{2}-96 x+24\right)$ \\
			\hline 5 & $\frac{1}{120}\left(-x^{5}+25 x^{4}-200 x^{3}+600 x^{2}-600 x+120\right)$ \\
			\hline 6 & $\frac{1}{720}\left(x^{6}-36 x^{5}+450 x^{4}-2400 x^{3}+5400 x^{2}-4320 x+720\right)$ \\
			\hline
			 $\ldots$ & $\ldots$\\
			\hline$n$ & $\frac{1}{n !}\left((-x)^{n}+n^{2}(-x)^{n-1}+\ldots+n(n !)(-x)+n !\right)$ \\
			\hline
		\end{tabular}
		\caption{Liste de quelques polynômes de Laguerre}
	\end{table}
	Et donnons quelques tracés de ces polynômes de Laguerre :
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{img/algebra/laguerre_polynomials.jpg}
		\caption[Tracé des six premiers polynômes de Laguerre]{Tracé des six premiers polynômes de Laguerre (source : Wikipédia)}
	\end{figure}
	Maintenant, nous pouvons facilement prouver que :
	
	nommé "\NewTerm{formule  de Rodrigues'}\index{formule de Rodrigues}" pour les polynômes de Laguerre (il y a différentes manières de l'exprimer et différentes versions également selon le type de polynôme de Laguerre auquel nous avons affaire !).
	
	Si on utilise la représentation explicite des polynômes de Laguerre :
	
	alors le résultat découle facilement de la règle de différenciation de Leibniz pour les produits :
	
	Soit $f(x)=x^{n}$ et $g(x)=e^{-x}$. Alors $f^{(k)}(x)=x^{n-k} n ! /(n-k) !$ et $g^{(n-k)}(x)=(-1)^{n-k} e^{-x}$. Maintenant, si nous  le nettoyons et refaisons la somme dans l'ordre inverse, c'est-à-dire de $k \rightarrow n-k$ nous retombons sur la formule de Rodrigues ci-dessus.
	
	Montrons maintenant que la fonction génératrice fonctionnelle (\SeeChapter{voir section Séquences et Séries page \pageref{functional generating function}}) des polynômes de Laguerre est donnée par :
	
	La preuve courte est donnée par (on utilise simplement la série de Maclaurin de $e^{z}$ où $z=-xt/(1-t)$) :
	
	En différenciant la fonction génératrice ci-dessus par rapport à $x$ et $t$, nous obtenons les deux relations de récurrence pour les polynômes de Laguerre comme suit :
	
	Pour dériver la première relation de récurrence, nous partons de :
	
	On différencie les deux côtés par rapport à $t$ pour obtenir :
	
	En multipliant les deux membres par $(1-t)^{2}$ et en simplifiant, on obtient :
	
	Nous égalisons maintenant les coefficients de $t^{n}$ des deux côtés dans la relation ci-dessus pour obtenir :
	
	Pour obtenir la deuxième relation de récursivité, nous repartons de la fonction génératrice :
	
	Nous différencions maintenant les deux côtés de l'égalité ci-dessus par rapport à $x$ pour obtenir :
	
	en utilisant la fonction génératrice cette dernière peut être réécrite :
	
	Dès lors:
	
	En égalisant les coefficients $t^{n}$ des deux côtés de ce qui précède nous donne :
	
	Dès lors:
	
	et après réarrangement :
	
	De de la première relation de récurrence :
	
	Nous différencions ce qui précède par rapport à $x$ pour obtenir :
	
	En substituant les valeurs de $L_{n-1}^{\prime}(x)$ et $L_{n+1}^{\prime}(x)$ dérivées plus tôt dans la relation précédente, nous obtenons :
	
	et après simplification on obtient la seconde relation de récurrence :
	
	Pour la preuve de l'orthogonalité des polynômes de Laguerre, le lecteur peut se référer à la section Analyse page \pageref{orthogonality of Laguerre polynomial}.
		
	\paragraph{Polynômes de Laguerre associés}\label{Associated Laguerre polynomials}\mbox{}\\\\
	\textbf{Définition (\#\mydef):} Dans de nombreuses applications, notamment en mécanique quantique (encore !), on a besoin des "\NewTerm{polynômes de Laguerre associés}\index{polyn\-omes de Laguerre associ\'es}" aussi nommés "\NewTerm{polynômes de Laguerre généralisés}\index{polyn\-omes de Laguerre g\'en\'eralis\'es}" définis par:
	
	A partir de la forme série de $L_{n}(x)$ nous vérifions que les plus petits polynômes de Laguerre associés sont donnés par :
	
	Dès lors en général:
	
	Les premiers polynômes de Laguerre associés sont :
	\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|}
			\rowcolor[HTML]{C0C0C0}\hline$n$ & \multicolumn{1}{|c|} {$L^k_{n}(x)$} \\
			\hline $L_0^k$ & $1$ \\
			\hline $L_1^k$ & $-x+(\alpha+1)$ \\
			\hline $L_2^k$ & $\frac{x^2}{2}-(\alpha+2)x+\frac{(\alpha+1)(\alpha+2)}{2}$ \\
			\hline $L_3^k$ & $-\frac{x^3}{6}+\frac{(\alpha+3)x^2}{2}-\frac{(\alpha+2)(\alpha+3)x}{2}+\dfrac{(\alpha+1)(\alpha+2)(\alpha+3)}{6}$ \\
			\hline
		\end{tabular}
		\caption{Liste de quelques polynômes de Laguerre associés}
	\end{table}
	Et un tracé de certains d'entre eux:
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{img/algebra/associated_laguerre_polynomials.jpg}
		\caption[Tracés de quelques polynômes de Laguerre associés]{Tracés de quelques polynômes de Laguerre associés (source: Wikipédia)}
	\end{figure}
	Ou en utilisant une relation de récurrence pour tout $k\leq 1$ :
	
	Une fonction génératrice peut être développée en différenciant la fonction génératrice de Laguerre $k$ fois pour donner :
	
	A partir des deux derniers membres de cette équation, en annulant le facteur commun $z^{n}$, nous obtenons:
	
	De là, pour $x=0$, le développement binomial :
	
	nous donne:
	
	Les relations de récurrence peuvent être dérivées de la fonction génératrice ou en différenciant les relations de récurrence polynomiales de Laguerre. Parmi les nombreuses possibilités, nous avons :
	
	Ainsi, nous obtenons en dérivant une fois l'équation différentielle ordinaire de Laguerre :
	
	et finalement en différenciant l'équation différentielle ordinaire de Laguerre $k$ fois :
	
	En ajustant l'indice $n \rightarrow n+k$, nous avons "\NewTerm{l'équation différentielle ordinaire de Laguerre associée}\index{\'equation diff\'erentielle ordinaire de Laguerre associ\'ee}":
	
	Plus connue sous la forme suivante :
	
	Lorsque des polynômes de Laguerre associés apparaissent dans un problème physique, c'est généralement parce que ce problème physique implique l'équation différentielle ci-dessus. L'application la plus importante concerne les états liés de l'atome d'hydrogène, qui en sont dérivés ! La "\NewTerm{représentation de Rodrigues du polynôme de Laguerre associé}\index{Représentation de Rodrigues du polynôme de Laguerre associé}" :
	
	peut être obtenue par:
	
	Les polynômes de Laguerre associés présentent les mêmes propriétés d'orthogonalité que les polynômes de Laguerre !
	
	Dérivons à nouveau un résultat utile pour notre étude de la chimie quantique : la constante de normalisation du polynôme de Laguerre associé !
	
	En utilisant la formule de Rodrigue pour le polynôme de Laguerre associé, nous obtenons :
	
	Par conséquent:
	
	pour $m \leqslant n$ (par induction sur $m$ et intégration par parties). En prenant $m=n$ (rappelons que $L_{n}^{k}(x)$ est un polynôme de degré $n$ avec le coefficient dominant égal à $(-1)^{n} / n !$ ) , on obtient le nécessaire :
	
	
	\paragraph{Fonctions de Laguerre}\label{Laguerre functions}\mbox{}\\\\
	En posant:
	
	$\psi_{n}^{k}(x)$ satisfait l'équation différentielle ordinaire auto-adjointe :
	
	Les $\psi_{n}^{k}(x)$ sont parfois nommés "\NewTerm{fonctions de Laguerre}\index{fonction de Laguerre}".
	
	Une autre forme utile est donnée en définissant (cela correspond à modifier la fonction $\psi_{n}^{k}(x)$ pour éliminer la dérivée première dans l'équation différentielle ordinaire précédente) :
	
	La substitution dans l'équation de Laguerre associée donne (correspondant à l'équation différentielle ordinaire sans dimension du terme radial du rotateur non rigide que nous rencontrons lors de notre étude de l'atome hydrogénoïde dans la section de Chimie Quantique)\label{radial term dimensionless non-rigid rotator}:
	
	Prouvons cette affirmation ! Pour atteindre la dérivée seconde, nous avons besoin de la dérivée première, et utilisons la notation :
	
	où $v=L_{j}^{k}(x)$, car les indices ne changent pas et ne servent qu'à ajouter de l'encombrement, et nous pouvons nous rappeler que la variable indépendante est $x$. La dérivée première est alors :
	
	Similairement nous obtenons:
	
	En substituant la dérivée seconde et la fonction dans l'équation différentielle ordinaire de Laguerre associée :
	
	et en divisant par le facteur commun de $e^{-x / 2} x^{(k+1) / 2}$, les termes restants sont :
	
	qui est l'équation de Laguerre associée. Puisque $v=L_{j}^{k}(x)$, et les $L_{j}^{k}(x)$ sont des solutions de l'équation de Laguerre associée. La dernière ligne ci-dessus équivaut à :
	
	qui est l'équation de Laguerre associée, que nous savons être un énoncé vrai, donc :
	
	en sont des solutions ! La preuve est terminée.
	
	L'intégrale de normalisation correspondante :
	
	est (sans démonstration):
	
	Notons que les $\Phi_{n}^{k}(x)$ ne forment pas un ensemble orthogonal (sauf avec $x^{-1}$ comme fonction de pondération) à cause de $x^{-1}$ dans le terme $(2 n+k+1) / 2 x$.

	\begin{flushright}
	\begin{tabular}{l c}
	\circled{90} & \pbox{20cm}{\score{3}{5} \\ {\tiny 70 votes,  56.29\%}} 
	\end{tabular} 
	\end{flushright}
	
	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Algèbre Ensembliste}\label{set algebra}
	\lettrine[lines=4]{\color{BrickRed}N}ous allons aborder maintenant dans ce livre l'étude des structures ensemblistes de manière très pragmatique (puisque rappelons que ce site est dédié aux ingénieurs). Ainsi, il sera fait usage du minimum de formalisme et seulement les démonstrations des éléments que nous considérons comme absolument essentiels à l'ingénieur seront présentées. Par ailleurs, de nombreuses démonstrations seront faites par l'exemple et nous nous focaliserons en grande partie sur la théorie algébrique des groupes car elle a une place presque prédominante en physique plus que pour les autres structures ensemblistes.

	\subsection{Algèbre et Géométrie Corporelle}
	Les symétries des figures géométriques, des cristaux et de tous les autres objets de la physique macroscopique font l'objet depuis des siècles d'observations et d'études. En termes modernes, les symétries d'un objet donné forment un groupe. 

	Depuis le milieu du 19ème siècle, la théorie des groupes a pris une extension énorme, et ses applications à la mécanique quantique et à la théorie des particules élémentaires se sont développées tout au long du 20ème siècle.
	
	Dans une lettre de 1877 au mathématicien Adolph Mayer, Sophus Lie écrit qu'il a créé la théorie des groupes en janvier 1873. Il s'agit bien sûr des groupes qu'il appelait "\NewTerm{groupes continus}\index{groupes continus}" et qui sont appelés aujourd'hui "\NewTerm{groupes de Lie}\index{groupes de Lie}\label{lie group}". Lie cherchait à étendre l'usage des groupes du domaine des équations algébriques, où Galois les avait introduites, à celui des équations différentielles.
	
	Dès 1871, la notion de générateur infinitésimal d'un groupe à un paramètre de transformations était apparue dans son oeuvre. C'est l'ensemble des générateurs infinitésimaux des sous-groupes à un paramètre d'un groupe continu qui forme ce que nous appelons aujourd'hui une \NewTerm{algèbre de Lie}\index{alg\'ebre de Lie}".
	
	Ce furent Wigner et Weyl qui montrèrent le rôle prééminent de la théorie des groupes, et de leurs représentations en particulier, dans la nouvelle mécanique quantique que développaient Heisenberg et Dirac. L'idée générale de la théorie des représentations est d'essayer d'étudier un groupe en le faisant agir sur un espace vectoriel de manière linéaire: nous essayons ainsi de voir le groupe comme un groupe de matrices (d'où le terme "représentation"). Nous pouvons ainsi, à partir des propriétés relativement bien connues du groupe des automorphismes de l'espace vectoriel (\SeeChapter{voir section Théorie des Ensembles page \pageref{automorphism}}), arriver à déduire quelques propriétés du groupe qui nous intéressent.
	
	Nous pouvons considérer la théorie des représentations de groupes comme une vaste généralisation de l'analyse de Fourier. Son développement est continu et elle a, depuis le milieu du 20ème siècle, des applications innombrables en géométrie différentielle, en théorie ergodique, en théorie des probabilités, en théorie des nombres, dans la théorie des formes automorphes, dans celle des systèmes dynamiques ainsi qu'en physique, chimie, biologie moléculaire et traitement du signal. À l'heure actuelle, des branches entières des mathématiques et de la physique en dépendent.
	
	Avant de commencer, nous renvoyons le lecteur au chapitre traitant de la Théorie Des Ensembles pour qu'il se rappelle de la structure et des propriétés fondamentales qui constituent le groupe et également au chapitre d'Algèbre Linéaire (car nous en utiliserons quelques résultats).
	
	\subsubsection{Groupes Cycliques}\label{cyclic groups}
	Le groupe cyclique (dont la définition a déjà été vue dans le chapitre de Théorie des Ensembles) va nous servir de base dans le cadre de l'étude des groupes finis. Par ailleurs, plutôt que de faire des développements généralisés nous avons préféré prendre des exemples particuliers afin de présenter l'idée de groupe cyclique (approche plus adaptée à l'ingénieur).

	Nous allons donc prendre l'exemple fort sympathique des heures de la montre... avec trois approches différentes qui successivement (!) permettront d'aborder un groupe cyclique simple.
	\begin{enumerate}
		\item Première approche:
		
		Imaginons donc une horloge avec une aiguille qui peut prendre $12$ positions possibles (mais pas de positions intermédiaires). Nous noterons de manière spéciale les $12$ positions possibles: $\overline{0},\overline{1},\overline{2},...,\overline{11}$ (le trait au-dessus des nombres n'est pas innocent!).
		
		Rien ne nous empêche sur l'ensemble de ces positions de définir une addition, par exemple:
		
	  	ce qui est similaire aux résultats que nous obtenons lorsque dans notre quotidien nous faisons des calculs avec notre montre.
	  	
	  	\item Deuxième approche (première extension):
	  	
	  	Si nous observons bien une montre ou une horloge, nous remarquons qu'à chaque fois que nous rajoutons $12$ (ou retirons $12$...) à une valeur des heures de notre montre alors nous tombons sur un ensemble de nombres bien déterminé qui sont aussi dans $\mathbb{Z}$. Ainsi (évidemment dans le cadre d'une montre/horloge seules les premières valeurs positives nous intéressent la plupart du temps mais ici nous faisons des maths alors nous généralisons un peu...):
	  	
		Nous retrouvons ici un concept que nous avions déjà vu dans le chapitre de Théorie Des Nombres. Il s'agit de classes de congruences et l'ensemble de ces classes forme l'ensemble quotient $\mathbb{Z}/12\mathbb{Z}$. Si nous munissons cet ensemble quotient d'une loi d'addition, il est normalement facile d'observer que celle-ci est une loi interne à l'ensemble quotient, qu'elle est associative, qu'il existe un élément neutre et que chaque élément possède un symétrique (inverse).
		
		Ainsi, cet ensemble quotient muni uniquement de la loi d'addition (sinon en ajoutant la multiplication nous pouvons former un anneau) est un groupe commutatif.

		\item Troisième approche (deuxième et dernière extension):
		
		Voyons une troisième et dernière approche qui explique pourquoi le groupe quotient est cyclique. 
		
		Si nous projetons la rotation des aiguilles de notre montre (toutes les rotations dans l'algèbre ensembliste se font traditionnellement dans le sens des aiguilles d'une montre!) dans $\mathbb{C}$ et que nous définissons:
		
		Nous avons alors $x^{12}=x^0=1$ et:
		
		ce qui explique pourquoi le groupe quotient $(\mathbb{Z}/12\mathbb{Z},+)$ est appelé "\NewTerm{groupe cyclique}\index{groupe cyclique}" (par isomorphisme de groupe selon ce qui a été vu dans la section de Théorie des Ensembles). Son isomorphe est noté $C_{12}$ et tous les éléments sont modulus $1$. Il est commun de noter tous les nombres complexes de module $1$ comme suit:
		
		Si nous représentons dans $\mathbb{C}$ l'ensemble isomorphe nous obtenons alors sur le cercle unité un polygone ayant  $n$ sommets comme le montre la figure ci-dessous:
		\begin{figure}[H]
			\centering
			\includegraphics{img/algebra/c_12_cyclic_group.jpg}
			\caption{$C_{12}$ Groupe cyclique}
		\end{figure}
		Par ailleurs, le nombre d'éléments composants $\mathbb{Z}/12\mathbb{Z}$ étant fini, $(\mathbb{Z}/12\mathbb{Z},+)$ est fini. Contrairement au groupe qui $(\mathbb{Z},+)$ est lui un groupe discret infini. 
		
		Ce concept de finitude sera peut-être plus évident avec l'exemple que nous ferons de suite après avec où le lecteur observera que cet ensemble a le même nombre d'éléments que $C_4$.
	\end{enumerate}
	
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Les mathématiciens appellent $C_n$ le "groupe des racines $n$-èmes de l'unité". Une racine $n$-ème de l'unité (parfois appelée "\NewTerm{nombre de De Moivre}\index{nombre de De Moivre}") est donc un nombre complexe dont la puissance $n$-ème vaut $1$. Par ailleurs, pour un entier $n$ donné, toutes les racines $n$-èmes de l'unité sont situées sur le cercle unité et sont les sommets d'un polygone régulier à $n$ côtés ayant un sommet d'affixe $1$.
	\end{tcolorbox}
	
	\textbf{Définition (\#\mydef):} Un "\NewTerm{groupe fini}\index{groupe fini}\label{finite group}" est un groupe mathématique avec un nombre fini d'éléments comme les "groupes de permutations", les "groupes symétriques", les "groupes cycliques", etc.
	
	Ce qui intéresse les physiciens particulièrement dans un premier temps ce sont les représentations des groupes finis (aussi les groupes continus que nous verrons plus loin). Ainsi, la représentative de $\mathbb{Z}/n\mathbb{Z}$ nous est connue puisque la rotation dans le plan complexe est donnée comme nous l'a montrée notre étude des complexes dans le section sur les Nombres:
	
	avec $k\in[0,n-1]$. Cette représentative est un sous-groupe du groupe des rotations$\text{O}(2)$ sur lesquelles nous reviendrons plus loin. Le groupe des rotations du plan étant lui-même un sous-groupe du groupe linéaire $\text{GL}(2)$ (nous en donnerons une définition précise et un exemple plus loin).
	
	Au fait, les mathématiciens sont capables de démontrer que tous les groupes quotients $\mathbb{Z}/n\mathbb{Z}$ sont cycliques à isomorphisme près avec $C_n$ et ils disent alors que est un quotient fini du groupe monogène $\mathbb{Z}$...
	
	Cette approche est par contre peut-être un peu abstraite. Alors, si le lecteur se rappelle de la section de Théorie Des Ensembles nous avons vu une définition bien précise de ce qu'était la cyclicité d'un groupe: Un groupe $G$ est dit cyclique si $G$ est engendré par la puissance d'au moins un de ses éléments appelé "générateur" tel que:
	
	Vérifions que ce soit bien le cas pour le groupe:
	
	qui constitue un cas scolaire.
	
	Nous noterons les éléments qui constituent ce groupe:
	
	Ceci étant fait, il convient de faire attention que dans la définition ensembliste du groupe cyclique nous parlons de "puissance" si la loi interne du groupe est la multiplication mais si la loi interne est l'addition, nous avons alors:
	
	Le premier élément générateur du groupe $G=\left\lbrace \mathbb{Z}/4\mathbb{Z},+ \right\rbrace$ est $1$. Effectivement:
	
	Le deuxième élément générateur du même groupe est $3$:
	
	Par contre, le lecteur pourra vérifier que $2$ n'est pas générateur de ce groupe!
	
	Au fait, en ce qui concerne les groupes $G=\left\lbrace \mathbb{Z}/n\mathbb{Z},+ \right\rbrace$ les mathématiciens arrivent à démontrer que seuls les éléments du groupe qui sont premiers avec $n$ sont générateurs (c'est-à-dire les éléments dont le plus grand commun diviseur est $1$).
	
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Un peu similaire à la façon dont les nombres premiers sont les blocs de construction de base des entiers, il existe un type spécial de groupes finis qui sont les groupes de construction de base de tous les groupes communs finis. Ces types particuliers de groupes sont appelés "groupes simples finis". La classification des groupes finis et simples est sans précédent dans l'histoire des mathématiques, car sa preuve fait $15'000$ pages et a nécessité près de 50 ans pour être achevée et a impliqué quelques centaines de mathématiciens. C'est pourquoi on l'appelle le "théorème énorme".
	\end{tcolorbox}
	
	Ce sera tout pour notre introducton aux groupes cycliques pour ingénieurs. Maintenant passons à une introduction succinte à un autre type de groupes importants.
	
	\subsubsection{Groupes de Transformations}
	Le groupe des rotations est celui qui intéresse le plus les physiciens surtout dans les domaines des matériaux, de la chimie, de la physique quantique et de l'art... Les mathématiciens apprécient eux l'étude des groupes de rotations dans le cadre de la géométrie bien évidemment (mais pas seulement) et les informaticiens tout autant les groupes linéaires. Nous avons d'ailleurs vu un exemple de groupe de rotations juste précédemment.
	
	\textbf{Définition (\#\mydef):} Nous appelons "\NewTerm{groupe linéaire d'ordre $n$}\index{groupe lin\'eaire d'ordre $n$}"" et nous notons $\text{GL}(n)$ les matrices inversibles $n\times n$ ou dites aussi "\NewTerm{matrices régulières}\index{matrices r\'eguli\'eres}" (donc le déterminant est non nul selon ce que nous avons vu dans le chapitre d'Algèbre Linéaire) dont les coefficients sont dans un corps quelconque $\mathbb{K}$: (le corps $\mathbb{R}$ ou le groupe $\mathbb{C}$ la majorité du temps):
	
	Le groupe est ainsi nommé car les colonnes d'une matrice inversible sont linéairement indépendantes.
	
	Nous considérerons comme évident que $\text{GL}(n)$ est un groupe: la multiplication des matrices est associative et chaque matrice de $\text{GL}(n)$ possède un inverse par définition (étant donné que le déterminant est non nul). D'autre part, le produit de deux matrices régulières est encore une matrice régulière.
	
	Un exemple simple et important de groupe linéaire est celui du sous-"\NewTerm{groupe des transformations affines}\index{groupe des transformations affines}" du plan qui est traditionnellement noté (c'est intuitif):
	
	avec  $a,b,c,d,\alpha,\beta\in \mathbb{R},ad-bc\neq 0$ (nous verrons le pourquoi du comment de l'inégalité un peu plus loin).
	
	\textbf{Définition (\#\mydef):} Le "\NewTerm{groupe affine}\index{groupe affine}" ou "\NewTerm{groupe affine général}\index{groupe affine g\'en\'eral}" de tout espace affine $A$ sur un corps $\mathbb{K}$ est le groupe noté $\text{Aff}(A)$ ou $\text{Aff}(n,\mathbb{K})$ de toutes les transformations affines inversibles de l'espace vers lui-même. C'est un "\NewTerm{Groupe de Lie}\index{Groupe de Lie}" si $\mathbb{K}$ est le corps des quaternions réels ou complexes.
	
	Prenons un exemple pratique:
	
	ce qui appliqué à un cercle donnerait:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/affine_group_transformation_example.jpg}
		\caption{Transformations affines sur un cercle}
	\end{figure}
	Cette transformation est une manière de définir les ellipses comme images d'un cercle par une transformation affine.
	
	Les coefficients $\alpha,\beta$ sont sans importance pour la forme de l'image. En fait, ils induisent bien évidemment des translations sur les figures. Nous pouvons donc nous en passer si nous cherchons seulement à la déformer.
	
	Ainsi, il nous reste:
	
	ce qui peut s'écrire sous forme matricielle:
	
	La transformation se réduit donc à la matrice:
	
	et comme nous l'avons vu en algèbre linéaire, la multiplication matricielle est associative mais n'est pas commutative, donc la transformation linéaire ne l'est pas non plus. 
	
	L'élément neutre est la matrice:
	
	et l'inverse de $F$ est:
	
	et comme nous avons imposé $ad-bc\neq 0$ tout élément y possède donc un inverse. Ainsi, le groupe linéaire affine est non commutatif et... forme bien un groupe....
	
	Comme nous allons le voir, tous les groupes de Lie "classiques" sont des sous-groupes de $\text{GL}(n)$.
	
	\textbf{Définition (\#\mydef):} Nous appelons "\NewTerm{groupe spécial linéaire d'ordre $n$}\index{groupe sp\'ecial lin\'eaire d'ordre $n$}" et nous notons $\text{SL}(n)$ les matrices inversibles (carrées $n\times n$) dont les coefficients sont dans un corps quelconque et dont le déterminant est égal à l'unité:
	
	Il s'agit évidemment d'un sous-groupe de $\text{GL}(n)$.
	
	En reprenant l'exemple précédant et en se rappelant que le déterminant d'une matrice carrée bidimensionnelle est  (\SeeChapter{voir section d'Algèbre Linéaire \pageref{determinant}}):
	
	nous remarquons bien géométriquement ce que signifie d'avoir un déterminant unitaire dans ce cas! Effectivement nous avons vu dans la section d'Algèbre Linéaire lors de notre interprétation géométrique qu'avoir un déterminant équivaut à une surface. Ainsi, le fait d'avoir $ad-bc$ unitaire permet donc que quel que soit l'ordre de la transformation, nous avons l'aire qui vaut toujours $1$. Ainsi, le groupe spécial linéaire conserve les surfaces.
	
	\textbf{Définition (\#\mydef):} Nous appelons "\NewTerm{groupe orthogonal réel d'ordre $n$}\index{groupe orthogonal r\'eel d'ordre $n$}" et notons $\text{O}(n)$ les matrices orthogonales (voir section d'Algèbre Lináire pour un rappel de ce que sont les matrices orthogonales) données par:
	
	Par ailleurs, nous avons démontré dans la section d'Algèbre Linéaire lors de notre étude des matrices de rotations que $A^TA=I_n$  implique $\det(A)=\pm 1$.
	
	C'est le cas par exemple de la matrice de $\text{O}(2)$ vue précédemment (elle appartient au groupe orthogonal mais aussi au groupe des rotations que nous verrons plus loin):
	
	qui est orthogonale comme il est facile de le vérifier (multipliez simplement la matrice par sa transposée pour vérifier si vous obtenez la matrice identité).
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	$\text{O}(1)$ est constitué aussi de l'ensemble des matrices triviales.... $[+1] [-1]$ qui sont simplement des vecteurs à une composante... c'est à dire de simples scalaires.
	\end{tcolorbox}
	\textbf{Définition (\#\mydef):} Si $A\in \text{O}(n)$ et que $\det(A)=1$ nous obtenons alors un sous-groupe de $\text{O}(n)$ appelé "\NewTerm{groupe spécial orthogonal réel d'ordre $n$}\index{groupe sp\'ecial orthogonal r\'eel d'ordre $n$}\label{special real group orthogonal}" et alors défini par:
	
	La matrice de rotations donnée précédemment fait partie de ce groupe puisque son déterminant est égal à l'unité! Par ailleurs, ce groupe occupe une place très spéciale en physique et nous le retrouverons maintes fois lors de notre étude de la physique quantique.
	
	Le sous-groupe $\text{SO}(2)$, appelé aussi parfois "\NewTerm{groupe cercle}\index{groupe cercle}" et noté $S^1$, que nous avions aussi étudié dans la section de Géométrie Euclidienne a une représentative donnée par la matrice:
	
	et occupe une place à part dans la famille des groupes $\text{SO}(n)$ avec $n$ supérieur à l'unité. Effectivement il est le seul à être commutatif. Par ailleurs, il est isomorphe à $e^{\mathrm{i}\theta}$ soit à $\text{U}(1)$ le groupe multiplicatif des nombres complexes de module $1$. C'est aussi le groupe de symétrie propre d'un cercle et l'équivalent continu de $C_n$.
	
	Le sous-groupe $\text{SO}(3)$ donné par la matrice (\SeeChapter{voir section Géométrie Euclidienne page \pageref{3d rotation matrix around x}}):
	
	pour la rotation autour de l'axe $x$ dans l'espace tridimensionnel n'est pas commutatif (les matrices de rotation dans le plan étant elles pour rappel commutatives!). Par ailleurs les quaternions, dont la représentative est donc $\text{SO}(3)$, forment un groupe non commutatif aussi (par rapport à la loi de multiplication) comme nous l'avons vu dans la section sur les Nombres.
	
	Par rapport à un vecteur unitaire on se rend facilement compte visuellement parlant que $\text{SO} (3)$ est un sous-groupe fermé de $\text{GL}(3)$, c'est-à-dire de l'ensemble des groupes linéaires de dimension $3$.
	
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	$\text{SO}(1)$  est constitué de la matrice $[1]$ (c'est-à-dire un simple scalaire unitaire!).
	\end{tcolorbox}	
	\textbf{Définition (\#\mydef):} Nous appelons "\NewTerm{groupe unitaire d'ordre $n$}\index{groupe unitaire d'ordre $n$}\label{unitary linear group}" et nous notons $\text{U}(n)$ les matrices dont les composantes sont complexes (dans le cadre de ce site le plus souvent) ou réelles et qui sont orthogonales:
	
	Remarquons par ailleurs que toute matrice unitaire à coefficients complexes et à une dimension... (de $\text{U}(n)$ donc...) est un nombre complexe de module unitaire, qui peut toujours s'écrire sous la forme $e^{\mathrm{i}\mathbb{R}}$.
	
	
	Nous en avons déjà vu un exemple aussi dans ce livre lors de notre étude des spineurs (voir section du même nom page \pageref{spinors}). Il s'agit des matrices de Pauli (utilisées dans la section de Physique Quantique Relativiste) données par:
	
	\textbf{Définition (\#\mydef):} Nous appelons "\NewTerm{groupe spécial unitaire d'ordre $n$}\index{groupe sp\'ecial unitaire d'ordre $n$}\label{special unitary group}" et nous notons $\text{SU}(n)$ les matrices dont les coefficients sont complexes et qui sont orthogonales et dont le déterminant est unitaire:
	
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	$\text{U}(1)$ est égal à $\text{SU}(1)$ et il s'agit donc du cercle unité complexe égal à $e^{\mathrm{i}\mathbb{R}}$. Par ailleurs, $\text{SO}(2)$ est commutatif et isomorphe à $\text{U}(1)$ car c'est l'ensemble des rotations du plan.
	\end{tcolorbox}
	Un exemple connu est toujours celui des matrices de Pauli mais simplement écrites sous la forme utilisée en Physique Quantique Relativiste (voir section du même nom page \pageref{pauli matrices}):
		
	qui font partie de $\text{SU}(2)$ et qui comme nous l'avons montré (implicitement) au début de la section Calcul Spinoriel est isomorphe au groupe des quaternions $\text{SO} (3)$ de module $1$ sur la sphère de dimension $3$. Relations que les mathématiciens appellent dans le cas présent un "homomorphisme de revêtement"...
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Le groupe spécial unitaire possède une importance particulière en physique des particules. Si le groupe unitaire $\text{U}(1)$ est le groupe de jauge de l'électromagnétisme (pensez au nombre complexe apparaissant dans les solutions de l'équation d'onde!),  $\text{SU}(2)$ est le groupe associé à l'interaction faible, et $\text{SU} (3)$ celui de l'interaction forte. C'est par exemple grâce à la structure des représentations de $\text{SU}(3)$ que Gell-Mann a conjecturé l'existence des quarks.
	\end{tcolorbox}
	Avec une approche différente de celle vue dans le chapitre de Calcul Spinoriel comment montrer que les matrices de Pauli sont les bases de $\text{SU}(2)$?
	
	D'abord, rappelons que nous avons montré dans de Calcul Spinoriel que toute rotation dans l'espace de trois dimensions pouvait s'exprimer à l'aide de la relation:
	
	Et nous avons vu dans le chapitre d'Informatique Quantique qu'une formulation explicitée et décomposée de la relation précédente était:
	
	et donc que tout élément de $\text{SU}(2)$ est produit de ces trois matrices qui font chacune décrire à l'extrémité d'un vecteur dans l'espace une courbe!
	
	Maintenant, nous remarquons que ces trois matrices sont égales lorsque $\theta=0$:
	
	Nous obtenons alors la matrice identité. Donc si nous cherchons la tangente en ce point conjoint, nous pouvons dès lors construire une base ($3$ vecteurs orthogonaux).
	
	Regardons ceci:
	
	Ainsi,  $\text{SU}(2)$ admet pour base:
	
	et ce sont en d'autres termes les générateurs infinitésimaux du groupe $\text{SU}(2)$. $\text{SU}(2)$ a donc une base qui est une Algèbre de Lie selon le vocabulaire des mathématiciens.
	
	Ce résultat est assez remarquable... Puisque $\text{SU}(2)$ et $\text{SO}(3)$ sont isomorphes, nous pouvons alors obtenir la base de l'Algèbre de Lie de $\text{SO}(3)$ alors avec la même méthode!!!
	
	Voyons ceci! Nous avons vu dans le chapitre de Géométrie Euclidienne que les matrices de rotations étaient données par (nous changeons le $R$ par un $U$ afin de ne pas confondre avec les matrices précédentes):
	
	Nous remarquons à nouveau qu'en $\theta=\gamma=\phi=0$ la courbe que fait décrire à un vecteur les trois matrices de rotations passe par:
	
	Alors de la même manière que pour $\text{SU}(2)$, nous calculons les dérivées en ces angles pour déterminer les matrices de base génératrices de $\text{SO}(3)$:
	
	L'algèbre de Lie de $\text{SO} (3)$ admet donc pour base:
	
	En physique, on préfère travailler avec des matrices complexes. Nous introduisons alors les matrices:
	
	Il faut alors remarquer que si nous définissons:
	
	nous avons trivialement pour la complexe conjuguée de la matrice transposée:
	
	et au fait... nous avons aussi les relations de non-commutation (ce que nous pouvons développer sur demande):
	
	et aussi la relation de commutation:
	
	ce que satisfont aussi les matrices de Pauli et... pour rappel (ou information pour ceux qui n'ont pas encore lu la section de Physique Quantique Ondulatoire) les $J_i$ sont les opérateurs du moment cinétique total du système de couplage spin-orbite!!!
	
	La plupart des groupes que nous avons vus jusqu'à présent peuvent être résumés avec la figure suivante :
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/special_linear_group.jpg}
	\end{figure}
	Les rotations avec les quaternions indiquées dans la figure ci-dessus sont étudiées dans la section Nombres du chapitre Arithmétique.
	
	\pagebreak
	\subsubsection{Groupes de Symétries}
	Le groupe de symétries d'un objet noté $X$ (image, signal etc. en 1D, 2D, 3D ou autre) est le groupe de toutes les isométries (une isométrie est une transformation qui conserve les longueurs) sous lesquelles il est invariant avec la composition en tant qu'opération.
	
	Tout groupe de symétries dont les éléments ont un point fixe commun, ce qui est vrai pour tous les groupes de symétries de figures limitées, peut être représenté comme un sous-groupe du groupe orthogonal O(n) en choisissant l'origine pour point fixe. Le groupe de symétries propre est alors un sous-groupe du groupe orthogonal spécial $\text{SO}(n)$, et par conséquent, il est aussi appelé le "groupe de rotations" de la figure.
	
	Dans ce qui suit, nous allons interpréter la composée de deux opérations de symétries ou de rotations comme une multiplication au même titre que pour les permutations.
	
	Voyons d'abord deux définitions fondamentales:
	
	\textbf{Définitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] Le "\NewTerm{groupe des sym\'etries}\index{groupe des sym\'etries}", appelé aussi "\NewTerm{groupe des invariants}\index{groupe des invariants}", de $X$ est l'ensemble des symétries de $X$, muni de la structure de multiplication donnée par composition qui laisse $X$ invariant.
		
		\item[D2.] "\NewTerm{L'ordre}\index{ordre d'un groupe}" d'un groupe est le nombre total de toutes ses symétries uniquement (y compris l'identité!).
	\end{enumerate}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemples:}\\\\
	E1. Le coeur (...):
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/symmetries_heart.jpg}
	\end{figure}
	a un groupe de symétries total à $2$ éléments, à savoir l'application identité $\text{id}$ et l'application qui est la réflexion dans l'axe vertical $r_v$ (sous-groupe de symétries à $1$ élément). Nous observons que le symétrique est donné aussi via la relation:
	
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	E2. La lettre phi (...):
	\begin{center}
	\[ \scalebox{8}{$\Phi$} \]
	\end{center}
	a un groupe de symétrie total à $4$ éléments, à savoir l'application identité $\text{id}$, les deux réflexions $r_h$ et $r_v$ et la rotation par l'angle $\pi$ que nous noterons $t_\pi$ (sous-groupe de rotations à $1$ élément). Cette forme possède donc un groupe de symétries d'ordre $3$.\\
	
	Dans ce groupe nous avons:
	
 	(et c'est commutatif!), $t_{\pi}\circ t_{\pi}$ est la rotation par un angle  $2\pi$, ce qui est la même application que l'application identité, donc $t_{\pi}\circ t_\pi=\text{id}$.\\
 	
 	Ainsi, le groupe de symétries de cette lettre est commutatif et la loi de composition est bien interne. C'est donc bien un groupe!\\
 	
 	E3. Le pentagone régulier:
 	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/symmetries_pentagon.jpg}
	\end{figure}
	a un groupe de symétries total à $10$ éléments à savoir les $5$ rotations $\text{id},t_{2\pi/5},t_{4\pi/5},t_{6\pi/5},t_{8\pi/5}$ ainsi que les $5$ réflexions dans les $5$ axes de symétries. C'est donc un groupe de symétries d'ordre $5$ correspondant au groupe cyclique $\mathbb{Z}/ 5\mathbb{Z}$.
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Plus généralement, le groupe de symétries d'un $n$-gone régulier (si $n$ est impair) a exactement $2n$ éléments. Ce groupe s'appelle le "\NewTerm{groupe diédral d'ordre $n$}\index{groupe di\'edral d'ordre $n$}" et est noté le plus souvent $D_{2n}$ (il faut faire attention car certains auteurs ne multiplient pas $n$ par le facteur $2$ ce qui fait que l'indice représente alors directement l'ordre et non le nombre d'éléments).
	\end{tcolorbox}	
	Le pentagone a donc $D_{10}$ pour groupe diédral et $\mathbb{Z}/ 5\mathbb{Z}$ en est un "\NewTerm{sous-groupe distingué}\index{sous-groupe distingu\'e}" (nous reviendrons plus tard sur cette notion de sous-groupe distingué).
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	E4. Le groupe diédral $D_6$ d'ordre $3$ des isométries d'un triangle équilatéral (polygone régulier) a $6$ éléments que nous noterons (afin que l'écriture soit moins lourde):
	
	où $\sigma_1,\sigma_2,\sigma_3$ sont les symétries par rapport aux trois bissectrices (respectivement médiatrices). La table de compositions de ce groupe diédral montre aussi que ce groupe est non-commutatif:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/equilateral_dihedral_group_representation.jpg}
	\end{figure}
	\begin{table}[H]
		\begin{center}
		\begin{tabular}{>{\columncolor[gray]{0.75}}c||c|c|c|c|c|c|}
	\hline
	\rowcolor[gray]{0.75}$\nearrow D_6$ & $id$ & $t_{2\pi/3}$ & $t_{4\pi/3}$ & $\sigma_1$ & $\sigma_2$ & $\sigma_3$ \\
	  \hline \hline
	  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
	 $\text{id}$ & $\text{id}$ & $t_{2\pi/3}$ & $t_{4\pi/3}$ & $\sigma_1$ & $\sigma_2$ &$\sigma_3$ \\
	 \hline
	 $t_{2\pi/3}$ & $t_{2\pi/3}$ & $t_{4\pi/3}$ & $\text{id}$ & $\sigma_3$ & $\sigma_1$ &$\sigma_2$ \\\hline
	 $t_{\pi/3}$ & $t_{4\pi/3}$ & $\text{id}$ & $t_{2\pi/3}$ & $\sigma_2$ & $\sigma_3$ &$\sigma_1$ \\  \hline
	 $\sigma_1$ & $\sigma_1$ & $\sigma_2$ &$\sigma_3$ &$\text{id}$ & $t_{2\pi/3}$ & $t_{4\pi/3}$ \\\hline
	 $\sigma_2$ & $\sigma_2$ & $\sigma_3$ &$\sigma_1$  & $t_{4\pi/3}$ & $\text{id}$ & $t_{2\pi/3}$ \\\hline
	 $\sigma_3$ & $\sigma_3$ & $\sigma_1$ &$\sigma_2$  & $t_{2\pi/3}$ & $t_{4\pi/3}$ & $\text{id}$  \\
	  \hline
		\end{tabular}
		\end{center}
		\caption{Symétries du groupe diédral d'ordre $3$}
	\end{table}
	Nous reviendrons sur cet exemple lorsque nous introduirons un peu plus loin le concept de groupe distingué lors de notre étude des groupes de permutations et la définition des groupes distingués.\\
	
	E5. Regardons un dernier exemple appliqué à la chimie en énumérant les opérations de symétries qui laissent la molécule $\mathrm{NH}_3$ (tétraèdre) invariante.
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/nh3.jpg}
	\end{figure}
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	Le groupe de transformations contient $6$ éléments: l'identité $\text{id}$, $C_3$ qui est la rotation de $2\pi/3$, la rotation de (que nous noterons par la suite $C_{-3}$) toutes deux selon l'axe $z$ (perpendiculaire au plan $xy$ donc...) et $3$ axes  $\sigma_1,\sigma_2,\sigma_3$ de symétrie/réflexion passant chacun par le milieu d'une des arêtes de base au milieu de l'arête opposée comme le montre la figure ci-dessous (pyramide vue du dessus):
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/tetrahedral_group_representation.jpg}
		\caption{Opérations laissant invariant un tétraèdre}
	\end{figure}
	La combinaison des différents éléments de symétries montre que la table de compositions est (ce qui prouve que la loi est interne et que nous travaillons donc bien dans un groupe):
	\begin{table}[H]
		\begin{center}
		\begin{tabular}{>{\columncolor[gray]{0.75}}c||c|c|c|c|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75}$\nearrow $ & $\text{id}$ & $\sigma_1$ & $\sigma_2$ & $\sigma_3$ &$C_3$&$C_{-3}$\\
		  \hline \hline
		  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
		 $\text{id}$ & $\text{id}$ & $\sigma_1$ & $\sigma_2$ & $\sigma_3$ &$C_3$&$C_{-3}$ \\
		 \hline
		 $\sigma_1$ & $\sigma_1$ & $\text{id}$ & $C_{-3}$& $C_3$& $\sigma_3$ &$\sigma_2$ \\\hline
		 $\sigma_2$ & $\sigma_2$ & $C_3$ & $\text{id}$  & $C_{-3}$& $\sigma_1$  & $\sigma_3$ \\\hline
		 $\sigma_3$ & $\sigma_3$ & $C_{-3}$ & $C_3$  & $\text{id}$ & $\sigma_2$ &$\sigma_1$ \\  \hline
		$ C_3$ &  $C_{3}$ &  $\sigma_2$ &$\sigma_3$ & $\sigma_1$ &$C_{-3}$&$id$\\  \hline
		$C_{-3}$&$C_{-3}$& $\sigma_3$ &$\sigma_1$ & $\sigma_2$ &$\text{id}$&$C_3$\\
		  \hline
		\end{tabular}
		\end{center}
		\caption{Compositions de transformations du tétraèdre}
	\end{table}
	Attention à l'ordre des opérations dans le tableau ci-dessus, nous appliquons d'abord l'élément de ligne puis l'élément de colonne!\\
	
	Nous constatons que le groupe n'est donc pas commutatif.
	\end{tcolorbox}

	\pagebreak
	\paragraph{Orbite et Stabilisateur}\mbox{}\\\\
	Nous allons voir maintenant deux définitions que nous retrouverons en cristallographie (leur nom n'est pas innocent!).
	
	\textbf{Définition (\#\mydef):} L'orbite d'un élément $x$ de $E$ est donnée par:
	
	L'orbite de $x$ est l'ensemble des positions (dans $E$) susceptibles d'être occupées par l'image de $x$ sous l'action de $G$. Les orbites forment évidemment une partition de $E$. 
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemple:}\\\\
	Considérons un ensemble $E$ sur lequel agit un groupe $G$, par:
	
	l'ensemble des $6$  sommets d'un hexagone sur lequel nous faisons agir le groupe $G=\{\text{id},t_{2\pi/3},t_{4\pi/3}\}$. Nous observons déjà trivialement que $G$ est bien un groupe!\\
	
	Maintenant, prenons un élément de $E$, par exemple $S_0$.\\
	
	Son orbite va donc être par définition:
	
	\end{tcolorbox}
	\textbf{Définition (\#\mydef):} Le stabilisateur $x$ d'un élément de $E$ est l'ensemble:
	
	des éléments qui laissent $x$ invariant sous leur action. C'est un sous-groupe de $G$.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemple:}\\\\
	Pour reprendre notre exemple précédent. Son stabilisateur va être réduit à:
	
	\end{tcolorbox}
	
	
	\pagebreak
	\subsubsection{Groupes de Permutations}
	Les groupes symétriques ont une importance non négligeable dans certains domaines de la physique quantique mais aussi en mathématiques en Algèbre Linéaire (pour la théorie du déterminant comme nous le verrons dans la section correspondante) et dans le cadre de la théorie de Galois (voir plus bas) et dans les codes correcteurs d'erreurs (voir la section du même nom et en particulier l'exemple avec les cartes de crédti). Il convient donc d'y porter aussi une attention toute particulière.
	
	Rappelons d'abord (\SeeChapter{voir section Probabilités page \pageref{simple permutation without repetitions}}) que dans un ensemble $\{1,...,n\}$ il y a $n!$ permutations possibles. Les mathématiciens disent, à juste titre, qu'il y a $n!$ bijections et appellent ce nombre "\NewTerm{ordre du groupe de permutations}\index{ordre du groupe de permutations}".
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemple:}\\
	Prenons par exemple l'ensemble $\{1,2,3\}$. Cet ensemble a $3!$ permutations possibles qui sont notées dans le cadre des groupes de permutation de la manière suivante:
	
	Ce qui se lit dans l'ordre: application identité id, $1$ amène sur $2$ ou $2$ sur un $1$ (en termes de position!), $1$ amène sur $3$ ou $3$ sur $1$, $2$ amène sur $3$ ou $3$ sur $2$, $1$ amène sur $2$ qui amène sur $3$ qui amène sur $1$, $1$ amène sur $3$ qui amène sur $2$ qui amène sur $1$.
	
	Nous pouvons observer facilement que la composition de deux permutations n'est pas commutative:
	
	et que la composition de deux permutations est une loi interne:
	
	avec un élément neutre qui est bien l'identité id. Nous avons donc bien un groupe non commutatif. Rappelons également au lecteur que certains éléments du groupe, s'ils sont bien choisis, peuvent former un sous-groupe. C'est l'exemple de:
	
	qui est un sous-groupe de $S_3$ (il est facile de vérifier qu'il possède toutes les propriétés d'un groupe).
	\end{tcolorbox}
	
	\textbf{Définition (\#\mydef):} Un sous-groupe $H$ d'un groupe $G$ est appelé "\NewTerm{groupe distingué}\index{groupe distingu\'e}" si, pour tout $g$ de $G$ et tout $h$ de $H$, nous avons que $ghg^{-1}$ est élément de $H$. Les mathématiciens appellent cela un "\NewTerm{automorphisme intérieur}\index{automorphisme intérieur}"...
	
	Voyons d'abord un exemple géométrique parlant après quoi nous reviendrons à cette définition avec $S_3$.

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemple:}\\
	Nous avons vu plus haut les éléments du groupe de symétrie diédral d'ordre $3$ du triangle équilatéral. Géométriquement ils correspondent tous à des déplacements dans le plan dans lequel se trouve le triangle. Nous avions obtenu pour rappel le tableau de compositions suivant:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/algebra/equilateral_dihedral_group_representation.jpg}
	\end{figure}
	\begin{table}[H]
		\centering
		\begin{tabular}{>{\columncolor[gray]{0.75}}c||c|c|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75}$\nearrow D_6$ & $id$ & $t_{2\pi/3}$ & $t_{4\pi/3}$ & $\sigma_1$ & $\sigma_2$ & $\sigma_3$ \\
		  \hline \hline
		  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
		 $\text{id}$ & $\text{id}$ & $t_{2\pi/3}$ & $t_{4\pi/3}$ & $\sigma_1$ & $\sigma_2$ &$\sigma_3$ \\
		 \hline
		 $t_{2\pi/3}$ & $t_{2\pi/3}$ & $t_{4\pi/3}$ & $\text{id}$ & $\sigma_3$ & $\sigma_1$ &$\sigma_2$ \\\hline
		 $t_{\pi/3}$ & $t_{4\pi/3}$ & $\text{id}$ & $t_{2\pi/3}$ & $\sigma_2$ & $\sigma_3$ &$\sigma_1$ \\  \hline
		 $\sigma_1$ & $\sigma_1$ & $\sigma_2$ &$\sigma_3$ &$\text{id}$ & $t_{2\pi/3}$ & $t_{4\pi/3}$ \\\hline
		 $\sigma_2$ & $\sigma_2$ & $\sigma_3$ &$\sigma_1$  & $t_{4\pi/3}$ & $\text{id}$ & $t_{2\pi/3}$ \\\hline
		 $\sigma_3$ & $\sigma_3$ & $\sigma_1$ &$\sigma_2$  & $t_{2\pi/3}$ & $t_{4\pi/3}$ & $\text{id}$  \\
		  \hline
		\end{tabular}
	\end{table}
	D'abord, nous constatons facilement à l'aide de ce tableau que nous avons:
	\begin{itemize}
		\item Le sous-groupe formé de $\{\text{id}\}$ d'ordre $1$
	
		\item Le sous-groupe formé de $\{\text{id},t_{2\pi/3},t_{4\pi/3}\}$ d'ordre $3$

		\item Le sous-groupe formé de $\{\text{id},\sigma_1\}$ d'ordre $2$

		\item Le sous-groupe formé de $\{\text{id},\sigma_2\}$ d'ordre $2$

		\item Le sous-groupe formé de $\{\text{id},\sigma_3\}$ d'ordre $2$
	\end{itemize}
	Parmi ces cinq sous-groupes, voyons lesquels sont distingués (cela est relativement facile à visualiser à l'aide du tableau de compositions):
	\begin{itemize}
		\item Le sous-groupe formé de $\{\text{id}\}$
	
		\item Le sous-groupe formé de $\{\text{id},t_{2\pi/3},t_{4\pi/3}\}$ 
	\end{itemize}
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	Nous allons voir maintenant une chose remarquable! En numérotant par $1$, $2$ et $3$ les sommets du triangle équilatéral et en prenant les rotations dans le sens des aiguilles d'une montre, nous pouvons identifier les éléments de $D_6$ aux éléments suivants de $S_3$:
	
	et reconstruire la même table de compositions (copie de la précédente mais juste avec le changement d'écriture... hé hé!):
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/equilateral_dihedral_group_representation.jpg}
	\end{figure}
	\begin{table}[H]
		\begin{center}
		\begin{tabular}{>{\columncolor[gray]{0.75}}c||c|c|c|c|c|c|c|c|}
		\hline
		\rowcolor[gray]{0.75}$\nearrow S_3$ & $ (1) $ & $ (1\ 2\ 3) $ & $(1\ 3\ 2)$ & $(1\ 2)$ & $(1\ 3)$ & $(2\ 3)$ \\
		  \hline \hline
		  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
		 $(1)$ & $(1)$ & $(1\ 2\ 3)$ & $(1\ 3\ 2)$ & $(1\ 2)$ &$(1\ 3)$&$(2\ 3)$ \\
		 \hline
		 $(1\ 2\ 3)$ & $(1\ 2\ 3)$ & $(1\ 3\ 2)$ & $(1)$& $(2\ 3)$ & $(1\ 2)$ & $(1\ 3)$ \\ \hline
		$(1\ 3\ 2)$ & $(1\ 3\ 2)$ & $(1)$ & $(1\ 2\ 3)$& $(1\ 3)$ &$(2\ 3)$ &$(1\ 2)$ \\ \hline
		 $(1\ 2)$ &$(1\ 2)$ &$(1\ 3)$& $(2\ 3)$ &  $(1)$&$(1\ 2\ 3)$  & $(1\ 3\ 2)$  \\ \hline
		$(1\ 3)$ &$(1\ 3)$ &$(2\ 3)$ &$(1\ 2)$ &$(1\ 3\ 2)$ & $(1)$ & $(1\ 2\ 3)$  \\ \hline
		$(2\ 3)$ &$(2\ 3)$ &$(1\ 2)$ &$(1\ 3)$ &$(1\ 2\ 3)$ & $(1\ 2\ 3)$ & $(1)$   \\\hline
		\end{tabular}
		\end{center}
	\end{table}
	\end{tcolorbox}
	Bon... ce petit interlude fermé, revenons au groupe distingué de $S_3$ (car il va être important pour notre introduction aux groupes de Galois) et rappelons d'abord que:
	
	et nous voyons que le sous-groupe distingué est formé de:
	
	\textbf{Définition (\#\mydef):} Pour tout sous-groupe $H$ stable par les automorphismes intérieurs d'un groupe $G$, nous appelons "\NewTerm{indice de $H$ dans $G$}" le quotient de l'ordre du groupe $G$ par l'ordre du sous-groupe $H$ et nous l'écrivons $[G/H]$.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemple:}\\\\
	L'indice du sous-groupe $\{(1), (1 2)\}$ dans le groupe $S_3$ est $6/2$ c'est-à-dire $3$.
	\end{tcolorbox}
	 Ce concept nous sera très utile lors de notre introduction aux corps de Galois plus loin.
	
	Considérons maintenant, la permutation particulière $\sigma$ pour aborder le sujet sous un angle différent mais équivalent:
	
	Les mathématiciens ont pour habitude de noter cela, dans un premier temps, sous la forme:
	 
	avec:
	
	Dès lors:
	 
	Etant donné $\sigma$ et $\tau$, deux permutations, il est naturel de regarder leur composition (rappelons que cela signifie d'abord $\sigma$, puis $\tau$ comme pour la composition de fonctions).
	
	Ainsi, si:
	
	Alors:
	
	et:
	
	Maintenant, l'idée est d'interpréter la composition comme une multiplication de permutations. Cette multiplication est alors non-commutative comme nous venons de le constater dans l'exemple précédent. Nous avons en général $\sigma \circ \tau \neq \tau \circ \sigma$.
	
	Chaque bijection a un inverse (une fonction réciproque). Dans notre exemple il s'agit de évidemment de:
	
	Géométriquement, pour calculer l'inverse $\sigma^{-1}$ d'un élément $\sigma$, il suffit de prendre la réflexion du dessin de $\sigma$ dans un axe horizontal comme le montre la partie gauche de la figure ci-dessous:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/permutations.jpg}
		\caption{Exemples de composées et d'inverses de permutations}
	\end{figure}
	Nous pouvons représenter une permutation $\sigma$ comme une matrice de permutation, c'est-à-dire la matrice qui mappe :
	
	Une matrice $n\times n$ est une matrice de permutation si et seulement si toutes ses entrées sont des zéros et des uns, chaque colonne a exactement un $1$ et chaque ligne a exactement un $1$. Dans ce cas, nous avons :
	 
	 \textbf{Définitions (\#\mydef):}
	 \begin{enumerate}
		\item[D1.] L'ensemble des permutations d'un ensemble avec $n$ éléments, muni de cette structure de multiplication, s'appelle le "\NewTerm{groupe des permutations d'ordre $n$}\index{groupe des permutations d'ordre $n$}" ou  "\NewTerm{groupe des substitutions d'ordre $n$}\index{groupe des substitutions d'ordre $n$}", et se note $S_n$ ou encore $S(n)$.
		
		\item[D2.] Nous disons qu'un élément $\sigma$ de $S_n$ est un "\NewTerm{cycle d'ordre $k$}\index{cycle d'ordre $k$}", ou un "\NewTerm{$k$-cycle}", s'il existe $a_1,a_2,...,a_k\in \{1,...,n\}$ tel que:
		\begin{itemize}
			\item $\sigma$ envoie $a_1$ sur $a_2$, $a_2$ sur $a_3$, ..., $a_{k-1}$, et $a_k$ sur $a_1$.

			\item $\sigma$ fixe tous les autres éléments de $S_n$
		\end{itemize}
		et nous notons le cycle ainsi:
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Exemple:}\\\\
		Pour mieux comprendre reprenons notre exemple de $S_4$:
		
		Ce groupe symétrique est un $3$-cycle noté $\sigma=(1\; 3\; 4)$ car dans l'ordre: $1$ envoie sur $3$, $3$ envoie sur $4$ et $4$ envoie sur $1$ (et le $2$ n'étant pas mentionné il reste fixe). Nous pouvons noter cela aussi des façons suivantes équivalentes: $\sigma=(3\; 4\; 1)$ ou encore $\sigma=(4\; 1\; 3)$.
		\end{tcolorbox}

		\item[D3.] L'ordre d'un $k$-cycle est $k$ (d'où le nom!).
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Exemple:}\\\\
		Effectivement si nous reprenons $\sigma=(1\; 3\; 4)$, nous avons alors:
		
		\end{tcolorbox}

		\item[D4.] Nous disons qu'une permutation $\sigma$ est un "\NewTerm{cycle}\index{cycle (permutation)}" s'il existe $k\in \mathbb{N}$ tel que $\sigma$ est un $k$-cycle.
		
		Attention! Toute permutation doit s'écrire comme un produit de cycles disjoints (c'est-à-dire qu'un nombre qui apparaît dans un cycle ne doit pas apparaître dans un autre cycle).
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Exemple:}\\\\
		Par exemple, dans $S_9$, nous avons:
		
		Donc cette permutation est un produit d'un $4$-cycle et d'un $3$-cycle disjoint.\\
		
		Nous laisserons d'ailleurs le lecteur vérifier par lui-même que le groupe cyclique engendré par $\sigma$ (qui dans le cas présent est sous-groupe de $S_9$) est d'ordre $12$ ($12$-cycle)...
		\end{tcolorbox}
		\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
		Les mathématiciens peuvent démontrer que si $\sigma$ est un élément qui a une décomposition en $c$ cycles disjoints de longueur $n_1,n_2,...,n_c$ alors l'ordre $\sigma$ de est le plus petit commun multiple des ordres de tous les cycles disjoints qui le composent.
		\end{tcolorbox}	
	\end{enumerate} 
	Nous supposerons également intuitif que dans le vocabulaire commun, un $2$-cycle dans $S_n$ s'appelle aussi une "\NewTerm{transposition}\index{transposition}".
	
	Allons un petit peu plus loin. Nous nous proposons de montrer par l'exemple que l'ensemble des transpositions engendre $S_n$. Autrement, dit, toute permutation s'écrit comme un produit de transpositions!
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemple:}\\\\
	Reprenons notre exemple (il s'agit d'une permutation paire):
		
	\end{tcolorbox}
	En général, un $k$-cycle s'écrit donc comme produit de $k-1$ transpositions.
	
	\begin{theorem}
	Comme les permutations d'un ensemble fini constituent un groupe. Cela signifie (entre autres choses) qu'il existe donc toujours un entier $k$, tel que $p$ opéré $k$ fois est la transformation identité (c'est-à-dire l'opération qui ne change rien).
	\end{theorem}
	\begin{dem}
	Si $G$ est un groupe fini et que $g\in G$, nous considérons la suite d'éléments (se rappeler que dans un groupe il ,n'y a, qu'une opération et donc le carré, le cube, etc. signifie que nous composons cette opération!):
	
	Par exemple, dans les groupes de permutations, l'opération est la composition des permutations.
	
	Étant donné que $G$ est fini et que cette suite est infinie, il existe forcément deux éléments égaux dans la suite... Il existe donc deux indices différents n et m tels que:
	
	En supposant que $n<m$, l'égalité précédente se simplifie et nous obtenons:
	
	où $e$ est l'élément neutre du groupe.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Nous allons voir que les permutations étant bijectives, nous pouvons créer sur des groupes finies des compositions d'opération de permutation qui finissent toujours par ramener à l'état initial (application identité id).
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemples:}\\\\
	E1. Dans une liste de $5$ objets, nous échangeons le premier et le troisième, et, en même temps, nous faisons passer le deuxième en position $4$, celui qui est en position $4$ est mis en position $5$ et celui qui est en position $5$ est mis en position $2$. En nous réitérons. Cela donne:
	
	Nous sommes revenus au point de départ après $6$ étapes.
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	E2. Considérons la "\NewTerm{transformation du photomaton}\index{transformation du photomaton}" d'une image de Mona Lisa de dimension de $256$ par $256$ pixels:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/mona_lisa.jpg}
		\caption{Transformation du photomaton}
	\end{figure}
	Nous pouvons avoir l'impression que chaque image a été obtenue à partir de la précédente en réduisant la taille de l'image de moitié, ce qui a donné quatre morceaux analogues que nous avons placés en carré pour obtenir une image ayant la même taille que l'image d'origine. Mais en fait il n'est est rien! Le nombre de pixels a été conservé (aucun pixel n'est dupliqué!!!) et en fait nous avons seulement déplacé les pixels par permutation pour avoir quatre images qui ne contiennent pas réellement toute l'information de l'image d'origine mais seulement une partie. \\
	
	En réitérant la procédure $8$ fois, nous retombons toujours sur l'image d'origine quelle que soit l'image de départ. La question est de comprendre alors pourquoi?\\
	
	Considérons que l'image d'origine est un carré d'une taille de $16$ pixels de large par $16$ pixels de haut (mais vous pouvez appliquer ce qui va suivre avec une image rectangulaire de n'importe quelle taille et vous verrez que cela marche aussi!). Chaque pixel d'une ligne (la démarche est exactement la même pour les colonnes!) est identifié par une coordonnée selon l'axe $X$ allant de $0$ à $15$.\\
	
	Nous avons ainsi une suite de nombres au début où les coordonnées de pixels correspondant à la leur coordonnée $x$ :
	
	Nous faisons alors la permutation qui consiste à noter $k$ la position d'un pixel et de faire:
	
	\end{tcolorbox}

	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	Cela donne alors à la première permutation:
	
	Ainsi, pour une image de $16$ par $16$ pixels, il faut quatre permutations, ce qui correspond à $2^4=16$. Donc, pour une image de $256$ pixels, nous avons $256=2^8$, d'où le fait qsu'il faille $8$ permutations pour retrouver la Mona Lisa d'origine avec:
	
	Ainsi, dans le cas général d'une image de largeur $L$ en nombre de pixels en comptant à partir de $1$, la transformation est:
	
	où $E^+[...]$ est la valeur entière supérieur la plus proche au cas où $L$ serait impair.\\
	
	Le lecteur aura aussi peut-être remarqué quelque chose d'intéressant si nous reprenons notre exemple avec l'image de $16$ pixels... Effectivement, prenons le troisième pixel depuis la gauche de coordonnée $x$ égale à $2$. En binaire, sa position initiale est alors $0010$. Après la première permutation, sa coordonnée $x$ est égale à $1$, soit en binaire: $0001$. Après la deuxième permutation, sa coordonnée $x$ est égale $8$, soit en binaire: $1000$, etc. Au fait nous voyons que chaque permutation se résume en binaire à décaler les bits vers la droite.
	\end{tcolorbox}
	\textbf{Définition (\#\mydef):} Soit $\sigma \in S_n$ une permutation. Nous disons que $\sigma$ est "\NewTerm{permutation paire}\index{permutation paire}" si, dans une écriture de $\sigma$ comme produit de transpositions, il y a un nombre pair de transpositions. Nous disons évidemment que $\sigma$ est une "\NewTerm{permutation impaire}\index{permutation impaire}" si, dans une écriture de $\sigma$ comme produit de transpositions, il y a un nombre impair de transpositions.
	
	Finissons par un petit complément... Nous avons que $S_3$ est un groupe des permutations d'ordre $3$ avec donc $3!=6$ permutations possibles.
	
	Si nous énumérons les $6$ permutations nous avons vu que nous obtenons:
	
	Parmi ceux-ci, seuls certains peuvent être écrits comme un produit pair de transpositions :
	
	Les permutations paires forment avec la permutation identité, un sous-groupe (non commutatif !) que l'on nomme le "\NewTerm{groupe alternatif d'ordre $n$}\index{groupe alternatif d'ordre $n$}" et que l'on note $AN $. C'est facile à vérifier avec l'exemple précédent.
		
	\subsection{Théorie de Galois}\label{galois theory}
	En algèbre abstraite, la théorie de Galois, du nom d'Évariste Galois, établit un lien entre la théorie des corps et la théorie des groupes. En utilisant la théorie de Galois, certains problèmes de la théorie des corps peuvent être réduits à la théorie des groupes, qui est, dans un certain sens, plus simple et mieux comprise.
	
	À l'origine, Galois utilisait des groupes de permutations pour décrire comment les diverses racines d'une équation polynomiale donnée sont liées les unes aux autres.
	
	La naissance et le développement de la théorie de Galois ont été provoqués par la question suivante, dont la réponse est connue sous le nom de "\NewTerm{théorème d'Abel-Ruffini}\index{théorème d'Abel-Ruffini}" : Pourquoi n'y a-t-il pas de formule pour les racines d'une équation polynomiale de degré cinq (ou supérieur) en termes de coefficients du polynôme, en utilisant uniquement les opérations algébriques habituelles (addition, soustraction, multiplication, division) et l'application de radicaux (racines carrées, racines cubiques, etc.)?.
	
	La théorie de Galois ne fournit pas seulement une belle réponse à cette question, elle explique également en détail pourquoi il est possible de résoudre des équations de degré quatre ou moins de la manière ci-dessus, et pourquoi leurs solutions prennent la forme qu'elles prennent. De plus, cela donne un moyen conceptuellement clair et souvent pratique de dire quand une équation particulière de degré supérieur peut être résolue de cette manière.
	
	La théorie de Galois trouve son origine dans l'étude des fonctions symétriques : les coefficients d'un polynôme monique (polynômes dont le coefficient affecté au degré le plus haut du dénominateur doit être $1$) sont (au signe près) les polynômes symétriques élémentaires dans les racines. Par exemple: roots. For instance:
	
 	où $1$, $a + b$ et $ab$ sont les polynômes élémentaires de degré $0$, $1$ et $2$ à deux variables.
 	
 	Nous avons essayé de rendre cette partie du livre aussi simple que possible. Nous espérons donc que notre objectif est atteint si vous comprenez ce qui suit. Commençons maintenant !!!
 	
 	\subsubsection{Polynômes élémentaires symétriques et invariants}
 	\textbf{Définition (\#\mydef):} Le "\NewTerm{$k$-ième polynôme symétrique élémentaire à $n$ variables}\index{polyn\-ome sym\'etrique \'el\'ementaire}", noté $s_k(x_k,\ldots , x_n)$ est la somme de tous les monômes de degré $k$ possibles avec $n$ variables avec chaque $x_i$ apparaissant PAS PLUS D'UNE FOIS DANS CHAQUE MONÔME. Formellement, pour $k\leq n$ :
	
	Un polynôme est dit "\NewTerm{invariant sous $S_n$}\index{polyn\-ome invariant}" si et seulement si c'est un polynôme dans les fonctions symétriques élémentaires $s_1,\ldots, s_n$.

	Dès lors:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemple:}\\\\
	Pour $n = 1$:
	
	Pour $n = 2$:
	
	Pour $n = 3$:
	
	Pour $n = 4$:
	
	Considérons maintenant l'équation :
	
	Elle peut être réécrite comme :
	
	ou comme nous connaissons les relations de Viète à deux racines, ces dernières peuvent être réécrites :
	
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	C'est-à-dire:
	
	De même, pour un polynôme du troisième degré :
	
	et dès lors:
	
	\end{tcolorbox}
	Les polynômes symétriques élémentaires apparaissent lorsque nous développons une factorisation linéaire d'un polynôme monique. Nous avons l'identité :
	
	C'est-à-dire que lorsque nous substituons des valeurs numériques aux variables $r_1,r_2,\dots,r_n$, nous obtenons le polynôme univarié monique (avec la variable $x$) dont les racines sont les valeurs substituées à $r_1,r_2,\dots, r_n$ et dont les coefficients sont à leur signe près les polynômes symétriques élémentaires. Ces relations entre les racines et les coefficients d'un polynôme sont nommées "\NewTerm{formules générales de Viète}\index{formules générales de Vi\'ete}" pour lesquelles nous avons déjà vu deux cas particuliers dans la section Calcul et que nous généraliserons plus loin.
	\begin{theorem}
	Si $r_1,r_2,\ldots, r_n$ sont les racines d'un polynôme de degré $n$, alors:
	
	\end{theorem}
	\begin{dem}
	Nous allons prouver ceci par récurrence sur le degré du polynôme. Si notre polynôme est de degré $n = 1$ avec la racine $r$, le côté gauche est $x-r$, et le côté droit est $x-s_1(r)= x-r$, donc l'équation est vraie pour $n = 1$. Supposons que l'équation soit vraie pour tous les polynômes de degré $n$. Soit $P(x)$ de degré $n+1$ de racines $r_1,\ldots,r_{n+1}$.

    Dès lors, nous pouvons écrire:
    
	où nous laissons $s_i$ dénoter $s_i(r1, \ldots , a_n)$ par souci de concision. En multipliant le membre de droite :
	
	Étant donné que:
	
	et:
	
	Nous avons alors:
	
	Reste maintenant que si l'on peut prouver que :
	
	pour tous les autres $i$, nous pourrons conclure que l'équation est vraie pour $n+1$, donc pour tous les $n$.
	
	Rappelez-nous que par définition :
	
	Alors:
	
	En séparant la somme par rapport aux monômes divisibles par $r_{n+1}$, nous voyons que ce qui précède est égal à (la plupart du temps le mieux est de vérifier cela en utilisant l'un des exemples précédents donnés plus tôt):
	
	il est donc clair que la relation que nous voulions tient!
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\subsubsection{Formules générales de Viète}
	Si nous écrivons l'équation du second degré comme suit :
	
	Nous savons déjà que:
	
	Et aussi pour un polynôme du troisième degré nous avons vu juste plus haut dans les exemples que si nous avons :
	
	alors:
	
	Nous pouvons facilement voir émerger un motif qui est:
	
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{90} & \pbox{20cm}{\score{4}{5} \\ {\tiny 16 votes,  81.25\%}} 
	\end{tabular} 
	\end{flushright}

	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Calcul Différentiel et Intégral}\label{differential and integral calculus}
	\lettrine[lines=4]{\color{BrickRed}L}e calcul différentiel est un des domaines les plus passionnants et vastes de la mathématique, et il existe une littérature considérable (colossale) sur le sujet. Les résultats initiés par des scientifiques comme Fermat, Newton, Leibniz, Euler et compagnie depuis la fin du 17ème siècle retrouvent des implications dans absolument tous les domaines de la physique, de l'informatique, de l'électronique, de la chimie, de la finance, de la biologie et de la mathématique elle-même.

	Les mathématiciens ont rédigé une telle quantité de théorèmes depuis sa naissance au milieu du 16ème siècle sur le sujet que la validation d'un échantillon de ceux-ci est parfois délicate car nécessitant à eux-seuls la vie d'un homme pour être parcourus (c'est un problème que la communauté des mathématiciens reconnaît) et vérifiés (ce qui fait que parfois personne ne les vérifie...).

	Ce constat fait, nous avons choisi de ne présenter ici que les points absolument nécessaires à la compréhension des outils fondamentaux de l'ingénieur. Les puristes nous excuseront donc pour l'instant de ne pas présenter certains théorèmes qui peuvent leur sembler indispensables mais que nous rédigerons une fois le temps venu...

	Nous allons principalement étudier dans ce qui va suivre ce que les mathématiciens aiment bien préciser (et ils ont raison): les cas généraux des fonctions réelles à une variable réelle. Les fonctions plus complexes (à plusieurs variables réelles ou complexes, continues ou discrètes) viendront une fois cette partie terminée.

	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Nous ne nous attarderons pas à démontrer les dérivées et primitives de toutes les fonctions car comme il y a une infinité de fonctions possibles, il y a également une infinité de dérivées et de primitives. C'est le rôle des professeurs dans les instituts scolaires d'entraîner les élèves à appliquer et à comprendre le raisonnement de dérivation et d'intégration par des applications sur des fonctions connues (l'Internet ne remplacera très probablement jamais l'école à ce niveau).
	\end{tcolorbox}	
	
	\subsection{Calcul Différentiel}\label{differential calculus}
	Soit une fonction $f$ réelle à une variable réelle $x$ notée $f(x)$ (nous nous limitons à ce cas de figure pour l'instant et étudierons les dérivées partielles dans des espaces à un nombre de dimensions quelconques plus loin) continue au moins dans un intervalle où se situe l'abscisse $a$.

	\pagebreak
	\textbf{Définitions (\#\mydef):} 	
	\begin{enumerate}
		\item[D1.] Nous appelons "\NewTerm{pente moyenne}\index{pente moyenne}", ou encore "\NewTerm{coefficient directeur}\index{coefficient directeur}" le rapport de la projection orthogonale de deux points $x_1 \neq x_2$ de la fonction $f$ non nécessairement continue sur l'axe des abscisses et des ordonnées tel que:
		
		Ce qui se représente sous forme graphique de la manière suivante avec une fonction particulière:
	
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[>=stealth',
	                    dot/.style={circle,draw,fill=white,inner sep=0pt,minimum size=4pt},scale=1.25]
	
		    % draw axis lines
		    \draw[->,thick] (-0.5,0) -- ++(11,0) node[below left]{$x$};
		    \draw[->,thick] (0,-0.5) -- ++(0,7) node[below right]{$y=f(x)$};
		    \coordinate (O) at (0,0);
		
		    % create path for function curve
		    \path[thick,red] (-0.3,2) to[out=-25, in=200] coordinate[pos=0.2] (p) coordinate[pos=0.6] (q) (9,5);
		    % fill area
		    \fill[blue, opacity=.1] (p) -| (q);
		    % draw the secant line with fixed length
		    \draw[shorten <=-1.5cm] (p) -- ($ (p)!7.5cm!(q) $) node[below right, pos=0.9]{Sécante};
		    % draw function curve
		    \draw[thick,red] (-0.3,2) to[out=-25, in=200] (9,5);
		
		    % draw all points
		    \node[dot,label={above:$P$}] (P) at (p) {};
		    \node[dot,label={above:$Q$}] (Q) at (q) {};
		    \node[dot] (p1) at (P |- O) {};
		    \node[dot] (p2) at (Q |- O) {};
		    \node[dot] (p3) at (P -| Q) {};
		
		    % draw lines between nodes and place text
		    \draw (P) -- node[left]{$f(x_{1})$} (p1) node[dot,label={below:$x_{1}$}]{};
		    \draw (p2) node[dot,label={below:$x_2=x_{1} + h$}]{} -- (p3);
		    \path (p1) -- node[below]{$h$} (p2);
		
		    % draw blue arrows between nodes
		    \draw[<->,blue,thick] (P) -- node[below]{$\Delta x=h$} (p3);
		    \draw[<->,blue,thick] (Q) -- node[right]{$f(x_{1} + h) - f(x_{1})=\Delta y=\Delta f$} (p3);
		
		    % draw the explanation for the y-value of point Q
		    \draw[help lines] (Q) -- (Q -| {(9.5,0)}) ++(-0.5,0) coordinate (p4);
		    \draw[help lines, <->] (p4) -- node[fill=white,text=black]{$f(x_{1} + h)=f(x_2)$} (p4 |- O);
			\end{tikzpicture}
		\end{figure}
		
		\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	$\Delta$ signifiant "un delta" exprime le fait que nous sous-entendons une différence d'une même quantité.
		\end{tcolorbox}
		
		Nous supposerons comme évident (sans démonstration) que deux fonctions dont les pentes sont les mêmes dans un même intervalle de définition, y sont parallèles (ou confondues).
		
		\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
		Nous démontrerons dans la section de Géométrie Analytique que deux fonctions dont la multiplication des pentes vaut $-1$ sont perpendiculaires.
		\end{tcolorbox}
		
		\item[D2.] Nous appelons "\NewTerm{nombre dérivé en $a$}\index{dériv\'ee}" ou "\NewTerm{pente instantanée}" ou encore "\NewTerm{dérivée première}\index{dériv\'ee premi\'ere}", la limite quand $h$ tend vers $0$ (si elle existe) du rapport de la projection orthogonale de deux points infiniment proches $x_1\neq x_2$ de la fonction $f$ continue (dans le sens qu'elle ne contient pas de "trous") sur l'axe des abscisses $x$ et des ordonnées $y$ tel que:
		
		Cette relation est parfois nommée "\NewTerm{quotient de différence de Newton}\index{quotient de diff\'erence de Newton}".
		
		Une interprétation graphique donne donc bien que $f'(a)$ est le coefficient directeur (la pente de la tangente au point d'abscisse $a$).
		
		\begin{tcolorbox}[title=Remarques,colframe=black,arc=10pt]
		\textbf{R1.} La lettre $\mathrm{d}$ (écrite à la verticale, non en italique comme le recommande la norme internationale ISO 80000-2 : \textit{Quantités et unités - Partie 2 : Signes et symboles mathématiques à utiliser dans les sciences naturelles et la technologie} ) signifie ici un "\NewTerm{différentiel}\index{diff\'erentiel}" et exprime le fait que nous prenons une différence infinitésimale de la quantité concernée.\\
	
		\textbf{R2.} Nous renvoyons le lecteur à la section d'Analyse Fonctionnelle  (page \pageref{limits}) pour la définition de ce qu'est exactement une fonction continue.
		\end{tcolorbox}
		\item[D3.] Soit $f$ une fonction définie sur un intervalle $I$ et dérivable en tout point $a$ de $I$, la fonction qui à tout réel $a$ de $I$ associe le nombre $f'(a)$ est appelée "\NewTerm{fonction dérivée de $f$ sur $I$}" et est notée $f'$.
		
		\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
		Au niveau des notations les physiciens adoptent suivant leur humeur différentes notations possibles pour les dérivées. Ainsi, considérons la fonction réelle $f(x)$ à une variable réele $x$, vous trouverez dans la littérature ainsi que dans le présent livre les différentes notations suivantes pour la dérivée première: 
		
		ou encore en considérant implicitement que $f$ est fonction de $x$ (ceci permet d'alléger un petit peu la tailles des développements):
		
		\end{tcolorbox}
	\end{enumerate}
	
	Nous pouvons de la même manière définir les dérivées d'ordre 2 (dérivée d'une dérivée), les dérivées d'ordre 3 (dérivée d'une dérivée d'ordre 2) et ainsi de suite. Nous rencontrerons par ailleurs très fréquemment de telles dérivées en physique (et même en maths pour l'analyse fonctionnelle).
	
	Précisons que les dérivées d'ordre 2 ont une interprétation très importante en physique et dans le domaines de l'optimisation (\SeeChapter{voir section Méthodes Numériques page \pageref{newton raphson method}}). Effectivement, si le signe de la dérivée première est positif puis devient négatif quand $x$ croît, alors nous devinons facilement que nous parcourons un maximum local d'une fonction (point où la dérivée est nulle) et que si le signe de la dérivée première est négatif puis devient positif quand $x$ croît, alors nous parcourons un minimum local de la fonction (point où la dérivée est aussi nul). En d'autres termes, lorsque la pente change de signe (s'annule en changeant de signe) la fonction passe par un extremum (minimum ou maximum) et la tangente est "horizontale" en ce point: parallèle à l'axe des abscisses. Nous parlons alors de "\NewTerm{point tournant}\index{point tournant}\label{turning point}":
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/algebra/turning_points.jpg}
		\caption{Points tournants mis en évidence}
	\end{figure}
	Mais lorsque la dérivée d'ordre 2 est nulle, cela signifie que la courbure de la fonction s'inverse. On parle alors d'un "\NewTerm{point d'inflexion}\index{point d'inflexion}\label{inflection point}" :
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=3cm,y=2cm]
            \draw[->,color=black] (-0.9,0.) -- (1.9,0.);
            \foreach \x in {}
            \draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt) node[below] {\footnotesize $\x$};
            \draw[->,color=black] (0.,-2.4) -- (0.,2.4);
            \foreach \y in {}
            \draw[shift={(0,\y)},color=black] (2pt,0pt) -- (-2pt,0pt) node[left] {\footnotesize $\y$};
            \draw[color=black] (0pt,-10pt) node[right] {};
            \clip(-0.9,-2.4) rectangle (1.9,2.4);
            \draw[<->,line width=1.2pt,color=red,smooth,samples=100,domain=-0.9:1.9] plot(\x,{3.0*(\x)^(3.0)-5.0*(\x)^(2.0)+(\x)+1.0});
            \draw[line width=1.2pt,color=blue,smooth,samples=100,domain=-0.9:1.9] plot(\x,{9.0*(\x)^(2.0)-10.0*(\x)+1.0});
            \draw [dash pattern=on 1pt off 1pt] (0.11111111164602015,-4.279272269869239E-9)-- (0.11111111164602015,1.0534979423868314);
            \draw (-0.32,0.05) node[anchor=north west] {$\frac{1}{3}$};
            \draw (0.06,0.07) node[anchor=north west] {$\frac{1}{9}$};
            \draw (0.52,0.03) node[anchor=north west] {$\frac{5}{9}$};
            \draw [dash pattern=on 1pt off 1pt] (0.54,0.54)-- (0.54,0.);
            \draw (0.24,0.70) node[anchor=north west] {\tiny{point d'inflection}};
            \draw (0.11,1.25) node[anchor=north west] {\tiny{maximum (point tournant)}};
            \draw (0.8,1.47) node[anchor=north west] {$f'(x)$};
            \draw (1.15,1.21) node[anchor=north west] {$f(x)$};
            \draw (0.98,-0.20) node[anchor=north west] {\tiny{minimum (point tournant)}};
		\end{tikzpicture}
		\caption{Point tournant et point d'inflexion}
	\end{figure}
	Donc chose très importante qu'il faudra toujours avoir en tête (!!!) quand vous poserez que la dérivée d'une fonction est nulle, c'est que nous pouvons avoir une dérivée qui s'annule en un point sans que ce soit un extremum (nous appelons cela un point d'inflexion). Pour contrôler que ce soit bien un maximum, nous pouvons calculer la dérivée seconde\label{second derivative} afin d'éliminer le cas d'un point d'inflexion. Sinon il faut recourir à un tableau de variation pour s'assurer que nous avons affaire à un maximum ou à un minimum comme par exemple avec la fonction $x^3-3x^2+2$:

	\begin{minipage}{\linewidth}\centering
    \begin{variations}
     x      & \mI &    & 0 &    & 2 &    & \pI  \\
     \filet
     f'     & \ga +    & 0    &  -  &  0   & \dr+      \\
     \filet
     \m{f}  & ~  & \c  & \h{~} & \d & ~    &  \c       \\
     \end{variations}
	\end{minipage} 	
	
	Dont le tracé correspondant est :
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/algebra/variation_plot_example_1.jpg}
		\caption[]{Plot of  function $x^3-3x^2+2$}
	\end{figure}
	Un autre exemple avec $f(x)=x^4-4x^3+11$:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/algebra/variation_plot_example_2.jpg}
		\caption[]{Tracé de la fonction $x^4-4x^3+11$}
	\end{figure}
	Avec une table de variation plus détailée:
	
	\begin{center}
		\begin{tikzpicture}[t style/.style={solid}]
		\tkzTabInit[espcl=2]{$x$/.5,$f'(x)$/.5,$f''(x)$/.5,$f(x)$/3} {$-\infty$,$0$,$2$,$3$,$+\infty$}
		\tkzTabLine{,-,0,-,t,-,0,+, }
		\tkzTabLine{,+,0,-,0,+,t,+, }
		
		\node [below] (n1) at (N13){$+\infty$};
		\node [below=1cm](n2) at (N23){$11$};
		\node [below=2cm] (n3) at ([yshift=1em]N33){$-5$};
		\node [above] (n4) at ([yshift=1em]N44){$-16$};
		\node [below] (n5) at (N53){$+\infty$};
		
		\node[below=1ex]at(n2){$ \mathrm{\Sigma.K.} $};
		\node[below=1ex]at([xshift=.5ex]n3){$ \mathrm{\Sigma.K.} $};
		\node[below=1ex]at([xshift=1ex]n4){$ \mathrm{T.E.} $};
		
		\draw[>->] (n1) to [out=-90,in=180] (n2);
		\draw[>->] (n2) to [out=0,in=90] (n3.west);
		\draw[>->] (n3.east) to  [out=-90,in=180] (n4);
		\draw[>->] (n4) to [out=0,in=-90] (n5);
		
		\end{tikzpicture}
	\end{center}

	Voici un exemple d'une fonction avec ses dérivées première et seconde avec Maple 4.00 :

	\texttt{>plot([tanh(x),diff(tanh(x),x),diff(tanh(x),x\$2)],x=-5...5,\\color=[red,green,blue]);}

	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/algebra/derivatives.eps}
		\caption{Tracé de la fonction tangente hyperbolique, sa dérivée première et seconde}
	\end{figure}

	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Deux bonnes fonctions pour se souvenir facilement de la propriété de la dérivée seconde sont $f(x)=x^2$ et $f(x)=-x^2$. Comme vous le savez, la première a un minimum global et la second un maximum global et si vous calculez la dérivée seconde pour la première, vous obtenez une constante positive et une constante négative pour la seconde.
	\end{tcolorbox}
	
	Un point d'inflexion très important dans les affaires et la science est le point d'inflexion de la distribution Normale (\SeeChapter{voir section Statistiques page \pageref{gauss distribution}}).
	
	Étant donné la fonction de la distribution de Gauss univariée:
	
	Les points d'inflexion se produisent lorsque la dérivée seconde $f''(x)=0$. Donc, nous devons le calculer. D'abord:
	
	Dès lors:
	
	Nous la posons comme étant égale à zéro:
	
	Nous avons donc deux situations possibles :
	
	Par conséquent, les deux points d'inflexion se produisent à :
	
	ou un écart-type au-dessus et au-dessous de la moyenne!

	Maintenant, suite à un problème de compréhension de la part d'un lecteur dans un des chapitres de ce livre, précisons une technique utilisée fréquemment par les physiciens. Considérons une dérivée d'ordre $2$ telle que:
	
	Si nous regardons le  $\dfrac{\mathrm{d}}{\mathrm{d}x}$ comme un opérateur différentiel (ce qu'il est!) nous pouvons bien évidemment écrire:
	
	Finalement nous avons:
	
	et donc il vient après simplification par $f(x)$:
	
	sinon quoi nous ne pouvons pas avoir cette égalité si l'opérateur agit explicitement sur une fonction dans une relation mathématique ou physique quelconque.

	Cela peut paraître évident pour certains mais parfois moins pour d'autres... et il était visiblement utile de préciser cela car c'est souvent utilisé dans les sections de Relativité Restreinte, Relativité Générale, Physique Quantique Corpusculaire et Physique Quantique Ondulatoire.

	Indiquons et démontrons maintenant deux propriétés intuitivement évidentes des dérivées et qui nous seront plusieurs fois indispensables pour certaines démonstrations sur ce site (comme par exemple dans le chapitre de méthodes numériques ou ici même...).

	\begin{theorem}
	Considérons d'abord deux nombres réels $a<b$ et $f$ une fonction à valeurs réelles continue sur $[a, b]$ et dérivable sur l'intervalle ouvert  $]a, b[$ telle que $f(a)=f(b)$. Alors nous voulons démontrer qu'il existe bien évidemment au moins un élément $c \in ]a, b[$ tel que $f'(c)=0$ (c'est typiquement le cas des fonctions polynômiales!).
	
	Cette propriété est appelée "\NewTerm{théorème de Rolle}\index{théorème de Rolle}\label{rolle theorem}" et donc explicitement elle montre qu'il existe au moins un élément où la dérivée de $f$ est nulle si en la parcourant nous revenons à la même valeur des images pour deux valeurs distinctes des abscisses, c'est-à-dire qu'il existe au moins un point où la tangente est horizontale.
	\end{theorem}

	\begin{dem}
	Si $f$ est constante, le résultat est immédiat... Dans le cas contraire, comme $f$ est continue sur l'intervalle fermé borné $[a, b]$ elle admet au moins un minimum global ou maximum global compte tenu que nous nous basons sur l'hypothèse que et que $f$ n'est pas constante. L'extrema est atteint en un point $c$ appartenant à l'intervalle ouvert $] a, b [$ (le fait de prendre l'intervalle ouvert permet dans certains cas d'éviter d'avoir un extrema à nouveau en $a$ ou en $b$).
	
	Supposons comme premier cas que $f(c)$ est maximum global. La dérivée de la fonction $f$ entre $c$ et un deuxième point $a$ alors un signe connu.
	\begin{itemize}
		\item Pour $h$ strictement positif et tel que $c + h$ appartienne à l'intervalle $[a, b]$:
		
		En considérant la limite quand $h$ tend vers $0$, la valeur de la dérivée $f'(c)$ est négative.
	
		 \item Pour $h$ strictement négatif et tel que $c + h$ appartienne à l'intervalle $[a, b]$:
		
		En considérant la limite quand $h$ tend vers $0$, le nombre dérivé $f'(c)$ est positif.
	\end{itemize}
	Au bout du compte, la dérivée de $f$ est nulle au point $c$.
	
	La démonstration est analogue si $f(c)$ est un minimum global, avec les signes des dérivées qui sont les opposés.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Maintenant, considérons deux réels $a,b$ et $f(x)$ une fonction continue sur $[a, b]$ (c'est-à-dire $\mathcal{C}([a,b])$) et dérivable sur $] a, b [$.

	\begin{theorem}
	Alors, nous nous proposons de montrer qu'il existe au moins un réel $c \in ]a,b[ $ tel que:
	
	Ce qui peut aussi s'écrire sous la forme suivante:
	
	avec $s\in ]0,1[$.
	
	Puisque le terme de gauche représente une augmentation finie du terme de droite, alors ce résultat est nommé "\NewTerm{théorème de la valeur moyenne}\index{th\'eor\'eme de la valeur moyenne}" ou mieux "\NewTerm{théorème des incréments finis}\index{th\'eor\'eme des incr\'ements finis}".
	
	Géométriquement cela signifie qu'en au moins un point $c$ du graphe de la fonction $f(x)$, il existe une tangente de coefficient directeur:
	
	Graphiquement cela donne:
	\begin{figure}[H]
	\centering
	\includegraphics{img/algebra/mean_value_theorem.eps}
	\caption{Représentation graphique du théorème de Rolle}
	\end{figure}
	
	\end{theorem}
	\begin{dem}
		Nous avons premièrement:
		
		parce que la pente de $h(x)$ est trivialement:
		
		et comme nous devons avoir $f(a)$ lorsque $x=a$ il s'ensuite la relation donnée précédemment.
		
		Ensuite, pour montrer qu'une telle valeur $c$ existe, l'idée est de ramener les deux points $a$ et $b$ dans la même ordonnée ce qui nous ramène au théorème de Rolle et pour cela, nous définissons une fonction $g (x)$ par :
		
		qui est telle qu'en effet $g(a)=g(b)$ ... et est dans ce cas égal à $0$ (mais cette valeur n'est pas pertinente).
	
		Par conséquent, le théorème de Rolle discuté ci-dessus indique qu'il existe un point entre $a$ et $b$ où la dérivée de $g(x)$ est nulle telle que $g'(c)=0$. Et en voyant ça :
		
		Nous avons alors
		
		Dès lors après simplification:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
	\end{dem}
	Puisque le terme de gauche représente un accroissement fini du terme de droite, alors ce résultat est appelée "\NewTerm{théorème des accroissements finis}\index{th\'eor\'eme des accroissements finis}"" (TAF). 
	
	A l'aide de ce petit théorème et des outils mathématiques introduits précédemment, nous pouvons construire un petit théorème fort utile et puissant en physique.
	
	\textbf{Définition (\#\mydef):} Nous appelons "\NewTerm{règle de L'Hôpital}\index{r\'egle de L'H\-opital}" (également appelée "règle de l'Hospital"\label{Hospital rule} ou "règle de Bernoulli") le procédé qui utilise la dérivée dans le but de déterminer les limites difficiles à calculer de la plupart des quotients et qui apparaissent souvent en physique.

	\begin{dem}
	Considérons deux fonctions $f(x)$ et $g(x)$ et telles que $f(a)=g(a)=0$ alors nous pouvons écrire:
	
	Alors selon la définition de la dérivée:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}		
	\end{dem}
	Nous pouvons généraliser ce résultat précédent initialement basé sur la contrainte un peu trop forte:
	
	\begin{dem}
	Rappelons donc que selon le théorème des accroissements finis, si $f(x)$ est dérivable sur un intervalle $]a, b[$ et continue sur $[a, b]$ alors il existe un réel $c$ dans l'intervalle $[a, b]$ tel que:\
	
	Si le théorème se vérifie pour deux fonctions satisfaisant aux mêmes contraintes alors nous avons deux fonctions telles que:
	
	Si $g'(c)$ est non nul nous avons alors tout à fait le droit d'écrire le rapport (certains appellent cela le "\NewTerm{théorème des accroissements fini généralisé}\index{th\'eor\'eme des accroissements fini g\'en\'eralis\'e}"...):
	
	ce qui sans perdre en validité tant que $c$ est dans l'étau $[a, x]$ peut s'écrire:
	
	Ainsi, lorsque $x \rightarrow a$ ce qui implique que l'étau $[a, x]$ se referme et donc $c \rightarrow a$, nous avons:
	
	Ainsi, nous venons de prouver quand dans la démonstration précédente de la règle de l'Hôpital la relation:
	
	que nous avions est vraie en toute généralité et qu'il n'est pas nécessaire que $f(a)=g(a)=0$ soit vrai pour que le résultat soit juste!
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Le résultat peut être généralisé, toujours sous la même condition, avec :
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemples:}\\\\
	E1. Nous voulons calculer:
	
	Il y a deux façons de résoudre ça ! On peut d'abord factoriser :
	
	Soit en utilisant la règle de l'Hôpital (nous voyons que l'hypothèse $f(2)=g(2)=0$ est satisfaite !) :
	
	Mais nous voyons que nous ne pouvons pas appliquer la règle de l'Hôpital deux fois car les hypothèses ne seraient pas remplies.\\
	
	E2. Nous voulons calculer:
	
	Nous ne pouvons évidemment pas appliquer la règle de l'Hôpital puisque $f(+\infty)\neq g(+\infty) \neq 0$. La technique de factorisation échouera également. Nous devons utiliser une astuce de puissance.
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	On divise par la puissance du plus grand dénominateur : 
	
	\end{tcolorbox}
	Nous mettons également en garde contre la précédente "règle de division par la puissance la plus élevée" qui est applicable dans le scénario spécifique où nous prenons une limite à l'infini ou moins l'infini ET l'expression limite est de la forme $P(x)/Q(x)$ où $P$ et $Q$ sont des combinaisons linéaires de puissances de $x$ et la plus grande puissance de $x$ apparaissant dans l'expression entière est non négative.
	
	\subsubsection{Différentielles}
	
	Nous avons indiqué précédemment ce qu'était un différentiel $\mathrm{d}$. Mais il existe en fait plusieurs types de sortes de différentielles d'une fonction (remarquez que nous distinguons le genre masculin et féminin du terme):
	\begin{enumerate}
		\item Les différentielles
		\item Les différentielles partielles
		\item Les différentielles totales exactes 
		\item Les différentielles totales inexactes 
	\end{enumerate}
	Rappelons que nous appelons "\NewTerm{différentiel $\mathrm{d}f$}\label{differential}" d'une fonction univariée à une variable la relation donnée par (voir texte précédent):
	
	Cependant, pour exprimer l'effet d'un changement de toutes les variables d'une fonction $f$ de plusieurs variables (ie multivariée), nous devons utiliser une autre type de différentielle que nous appelons la "\NewTerm{différentielle totale}\index{diff\'erentielle totale}" (dérivée en deux sous-familles: différentielle totale exacte et différentielle totale inexacte que nous détaillerons plus loin).
	
	Soit par exemple, une fonction $f(x, y)$ des deux variables $x$ et $y$. L'accroissement $\mathrm{d}f$ de la fonction $f$, pour un accroissement fini de $x$ à $x+\Delta x$ et $y$ à $y+\Delta y$ est évidemment donné par:
	
	Nous pouvons aussi écrire:
	
	Ou encore:
	
	Pour des accroissements infiniment petits de $x$ et $y$:
	
	Intéressons-nous dès lors aux deux termes au passage à la limite:
	
	Le premier terme de gauche, nous le voyons, ne donne finalement que la variation en $x$ de la fonction $f(x, y)$ en ayant $y$ constant sur la variation. Nous notons cela dès lors (si la connaissance des variables constantes est triviale, nous ne les indiquons plus):
	
	et de même:
	
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Quand une variable est fixée pour étudier la variation de l'autre, certains auteurs ou professeurs des anciennes générations aiment à dire: "toutes choses égales par ailleurs $f$ varie en fonction de ... de façon ....". Bref, c'est un usage que l'on retrouve dans d'autres domaines (comme les régressions linéaires à plusieurs variables explicatives) mais qui se perd...
	\end{tcolorbox}
	Les deux expressions:
	
	sont ce que nous appelons des "\NewTerm{différentielles partielles}\index{diff\'erentielles partielles}" ou plus simplement "\NewTerm{dérivée partielle}\index{d\'eriv\'ee partielle}\label{partial derivative}" (dont le cas d'application pratique le plus simple et probablement le plus intéressant et pédagogiquement pertinent disponible à ce jour sur l'entier du site est le modèle d'approvisionnement de Wilson avec rupture présenté dans la section de Techniques De Gestion).
	
	Nous avons alors:
	
	qui est la "\NewTerm{différentielle de $f$}". Les thermodynamiciens parlent souvent eux de la "\NewTerm{différentielle totale exacte de $f$}\label{total exact differential}" ou plus simplement "\NewTerm{différentielle exacte de $f$}" ou encore de "\NewTerm{dérivée totale}" mais aussi de "\NewTerm{dérivée extérieure}\index{d\'eriv\'ee ext\'erieure}".
	
	La relation précédente est un cas particulier de ce que les mathématiciens appellent en toute généralité une "\NewTerm{forme différentielle}\index{forme diff\'erentielle}":
	
	nous y reviendrons un peu plus loin... 
	
	Il est d'usage de noter:
	
	donc sous forme d'un champ vectoriel.
	
	Il est important de se rappeler de la forme de la différentielle totale car nous la retrouverons partout dans des opérateurs particuliers en physique, dans la mécanique des fluides, dans la thermodynamique, etc.
	
	Géométriquement, les dérivées partielles peuvent être interprétées comme suit: la fonction $f(x, y)$ définit une surface dans $\mathbb{R}$, dont l'intersection avec la plan $y=y_0$ est une courbe $f(x,y_0)$.
	
	La dérivée partielle $\partial_x f$  est alors la pente de cette courbe en tout point $x$. Nous avons alors naturellement la fonction suivante pour la pente au point $(x_0,y_0)$:
	
	De la même manière, la tangente à la courbe $f(x_0,y)$ sera donnée par:
	
	Le plan localement tangent au point $(x_0,y_0)$ déterminé pas ses deux tangentes est alors donné par:
	
	En réorganisant les termes tel que:
	
	Nous reconnaissons:
	
	Ainsi, par exemple, la surface représentée par la fonction:
	
	est représentée ci-dessous avec les deux tangentes passant par le point:
	
	et dont les équations respectives sont:
	
	et:
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/algebra/total_derivative_two_tangents.jpg}
		\caption[]{Les deux tangentes de la fonction au point d'intérêt}
	\end{figure}
	Nous avons le "\NewTerm{tangent plane}\index{tangent plane}" en ce point qui est alors donné par:
		
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/algebra/total_derivative_tangent_plane.jpg}
		\caption[]{Les deux tangentes de la fonction au point d'intérêt avec la plan tangent}
	\end{figure}
	
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	De la même manière, pour une fonction de plus de deux variables, par exemple $f(x,y,z)$, la différentielle totale $\mathrm{d}f$ est:
	
	Dans l'équation ci-dessus, la différentielle $\mathrm{d}f$ a été calculée à partir de l'expression de la fonction $f$. Puisqu'il existe une fonction $f$ qui vérifie l'expression de $\mathrm{d}f$, la différentielle $\mathrm{d}f$ est dite alors aussi "différentielle totale exacte".
	\end{tcolorbox}	
	Profitons pour faire une indication importante sur l'utilisation des dérivées partielles par les physiciens (et donc dans les nombreux chapitres y relatifs du site). Nous avons vu plus haut que si $f$ dépend de deux variables $x, y$ nous avons:
	
	et s'il ne dépend que d'une variable nous avons alors:
	
	Et alors dans le cas univarié:
	
	raison pour laquelle les physiciens mélangent allègrement les deux notations...
	
	Maintenant, il faut cependant savoir qu'il existe également des différentielles totales exactes qu'aucune fonction ne vérifie. Dans ce cas, nous parlons de "\NewTerm{différentielle totale inexacte}\index{diff\'erentielle totale inexacte}\label{total inexact derivative}" et pour déterminer si une différentielle totale est exacte ou inexacte, nous utilisons les propriétés des dérivées partielles (cas très important en thermodynamique!!!).
	
	Soit la fameuse forme différentielle générale (cela fait appel à de la géométrie différentielle):
	
	où $M (x, y)$ et $N (x, y)$ sont des fonctions des variables $x$ et $y$. Si $\mathrm{d}z$ est une différentielle totale exacte, alors:
	
	Il faut donc que in extenso:
	
	ou encore, en effectuant une seconde dérivation, que: 
	
	pour que la forme différentielle, soit une différentielle totale exacte.
	
	Avant de continuer, nous avons besoin d'un résultat donné par le "\NewTerm{théorème de Schwarz}\index{th\'eor\'eme de Schwarz}\label{Schwarz theorem}" (mais qui a été démontré à la fin du 17ème siècle par un des frères Bernoulli) qui s'énonce de la manière suivante:
	
	\begin{theorem}
	Soit une fonction $f$, si:
	
	sont continues alors nous avons (il faut vraiment vérifier que ce soit le cas!) un résultat très important dans la pratique:
	
	pour tout $(x_0,y_0)\in U$ où $U$ est le domaine de définition où $f$ est continue (et donc dérivable).
	\end{theorem}
	\begin{dem}
	Nous considérons l'expression:
	
	Posons:
	
	Nous avons alors:
	
	Par le théorème des accroissements finis:
	
	avec $s,t \in ]0,1[$. En reprenant les définitions de $g$ et $w$ nous obtenons:
	
	en appliquant à nouveau le théorème des accroissements finis aux deux membres entre parenthèses nous trouvons:
	
	avec $\tilde{s},\tilde{t} \in ]0,1[$. Pour finir nous voyons que nous avons:
	
	et par continuité lorsque $k,h\rightarrow 0$, nous avons:
	
	Plus simplement écrit:
	
	Donc si $f$ s'exprime sous forme différentielle totale exacte alors les différentielles croisées sont égales (la réciproque n'est pas forcément vraie).
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Par récurrence sur le nombre de variables nous pouvons démontrer le cas général (c'est long mais c'est possible, nous le ferons si besoin il y a...). 
	
	Donc finalement pour en revenir à notre problème initial, nous avons donc:
	
	Ce qui nous donne finalement la "\NewTerm{condition de Schwarz}\index{condition de Schwarz}":
	
	C'est donc la condition que doit satisfaire une forme différentielle pour être une différentielle totale exacte et la condition qu'elle ne doit pas satisfaire pour être une différentielle totale inexacte!!! C'est une propriété très important pour l'étude de la Thermodynamique!
	
	Afin de ne pas confondre les deux types de différentielles, nous utilisons le symbole $\delta$ pour représenter une différentielle totale inexacte:
	
	et $\mathrm{d}$ pour une différentielle totale exacte:
	
	La distinction est extrêmement importante car seules les différentielles totales exactes qui satisfont donc:
	
	ont une intégrale qui ne dépend \underline{que} des bornes d'intégration (puisque toutes les variables changent en même temps). Dès lors les différentielles totales inexactes dépendent \underline{que} des bornes d'intégration, ce qui signifie que:
	
	et donc sur un chemin fermé on peut avoir :
	
	Alors que pour les différentielles totales exactes:
	
	soit (voir la démonstration détaillée plus loin lorsque nous traiterons des intégrales curvilignes):
	
	Autrement dit, la variation d'une fonction dont la différentielle est totale exacte, ne dépend pas du chemin suivi, mais uniquement des états initiaux et finaux car elle s'exprime comme le gradient d'une fonction (voir la démonstration par l'exemple dans la section d'Électrostatique quand nous vérifions que la différence de potentiel est indépendante du chemin). Nous appelons une telle fonction qui satisfait à une différentielle totale exacte en physique, une \NewTerm{fonction d'état}\index{fonction d'\'etat}" et en mathématique une "\NewTerm{fonction holomorphe}\index{fonction holomorphe}" (\SeeChapter{voir la section d'Analyse Complexe page \pageref{holomorphic functions}}), c'est-à-dire une fonction dont la valeur ne dépend que de l'état présent et futur, et non de son histoire.
	
	Cette distinction est très importante et particulièrement en thermodynamique où il convient de déterminer si une quantité physique est une différentielle totale exacte (une "fonction d'état" donc) ou non afin de savoir comment évoluent les systèmes.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemple:}\\\\
	Un exemple important de forme différentielle en thermodynamique, est le travail élémentaire $\delta W$ d'une force exercée sur un corps en mouvement dans le plan $\text{O}xy$, nous avons:
	
	Les forces $F_x,F_y$ et ne dérivent pas nécessairement d'un même potentiel $U(x, y)$ tel que:
	
	dès lors  $\delta W$ est donc une différentielle totale inexacte!
	\end{tcolorbox}
	
	Enfin quelque chose peut-être important à garder à l'esprit ?! :
	\begin{itemize}
		\item Supposons $y=f(x,w)$, tandis qu'à leur tour $x=g(t)$ et $w=h(t)$. Comment $y$ change-t-il lorsque $t$ change ?
		
		Notez que si nous supposons $y=f(x,w(x))$ (une dépendance directe!) mais avec $x$ n'ayant aucune dépendance implicite, la relation ci-dessus devient\label{tota differential with direct dependency}:
		
		
		\item Supposons $y=f(x,w)$, tandis qu'à leur tour $x=g(t,s)$ et $w=h(t,s)$. Comment $y$ change-t-il lorsque $t$ change ?
		
	\end{itemize}
	Notez que le premier point est la "\NewTerm{dérivée totale}\index{d\'eriv\'ee totale}" alors que le deuxième point est la "\NewTerm{dérivée partielle totale}\index{d\'eriv\'ee partielle totale}".
	 
	 \subsubsection{Dérivée usuelles}\label{usual derivatives}
	 
	 Nous allons démontrer ici les dérivées les plus fréquentes (une petite trentaine) que nous puissions rencontrer en physique théorique et mathématique ainsi que certaines de leurs propriétés (en fait, nous allons toutes les appliquer dans les sections relatives à la Mécanique, l'Ingénierie, l'Atomistique, les Mathématiques Sociales, etc.). La liste est pour l'instant non exhaustive mais les démonstrations étant généralisées, elles peuvent s'appliquer à un grand nombre d'autres cas similaires (que nous appliquerons/rencontrerons tout au long de ce site).
	 \begin{enumerate}
	 	\item Dérivée de $f(x)=x^n$:
	 	
	 	Partons d'abord d'un cas particulier, la dérivée de $x^3$:
	 	
		La dérivée de la fonction cubique est donc $3a^2$.
		
		On peut généraliser ce résultat pour tout entier positif ou négatif $n$ et on verra que la fonction $f$ définie sur $\mathbb{R}$ par $f(x)=x^n$ est dérivable et que sa dérivée $f'$ est donné par $f'(x)=nx^{n-1}$.
		
		Effectivement:
		 
		 Ainsi, nous avons (quelques exemples peuvent être utiles pour comprendre la portée de ce résultat):
		 
		 Nous voyons donc qu'en ayant déterminé la dérivée d'une fonction de la forme $x^n$, nous avons également déterminé la dérivée de toute fonction qui est mise sous cette forme tel que:
		 
		 Cependant, les fonctions:
		 
		 ne sont pas dérivables en $x=0$ puisque la fonction n'y est plus définie (division par zéro). De plus, en ce qui concerne la fonction comportant la racine (puissance non entière), la dérivée n'est pas définie dans $\mathbb{R}_{-}^{*}$.
		 
		 \begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
		Il ressort immédiatement des résultats ci-dessus que si $f(x)=x^n$ pour $n$ étant un entier positif $f^{(k)}(x) = x^{n-k}\ n!/(n-k)!$ pour $k\leq n$.
		\end{tcolorbox}	
		 
		 \item Dérivée de $f(x)=c^{te}$:
		 
		 Le résultat précédent donne un résultat immédiat intéressant pour des fonctions constantes telles que :
		 
		 il n'est alors pas difficile de déterminer que la dérivée est simplement :
		 
		 La dérivée de toute fonction constante est donc nulle (il est important de se souvenir de ce résultat lorsque nous étudierons les propriétés des intégrales) !!!
		 
		 \item Dérivée de $f(x)=\cos(x)$:
		 
		 Soit donc $a$ un réel quelconque fixé, alors (attention! il est utile de connaître les relations trigonométriques remarquables que nous démontrons dans la section de Trigonométrie):
		 
		 Puisque en utilisant la règle de l'Hospital (ou en voyant que $\sin(x)$ peut être assimilié à une droite $f(x)=x$ à proximité de $x=0$):
		 
		 Donc pour résumer:
		 
		 
		 \item Dérivée de $f(x)=\sin(x)$:
		 
		 Soit donc $a$ un réel quelconque fixé, alors  (attention! il est utile de connaître les relations trigonométriques remarquables que nous démontrons dans la section de Trigonométrie):
		 
		 Puisque en utilisant la règle de l'Hospital (ou en voyant que $\sin(x)$ peut être assimilié à une droite $f(x)=x$ à proximité de $x=0$):
		 
		 Donc pour résumer:
		 
		 
		 \item Dérivée de $f(x)=\log_b(x)$:
		 
		 Nous commençons en écrivant:
		 
		 Dès lors:
		 
		 Ainsi:
		 
		 Multiplions et divisons par  $x$ l'expression figurant dans le membre droit de la dernière égalité:
		 
		 Désignons la quantité $\dfrac{\Delta x}{x}$ par $\alpha$. Il est évident que quand $\alpha \rightarrow 0$ quand $\Delta x$ pour un $x$ donné. Par conséquent:
		 
		 Or, nous retrouvons ici une autre provenance historique de la constante d'Euler (voir la section d'Analyse fonctionnelle page \pageref{Euler number} pour la preuve) où:
		 
		 Ainsi:
		 
		 Un cas particulier important est le cas où $b = e$. Nous avons alors:
		  
		 
		 \item Dérivée d'une somme de fonctions:
		 
		 Soient $u$ et $v$ deux fonctions. La fonction somme $s=u+v$ est dérivable sur tout intervalle où $u$ et $v$ sont dérivables, sa dérivée est la fonction $s'$ somme des fonctions dérivées $u'$ et $v'$ de $u$ et $v$.
		 
		 Soit $a$ un nombre réel et $u,v$ deux fonctions définies et différentiables sur $a$ :
		 
		 
		 Donc la dérivée d'une somme est la somme des dérivées.
		 
		 Ce résultat se généralise pour une somme d'un nombre quelconque fixé de fonctions.
		 
		 \item Dérivée d'un produit de fonctions (nommé souvent "\NewTerm{règle du produit}\index{r\'egle du produit}"):
		 
		 Soient $u$ et $v$ deux fonctions. La fonction produit $p=uv$ est dérivable sur tout intervalle où $u$ et $v$ sont dérivables, et $p'$ sera la notation pour la dérivée du produit. Voyons maintenant quelle est son expression.
		 
		 Soient $a$ un réel fixé et $u,v$ deux fonctions définies et dérivables en $a$:		 
		 
		 Nous rajoutons à cette dernière relation deux termes dont la somme est nulle tels que:
		 
		D'où le résultat fameux:
		
		Mais il existe une formulation plus générale que la dérivée première d'un produit:
		 \begin{theorem}
		 	Considérons pour cela toujours nos deux fonctions $u$ et $v$, $n$ fois dérivables sur un intervalle $I$. Alors le produit $uv$ est $n$ fois dérivable sur $I$ et:
		 	
		 	et ceci constitue la "\NewTerm{règle de différentiation de Leibniz pour le produit}\index{r\'egle de diff\'erentiation de Leibniz pour le produit}\label{Leibniz differentiation rule for products}" que nous avons utilisée dans la section de Calcul Algébrique pour l'étude des polynômes de Legendre (qui nous sont eux-mêmes indispensables pour l'étude de la Chimie Quantique).
		 	
		 	La démonstration de cette expression est très proche de celle faite pour le binôme de Newton (\SeeChapter{voir section Calcul Algébrique page \pageref{binomial theorem}}).
		 \end{theorem}
		 \begin{dem}
		 	Soit:
		 	
		 	D'autre part:
		 	
		 	La relation est ainsi bien initialisée.
		 	
		 	La démonstration se fait par récurrence. Ainsi, le but est de montrer que pour $\forall n \geq 0 \in \mathbb{N}$ que si:
		 	
		 	alors:
		 	
		 	Nous avons donc:
		 	
		 	Nous allons procéder à un changement de variable dans la première somme pour ne plus avoir le terme en $k + 1$. Nous posons pour cela $j=k+1$:
		 	
		 	Si nous revenons à la lettre $k$, nous avons donc:
		 	
		 	Nous avons donc:
		 	
		 	Nous voulons réunir les deux sommes. Pour cela, nous écartons les termes en trop dans chacune d'elles:
		 	
		 	Ce qui donne donc:
		 	
		 	D'après la formule de Pascal (\SeeChapter{voir section Probabilities page \pageref{pascal formula}}), nous avons:
		 	
		 	Donc:
		 	
		 	Mais nous avons en même temps:
		 	
		 	Dès lors:
		 	
		 	\begin{flushright}
				$\blacksquare$  Q.E.D.
			\end{flushright}
		 \end{dem}
		 
		 \item Dérivée d'un ratio de deux fonctions différerntiables (nomméel a  "\NewTerm{règle du quotient}\index{r\'egle du quotient}\label{quotient rule}"):
		 
		 Soit $f(x)=g(x)/h(x)$. L'application de la définition de la dérivée et des propriétés des limites donne la preuve suivante :
		 
		 Nous pouvons également faire la preuve en utilisant la différenciation implicite. En effet, soit $f(x)=g(x)/h(x)$, donc $g(x)=f(x)h(x)$. La règle du produit prouvée juste plus haut nous donne alors :
		
		En résolvant pour $f'(x)$ et en substituant $f(x)$ donne :
		
		 
		 \item Dérivée d'une fonction composite univariée:
		 
		 Considérons la fonction composée $f=g \circ u=g(u(x))$ de deux fonctions $g$ et $u$ différentiables, la première dans $u(a)$, la seconde dans $a$. On a donc :
		 
		 Posons maintenant $k=u(a+h)-u(a)$ alors nous avons :
		  
		 Continuons notre développement précédent :
		  
		 Ainsi, la dérivée d'une fonction composite est donnée par la dérivée de la fonction, multipliée par le "\NewTerm{dérivée intérieure}\index{d\'eriv\'ee int\'erieure}". De plus, ce type de dérivée est très importante car souvent utilisée en physique sous le nom de "\NewTerm{dérivation (univariée) en chaîne}\index{dérivation (univariée) en chaîne}" ou simplement "\NewTerm{règle de dérivation en chaîne}\index{r\'egle de d\'erivation en cha\-ine}".
		 
		 Voyons de quoi il s'agit. La relation précédente obtenue peut être réécrite d'une autre manière plus courante :
		 
			Ou typiquement lorsque nous avons plusieurs fonctions qui se multiplient :
		 
		 Nous parlons alors parfois de "\NewTerm{règle de puissance en chaîne}\index{r\'egle de puissance en cha\-ine}".
		 
		 \item Dérivée d'une fonction composée bivariée :
		 \begin{theorem}
		 La dérivée en $t$ d'une fonction composite $z=f(x(t),y(t))$ est donnée par :
		  
		 Nous supposons dans ce théorème et ses applications que $x=x(t)$ et $y=y(t)$ ont des dérivées premières en $t$ et que $z=f(x,y)$ a des dérivées continues du premier ordre dans un cercle ouvert centré sur $(x(t),y(t))$.
		 \end{theorem}
		 \begin{dem}
		 	Nous fixons $t$ et posons $(x,y)=(x(t),y(t))$. Nous considérons $\Delta t$ non nul et si petit que $(x(t+\Delta t),y(t+\Delta t))$ est dans le cercle où $f$ a des dérivées premières continues et posons $\Delta x=x (t+\Delta t)-x(t)$ et $\Delta y=y(t+\Delta t)-y(t)$. Alors par définition de la dérivée :
		 	
			Nous pouvons appliquer le "théorème de la valeur moyenne" qui dit que (voir la preuve plus loin lors de notre étude du Calcul Intégral):
						
			 à l'expression dans le premier ensemble de crochets à droite de la dernière égalité ci-dessus où $y$ est constant et à l'expression dans le deuxième ensemble de crochets où $x$ est constant. Nous en concluons qu'il existe un nombre $c_1$ entre $x$ et $x+\Delta x$ et un nombre $c_2$ entre $y$ et $y+\Delta y$ tels que :
			
			Nous ajoutons les deux relations ci-dessus et divisons par $\Delta t$ pour obtenir :
			
			Les fonctions $x=x(t)$ et $y=y(t)$ sont continues en $t$ car elles ont des dérivées en ce point. Par conséquent, comme $\Delta t\rightarrow 0$, les nombres $\Delta x$ et $\Delta y$ tendent tous les deux vers zéro et le cercle incluant les constantes $c_i$ s'effondre jusqu'au point $(x,y)$, Comme les dérivées partielles de $f$ sont continues, le terme $\partial_x f(c_1,y+\Delta y)$ tend vers $\partial_x f(x,y)$ et le terme $\partial_y f(x,c_2) $ tend vers $\partial_y f(x,y)$ comme $\Delta t\rightarrow 0$. De plus:
		
		quand $\Delta t\rightarrow 0$, ainsi la relation ci-dessus devient:
			
			 qui est parfois nommée "\NewTerm{règle en chaîne multivariée}\index{r\'egle en cha\-ine multivari\'ee}\label{multivariate chain rule}" (mais en réalité il s'agit seulement du cas bivarié...) et est une relation trèeeees importante pour l'étude de la physique!
			 
			 Cette dernière relation est parfois écrite de la manière suivante: 
			 
		 	\begin{flushright}
			$\blacksquare$  Q.E.D.
			\end{flushright}
		 \end{dem}
		 
		 \item Dérivée d'une fonction réciproque:
		 \begin{theorem}
		 	Si la fonction $f$ est continue, strictement monotone sur un intervalle $I$, dérivable sur $I$, alors la fonction réciproque est dérivable sur l'intervalle $f(I)$ et admet pour fonction dérivée:
		 	
		 \end{theorem}
		 \begin{dem}
		 	En effet nous pouvons écrire :
		 	
		 	C'est-à-dire (application identité) :
		 	
		 	Par application de la dérivation des fonctions composées:
		 	
		 	d'où:
		 	
		 	Pour une variable $x$, nous poserons pour la dérivée de la fonction réciproque:
		 	
		 	\begin{flushright}
			$\blacksquare$  Q.E.D.
			\end{flushright}
		 \end{dem}
		 \item Dérivée de la fonction $\arccos (x)$:
		 
		 	En utilisant le résultat précédent de la fonction réciproque et de la dérivée de $\cos (x)$  démontré plus haut, nous pouvons calculer la dérivée de la fonction $\arccos (x)$:
		 	
		 	
		 	\item Dérivée de $\arcsin (x)$:
		 
		 	En utilisant le résultat précédent de la fonction réciproque et la dérivée de $\sin (x)$, nous pouvons calculer la dérivée de la fonction $\arcsin (x)$:
		 	
		 	
		 	\item Dérivée d'un quotient de deux fonctions:
		 	
		 	Considérons que la fonction:
		 	
		 	est dérivable sur tout intervalle où les fonctions $u$ et $v$ sont dérivables et où la fonction $v$ est non nulle.
		 	
		 	La fonction $f$ peut être considérée comme le produit de deux fonctions: la fonction $u$ et la fonction $1/v$. Une produit de deux fonctions est dérivable si chacune d'elle est dérivable, il faut donc que la fonction $u$ soit dérivable et que la fonction $1/v$soit également dérivable ce qui est le cas quand $v$ est dérivable non nulle.
		 	
		 	
		 	\item Dérivée de la fonction $\tan(x)$:
		 	
		 	Par définition (\SeeChapter{voir section Trigonométrie page \pageref{definition trigonometric functions}}), pour $\forall x \neq k\dfrac{\pi}{2},k\in \mathbb{Z}$ nous avons:
		 	
		 	et en appliquant donc la dérivée d'un quotient vue précédemment, nous avons:
		 	
		 	ou:
		 	
		 	
		 	\item Dérivée de la fonction $\cot(x)$:
		 	
		 	Par définition (\SeeChapter{voir section Trigonométrie page \pageref{definition trigonometric functions}}), $\forall x \neq k\pi,k\in \mathbb{Z}$:
		 	
		 	et en appliquant donc la dérivée d'un quotient vue précédemment, nous avons:
		 	
		 	ou:
		 	
		 	
		 	\item Dérivée de la fonction $\arctan(x)$:
		 	
		 	Nous utilisons les propriétés dérivées des fonctions réciproques démontrées plus haut:
		 	
		 	
		 	\item Dérivée de la fonction $\text{arccot}(x)$:
		 	
		 	Selon la même méthode que précédemment:
		 	
		 	
		 	\item Dérivée de la fonction $e^x$:
		 	
		 	Nous démontrerons lors de notre étude des Méthodes Numériques (voir section du même nom \pageref{euler number computation}) que le "nombre d'Euler" peut être calculé selon la série:
		 	
		 	qui converge sur $\mathbb{R}$. En dérivant terme à terme cette série qui converge, il vient:
		 	
		 	Ainsi l'exponentielle est sa propre dérivée. Ainsi, nous pouvons nous permettre d'étudier les dérivées de quelques fonctions trigonométriques hyperboliques (\SeeChapter{voir section Trigonométrie page \pageref{hyperbolic trigonometry}}) et d'autres nombreux cas spécifiques (voir tous les autres chapitres de ce livre).
		 	
		 	\item Dérivée de la fonction $\sinh(x)$:
		 	
		 	Rappelons que (\SeeChapter{voir section Trigonométrie page \pageref{definition trigonometric functions}}):
		 	
		 	Donc trivialement:
		 	
		 	
		 	\item Dérivée de la fonction $\cosh(x)$:
		 	
		 	Rappelons que (\SeeChapter{voir section Trigonométrie page \pageref{definition trigonometric functions}}):
		 	
		 	Donc trivialement:
		 	
		 	
		 	\item Dérivée de la fonction $\tanh(x)$:
		 	
		 	Rappelons que (\SeeChapter{voir section Trigonométrie page \pageref{definition trigonometric functions}}):
		 	
		 	Donc en appliquant la dérivée d'un quotient nous obtenons:
		 	
		 	ou:
		 	
		 	
		 	\item Dérivée de la fonction $\coth(x)$:
		 	
		 	Rappelons que (\SeeChapter{voir section Trigonométrie page \pageref{definition trigonometric functions}}):
		 	
		 	Donc en appliquant la dérivée d'un quotient nous obtenons:
		 	
		 	
		 	\item Dérivée de la fonction $\text{arcsinh}(x)$:
		 	
		 	Nous appliquons les propriétés des dérivées des fonctions réciproques démontré plus haut:
		 	
		 	Mais (\SeeChapter{voir section Trigonométrie page \pageref{definition trigonometric functions}}):
		 	
		 	et dès lors:
		 	
		 	Etant donné que $\cosh(x)$ ne prend que des valeurs positives, nous avons:
		 	
		 	Donc finalement:
		 	
		 	
		 	\item Dérivée de la fonction $\text{arccosh}(x)$:
		 	
		 	Nous appliquons les propriétés des dérivées des fonctions réciproques:
		 	
		 	Or selon la même méthode que précédemment (\SeeChapter{voir section Trigonométrie page \pageref{definition trigonometric functions}}):
		 	
		 	d'où: 
		 	
		 	Etant donné que $\text{arccosh}(x)$ ne prend que des valeurs positives il en est de même pour $\sinh(x)$, nous avons alors:
		 	
		 	Donc finalement:
		 	
		 	
		 	\item Dérivée de la fonction $\text{arctanh}(x)$:
		 	
		 	En appliquant à nouveau les propriétés des dérivées des fonctions réciproques:
		 	
		 	
		 	\item Dérivée de la fonction  $\text{arccoth}(x)$:
		 	En appliquant à nouveau les propriétés des dérivées des fonctions réciproques:
		 	
		 	
		 	\item Dérivée de la fonction $a^x$ avec $a>0$:
		 	
		 	Donc (dérivée d'une fonction composée):
		 	
		 	
		 	\item Nous connaissons les règles de la chaîne pour la dérivée d'un produit de fonction. Mais qu'en est-il de la règle équivalent pour la puissance qui est assez fréquente en physique ?
	
			En effet, la dérivée de $f(x)^{g(x)}$ est considérée par beaucoup comme l'une des dérivées les plus polyvalentes !
			
			Premièrement, pour faire la preuve, il devrait être évident que :
			
			et:
			
		 	ne sont que des cas particuliers de la dérivée de $f(x)^{g(x)}$.
		 	
		 	Pour résoudre:
		 	
			nous devons utiliser un petit truc pour faciliter la différenciation. Et rien n'est aussi simple que :
			 
			Donc, puisque $x = e^{\ln(x)}$ nous l'utiliserons à notre avantage.
			
			Nous commençons par :
			
			Nous savons que:
			
			donc nous injectons cela pour obtenir ceci:
			
			Il ne nous reste plus qu'à différencier le membre de droite.
N'oublions pas que :
			
			Dès lors:
			
			Cela équivaut à écrire :
			
		   Il ne nous reste plus qu'à différencier le membre de droite. Voici la règle de multiplication simple :
			
			Et puis nous devons différencier le logarithme naturel :
			
		  	Donc, nous injectons cela pour obtenir ceci:
			
			Et puis nous distribuons $f(x)^{g(x)}$ pour obtenir :
			
			Alors si nous simplifions :
			
			Quelques exemples d'applications nous donnent donc :
			
			En effet, il suffit de poser $f(x) = x$ et $g(x) = n$ et nous obtenons:
		   	
			Ou une autre application :
			
			tout ce que nous avons à faire est de définir $f(x) = a$ et $g(x) = x$ et nous obtenons :
			
			
			
			\item Pour le plaisir (nous n'utilisons cette dérivée nulle part dans ce livre sur aucun cas pratique), calculons la dérivée de $x^x$.
			
		 \end{enumerate}
		 
	\pagebreak
	\subsubsection{Dérivation implicite}
	Jusqu'à présent, les fonctions dont nous nous sommes occupés sont des fonctions qui ont été définies explicitement. Une fonction est définie explicitement si {\it sortie est donnée directement en termes d'entrée}. Par exemple, dans la fonction :
	
	la valeur de $f(x)$ est donnée explicitement ou directement en termes d'entrée. Juste en connaissant l'entrée, nous pouvons immédiatement trouver la sortie. Un deuxième type de fonction qu'il nous est également utile de considérer est une "\NewTerm{fonction implicitement définie}\index{fonction implicitement d\'efinie}". Une fonction est définie implicitement si {\it sortie ne peut pas être trouvée directement à partir de l'entrée}. Par exemple (exemple simple et stupide) :
	
	est une fonction définie implicitement, car pour chaque valeur $x$ positive, il existe une valeur $f$ correspondante, mais nous ne pouvons pas la trouver directement à partir de la fonction. Nous aurions besoin de mettre les deux côtés au carré, puis nous aurions la fonction explicitement définie :
	 	
	Il nous est également possible d'avoir des fonctions implicitement définies que nous ne pouvons pas réécrire en tant que fonction explicitement définie !!!
	
	Par exemple, nous pourrions avoir la fonction :
	
	Pour une valeur $x$ donnée, il {\it peut} y avoir une valeur de sortie correspondante $f(x)$ qui en fait une déclaration vraie. De cette façon, une fonction $f(x)$ serait définie pour tous ces $x$ où il existe une solution. Par exemple, nous avons $f(0) = 0$, car la définition de $x=f(x)=0$ dans l'équation ci-dessus est une déclaration vraie. À l'heure actuelle, nous n'avons pas les outils appropriés pour résoudre une telle équation, mais le concept important ici est que nous pouvons avoir une fonction définie de cette manière!
	
	Lorsque nous parlons de fonctions, nous voulons dire que nous avons une règle qui nous fournit au plus une sortie pour une entrée donnée (il n'y a pas de sortie pour des entrées pour lesquelles la fonction n'est pas définie). Dans un sens plus général, nous pourrions vouloir examiner les règles qui nous fournissent plusieurs sorties pour une entrée donnée. Un tel exemple serait l'équation :
	
	qui est l'équation du cercle unité (\SeeChapter{voir section Géométrie Analytique page \pageref{equation of a circle}}). Il s'avère que cet objet est constitué de deux fonctions, à savoir :
	
	L'équation de ce cercle ne définit pas de fonction (parce que la sortie est multivaluée !!), mais elle définit un certain type de courbe dans le plan $x$-$y$. En général, nous devrions être capables de décrire une courbe arbitraire comme une combinaison d'un certain nombre de fonctions. Il est judicieux (et utile) de considérer la pente de certains points de la courbe, qui correspondrait simplement à la pente de la fonction spécifique qui définit cette partie de la courbe.
	
	Pour résoudre le problème de trouver la dérivée d'une fonction définie d'une manière telle que $\sin(f(x)) + f(x) = x$ ou par une courbe comme un cercle, nous utilisons à nouveau la règle de la chaîne. La manière dont nous allons l'employer est nommée "\NewTerm{différenciation implicite}\index{diff\'erenciation implicite}". Le processus fonctionne comme suit : nous différencions les deux côtés de l'équation par rapport à $x$ (ou la variable indépendante), puis nous résolvons la dérivée de la variable dépendante. Partout où nous trouvons la fonction $f$ (ou la variable dépendante), nous utiliserons la règle de la chaîne pour trouver la dérivée. Commençons par quelques exemples simples :
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemples:}\\\\
	E1. Nous voulons trouver $\mathrm{d}y/\mathrm{d}x$ si $y^2 = x$.\\
	
	Une méthode pour résoudre ce problème serait de le réécrire en termes de fonction explicite de $y$, et de le différencier. Puisque nous avons $y = \pm \sqrt{x}$, nous avons en fait deux fonctions, et nous trouverions :
	
	Cela fonctionne suffisamment bien dans cette situation, mais qu'en est-il d'une fonction que nous ne pouvons pas réécrire explicitement ? Il faudrait une différenciation implicite. Appliquons une différenciation implicite à cette situation, comme exercice: 
	
	Si nous ne pouvions pas réécrire $y$ explicitement en termes de $x$, c'est aussi loin que nous pourrions aller, mais nous savons que $y = \pm \sqrt{x}$, et en injectant ce résultat, nous trouvons que :
	$$\frac{\mathrm{d}y_1}{\mathrm{d}x} = \frac{1}{2\sqrt{x}} \quad \text{et} \quad \frac{\mathrm{d}y_2}{\mathrm{d}x} = -\frac{1}{2\sqrt{x}}$$
	encore une fois. De cette façon, nous avons pu calculer les deux dérivées en une fois.\\
	
	E2. Nous voulons trouver $\mathrm{d}f/\mathrm{d}x$ pour $\sin(f) + f = x$.\\
	
	Ici, nous n'avons pas d'autre choix que d'appliquer une différenciation implicite :
	
	\end{tcolorbox}
	Un cas célèbre d'application en mathématiques pures de la différenciation implicite (nous verrons plus loin pour la physique) est celui à l'origine de cette technique : le "\NewTerm{folium de Descartes}\footnote{Le nom vient du mot latin folium qui signifie "feuille".}" défini par :
	
	La courbe a été proposée pour la première fois par René Descartes en... 1638.
	\begin{figure}[H]
		\centering
			\includegraphics[scale=0.9]{img/algebra/folium_of_descartes.jpg}
		\caption[Le folium de Descartes avec asymptote]{Le folium de Descartes (vert) avec asymptote (bleu) (source : Wikipédia)}
	\end{figure}
	Son titre de gloire réside dans un incident dans le développement du calcul. Descartes a défié Pierre de Fermat de trouver la ligne tangente à la courbe à un point arbitraire puisque Fermat avait récemment découvert une méthode pour trouver des lignes tangentes. Fermat a résolu le problème facilement, ce que Descartes n'a pas pu faire. Depuis l'invention de l'algèbre, la pente de la ligne tangente peut être trouvée facilement en utilisant la différentiation implicite en n'importe quel point comme nous allons le montrer !
	
	Considérons maintenant que nous voulons trouver la pente de la courbe au point $(2,4)$. Ensuite, nous utilisons la différentiation implicite :
	
	En évaluant la dérivée au point $(2,4)$ nous trouvons :
	
	Maintenant que nous avons la pente de la ligne tangente au point d'intérêt, nous utilisons la forme point-pente\index{forme point-pente} :
	
	Maintenant, tant que c'est la ligne normale à la courbe en ce point qui est d'intérêt, nous devons trouver la ligne perpendiculaire à la ligne tangente. Cette ligne passera par le même point, mais la pente sera l'inverse négatif de la pente de la ligne tangente. Il s'ensuit que :
	
	Bon maintenant concentrons-nous sur un exemple appliqué à la physique (mise à part le fameux cas de la recherche de la tangente d'une ellipse). Nous prouverons dans la section de Mécanique des Continuums l'équation de Van der Waals (voir page \pageref{Van der Waals state equation})::
	
	Si $T$ reste constant, considérons que l'on veut trouver le taux de variation du volume par rapport à la pression, soit $\mathrm{d}V/\mathrm{d}P$! Nous pouvons nous défier ici de calculer ce taux de variation pour transformer cette équation en une fonction explicitement définie... Nous avons donc qu'il faut utiliser la différentiation implicite. D'abord:
	
	Cela donne immédiatement :
	
	Maintenant, nous distribuons :
	
	Après simplification et réarrangement nous obtenons :
	
	Finalement:
	
	Le cas particulier des exemples ci-dessus avec l'équation de Van der Waals et le folium de Descartes est introduit dans certains manuels comme suit avec un cas à deux variables :
	
	Dès lors:
	
	Se réduit à:
	
	Finalement:
	
	C'est tout pour le moment. Nous nous arrêterons ici sur ce sujet car nous n'avons pas besoin de plus de techniques ou d'exemples pour notre étude de la physique et de l'ingénierie comme présenté dans ce livre.
	
		\pagebreak
		\subsubsection{Régularité}\label{smoothness}
		La régularité a à voir avec le nombre de dérivées d'une fonction qui existent et sont continues. Le terme "fonction régulière" est souvent utilisé techniquement pour désigner une fonction qui a des dérivées de tous les ordres partout dans son domaine.
		 
		 \textbf{Définition (\#\mydef):} Une "\NewTerm{classe de différentiabilité}\index{classe de diff\'erentiabilit\'e}" est une classification des fonctions selon les propriétés de leurs dérivées. Les classes de différentiabilité d'ordre supérieur correspondent à l'existence de plus de dérivées.
		 
		 Considérons un ensemble ouvert sur la ligne réelle $\mathbb{R}$ et une fonction $f$ définie sur cet ensemble avec des valeurs réelles ou complexes. Soit $k$ un entier non négatif. La fonction $f$ est dite de classe (de différentiabilité) $\mathcal{C}^k$ si les dérivées $f', f'', ..., f(k)$ existent et sont continues (la continuité est impliquée par la différentiabilité pour toutes les dérivées sauf pour $f(k)$). La fonction $f$ est dite de classe $\mathcal{C}^\infty$, ou "lisse", si elle a des dérivées de tous les ordres. La fonction $f$ est dite de classe $\mathcal{C}^\omega$, ou simplement "analytique", si $f$ est lisse et si elle est égale à son développement en série de Taylor autour d'un point quelconque de son domaine (\SeeChapter{voir section Séquences et séries page \pageref{taylor series}}). $\mathcal{C}^\omega$ est donc strictement contenu dans $\mathcal{C}^\infty$.

		\pagebreak
		 \subsection{Calcul Intégral}\label{integral calculus}
		 Nous discuterons ici des principes de base du calcul intégral dans $\mathbb{R}^n$. Les sujets les plus avancés viendront en fonction du temps dont disposent les rédacteurs de ce livre mais le lecteur peut déjà se référer à la section Analyse Complexe (voir page \pageref{complex analysis}) pour les techniques d'intégration basées sur le théorème des résidus qui est très puissant et utile surtout pour certaines intégrales en physique quantique.
		 
		 \begin{fquote}[Anonymous]La différentiation est une science, l'intégration est un art.
 		\end{fquote}
		 
		 \subsubsection{Intégrale Définie}\label{definite integral}
		 L'origine du calcul intégral semble venir d'Archimède qui était fasciné par le calcul des aires de différentes formes. Il a utilisé un processus connu sous le nom de \textit{Méthode d'exhaustion}, qui utilisait des formes de plus en plus petites, dont les aires pouvaient être calculées exactement, pour remplir une région irrégulière et obtenir ainsi des approximations de plus en plus proches de la superficie totale. Dans ce processus, une zone délimitée par des courbes est remplie de rectangles, de triangles et de formes avec des formules de zone exactes. Ces aires sont ensuite additionnées pour approximer l'aire de la région incurvée. Cette sous-section présente les intégrales définies.
		 		 
		 La première idée du concept d'intégrale est de calculer l'aire algébrique (surface positive si au-dessus de l'axe $x$ ou négative en dessous) entre une courbe et son support. Voir la figure ci-dessous avec une zone positive et les notations pour les développements qui suivront :
		 \begin{figure}[H]
			\centering
			\includegraphics{img/algebra/integral_all_positive.jpg}
			\caption[]{Aire $A$ à calculer sous une courbe continue bornée}
		\end{figure}
		Ou avec des aires algébriques différentes (la différence entre la zone bleue et la zone jaune est nommée "\NewTerm{zone nette signée}\index{zone nette sign\'ee}") :
		 \begin{figure}[H]
			\centering
			\includegraphics{img/algebra/integral_all_positive_and_negative.jpg}
			\caption[]{Aire $A$ à calculer dans une fonction continue bornée positive et négative}
		\end{figure}
		Une valeur approchée de l'aire sous une courbe peut être obtenue par un découpage en $n$ bandes rectangulaires verticales de même largeur. En particulier on peut réaliser un encadrement de cette aire à l'aide d'une somme majorante $A_M$ et d'une somme minorante $A_m$ pour un découpage donné:
		\begin{figure}[H]
			\centering
			\begin{subfigure}{0.4\textwidth}
				\includegraphics[width=\textwidth]{img/algebra/integral_minorant_sum.jpg}
				\caption{Somme minorante des aires $A_m$}
			\end{subfigure}
			\begin{subfigure}{0.4\textwidth}
				\includegraphics[width=\textwidth]{img/algebra/integral_majorant_sum.jpg}
				\caption{Somme majorante des aires $A_M$}
			\end{subfigure}				
		\end{figure}
		Supposons que le nombre $n$ de bandes tende vers l'infini. Comme les bandes sont de même largeur, la largeur de chaque bande tend vers $0$ (objectivement il n'est pas nécessaire que la largeur des sous-intervalles du découpage soit la même partout).
		
		Si les sommes $A_m$ et $A_M$ ont toutes deux une limite lorsque le nombre $n$ de bandes tend vers l'infini, alors l'aire $A$ sous la courbe est comprise entre ces deux limites. Nous notons cela:
		
		Évidemment si ces deux limites sont égales, leur valeur est celle de l'aire sous la courbe.
		
		D'où une première définition directe de l'intégrale définie ou dite  "\NewTerm{intégrale de Riemann}\index{intégrale de Riemann}\label{riemann integral}".
		
		\textbf{Définition (\#\mydef):}	Soit un intervalle $[a, b]$, divisé en $n$ parties égales, soit $f$ une fonction continue sur l'intervalle $[a, b]$, soit $A_m$, la somme algébrique minorante et soit $A_M$, la somme algébrique majorante. Nous appelons "\NewTerm{intégrale définie}\index{intégrale définie}" de $f$, depuis $a$ jusqu'à $b$, notée:
		
		le nombre $A$ tel que:
		
		pourvu que cette limite existe. Si cette limite existe, alors nous disons que $f$ est "intégrable" sur $[a, b]$ et l'intégrale définie existe. 
		
		Le symbole:
		
		n'est que que le symbole de la somme discrète $\sum$ mais appliquée aux cas d'éléments infiniments petits.
		
		Les nombres $a$ et $b$ (qui peuvent parfois être des fonctions!) de l'intégrale sont appelés les "\NewTerm{bornes d'intégration}\index{bornes d'int\'egration}": $a$ est la "\NewTerm{borne inférieure}", $b$ est la  "\NewTerm{borne supérieure}".
		
		\begin{center}
		\begin{tikzpicture}[scale=2.3]
		  \shade[top color=blue,bottom color=gray!50] 
		      (0,0) parabola (1.5,2.25) |- (0,0);
		  \draw (1.05cm,2pt) node[above] 
		      {$\displaystyle\int_0^{3/2} \!\!x^2\mathrm{d}x$};
		
		  \draw[style=help lines] (0,0) grid (3.9,3.9)
		       [step=0.25cm]      (1,2) grid +(1,1);
		
		  \draw[->] (-0.2,0) -- (4,0) node[right] {$x$};
		  \draw[->] (0,-0.2) -- (0,4) node[above] {$f(x)$};
		
		  \foreach \x/\xtext in {1/1, 1.5/1\frac{1}{2}, 2/2, 3/3}
		    \draw[shift={(\x,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$\xtext$};
		
		  \foreach \y/\ytext in {1/1, 2/2, 2.25/2\frac{1}{4}, 3/3}
		    \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left] {$\ytext$};
		
		  \draw (-.5,.25) parabola bend (0,0) (2,4) node[below right] {$x^2$};
		\end{tikzpicture}
		\end{center}
		
		Intuitivement, il est évident que lorsque $a=b$, nous étendons la définition ainsi: 
		
		Enfin, signalons qu'il est tout à fait possible que l'intégrale soit négative ou même complexe puisqu'il s'agit d'une surface algébrique! C'est-à-dire qu'en général, le résultat peut être dans $\mathbb{C}$.
	\begin{tcolorbox}[title=Remarques,colframe=black,arc=10pt]
	\textbf{R1.} D'autres lettres que $x$  peuvent être employées dans la notation de l'intégrale définie. Ainsi si $f$ est intégrable sur $[a, b]$, alors $\int\limits_a^bf(x)\mathrm{d}x=\int\limits_a^bf(t)\mathrm{d}t=\int\limits_a^bf(s)\mathrm{d}s$, etc. C'est la raison pour laquelle la variable $x$ de la définition est dite "\NewTerm{variable muette}\index{variable muette}".\\

	\textbf{R2.} Comme nous le verrons plus loin, il est essentiel de ne pas confondre "\NewTerm{int\'egrale d\'efinie}\index{int\'egrale d\'efinie}" et "\NewTerm{intégrale indéfinie}\index{int\'egrale ind\'efinie}". Ainsi, une intégrale indéfinie, notée $\int\limits f(x)\mathrm{d}x$ est une fonction, ou, plus précisément, une famille de fonctions appelées aussi "\NewTerm{primitives de $f$}\index{primitie d'une fonction}" (voir plus bas) alors qu'une intégrale définie, notée $\int\limits_a^b f(x)\mathrm{d}x$ est une constante. 
	\end{tcolorbox}
	Voyons une deuxième approche de définition de l'intégrale, un peu plus rigoureuse que la précédente (suite à la demande de plusieurs lecteurs). Nous utiliserons par tradition cette fois-ci, le $S$ de la surface au lieu du $A$ de l'aire.
	
	Soit f une fonction bornée sur $[a, b]$. Nous considérons une subdivision de son support $[a, b]$ que nous notons:
	
	où les intervalles ne sont pas obligatoirement de tailles équivalentes.

	Nous notons pour $i=1,2,3,...,n$:	
	
	
	\textbf{Définitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] Nous appelons "\NewTerm{somme de Darboux inférieure}\index{somme de Darboux inf\'erieure}" associée à $f$ et $\sigma$ la surface:
		
		\begin{figure}[H]
			\centering
			\includegraphics{img/algebra/darboux_inferior_sum_concept.jpg}
			\caption{Principe du calcul de la somme de Darboux inférieure }
		\end{figure}
		
		\item[D2.] Nous appelons "\NewTerm{somme de Darboux supérieure}\index{somme de Darboux sup\'erieure}" associée à $f$ et $\sigma$ la surface:
		
		\begin{figure}[H]
			\centering
			\includegraphics{img/algebra/darboux_superior_sum_concept.jpg}
			\caption{Principe du calcul de la somme de Darboux supérieure }
		\end{figure}
	\end{enumerate}
	Une fonction $f$  est dite "\NewTerm{Riemann-Intégrable sur $[a, b]$}" si et seulement si les deux surfaces susmentionnées coïncident lorsque les intervalles deviennent infiniment petit.
	
	L'ensemble des fonctions Riemann-intégrables sur $[a, b]$ sont notées $\mathcal{R}_{[a,b]}$.
	
	Les sommes de Darboux ne sont pas très utiles pour le calcul effectif d'une intégrale, par exemple à l'aide d'un ordinateur, car il est en général assez difficile de trouver les $\inf$ et $\sup$ sur les sous-intervalles. On considère plutôt:
	
	La "\NewTerm{somme de Riemann}\index{somme de Riemann}" est défini par le fait que si nous désignons une "\NewTerm{partition}\index{partition}" (ou "\NewTerm{partition régulière}" s'ils sont tous ont la même largeur) de l'intervalle $[a,b]$ par:
	
	et que:
	
	où $\xi_i \in [x_{i-1},x_i]$, alors:
	
	Mais comme il faut bien choisir un $\xi_i$, souvent nous prenons soit celui à droite, soit celui à gauche, dès lors en prenant au hasard la "méthode des rectangles à gauche":
	
	Ce qui nous donnerait pour l'exemple ci-dessous:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/left_rectangle_integral.jpg}
		\caption{Principe du calcul de la méthodes des rectangles à gauche}
	\end{figure}
	Soit:
	
	Mais c'est facile pour une fonction en escalier... mais cela l'est moins pour une fonction continue pour laquelle nous n'obtiendrions qu'une valeur approchée de la surface réelle! L'idée est alors de prendre des intervalles de plus en plus petits:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/riemann_integral_left_rectangle.jpg}
		\caption{Principe du calcul de l'intégrale de Riemann avec la méthode des rectangles à gauche}
	\end{figure}
	Et dès lors, à la limite, nous obtenons la quantité voulue:
	
	Le fait de chercher cette limite s'appelle "calculer l'intégrale", et plus spécifiquement de la méthode choisie: "intégrale de Riemann".
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemple:}\\\\
	Nous voulons utiliser la construction de l'intégrale définie pour évaluer :
	
	en utilisant une approximation de l'extrémité droite pour générer la somme de Riemann.\\
	
	Pour cela, nous avons d'abord établi la somme de Riemann. Sur la base des limites d'intégration, nous avons $a=0$ et $b=2$. Pour $i=0,1,2,\ldots,n$, soit $P=\{x_i\}$ une partition régulière de $[0,2]$. Puis:
	
	Ainsi, la valeur de la fonction à l'extrémité droite de l'intervalle est :
	
	Alors la somme de Riemann prend la forme :
	
	En utilisant la relation de sommation de Gauss de $\displaystyle\sum_{i=1}^n i^2$ (\SeeChapter{voir section Séquences et Séries page \pageref{sum of squares integers}}), nous avons :
	
	Maintenant, pour calculer l'intégrale définie, nous devons prendre la limite comme $n\rightarrow+\infty$. Nous avons alors:
	 
	\end{tcolorbox}
	
	\subsubsection{Intégrale Indéfinie}
	Nous avons vu précédemment lors de notre étude des dérivées, le problème suivant: étant donnée une fonction $F(x)$, trouver sa dérivée, c'est-à-dire la fonction algébrique:
	
	\textbf{Définition (\#\mydef):} Nous disons que la fonction $F (x)$ est une "\NewTerm{primitive}\index{primitive}" ou "\NewTerm{int\'egrale ind\'efinie}\index{int\'egrale ind\'efinie}" de la fonction $f (x)$ sur le segment $[a, b]$, si en tout point de ce segment nous avons l'égalité $F'(x)=f(x)$.
	
	Deux définitions alternatives plus explicites et moins techniques sont : 
	\begin{itemize}
		\item Une "intégrale indéfinie" est une FONCTION de $x$ (ou une autre variable), tandis qu'une "intégrale définie" est une VALEUR !
		
		\item La collection de toutes les primitives de $f(x)$ est appelée "l'intégrale indéfinie" de $f$ par rapport à $x$.
	\end{itemize}
	
	Une autre manière de voir le concept d'intégrale indéfinie est de passer par le "\NewTerm{théorème fondamental du calcul intégral (et différentiel)}\index{th\'eor\'eme fondamental du calcul int\'egral (et diff\'erentiel)}" appelé aussi parfois "\NewTerm{théorème fondamental de l'analyse}\index{th\'eor\'eme fondamental de l'analyse}\label{fundamental theorem of calculus}" dont les deux propriétés s'énoncent ainsi:
	\begin{enumerate}
		\item Si $A$ est la fonction définie par $A(X)=\displaystyle\int\limits_a^X f(t)\mathrm{d}t$ pour tout $X$ dans $[a, b]$, alors $A$ est la primitive de $f$ sur $[a, b]$ qui s'annule en $a$ (ou en d'autres termes: $f (t)$ est la dérivée de $A$).
		
		\item Si $F$ est une primitive de $f$ sur $[a, b]$, alors:
		
		qui peut aussi s'écrire évidemment comme suit :
		
		Certaines personnes disent alors que la somme des changements à l'extérieur d'une fonction (le côté gauche de l'égalité) est égale aux changements à l'intérieur de la fonction (le côté droit de l'égalité). C'est typiquement un cas particulier à $1$ dimension du théorème de Stokes!
	\end{enumerate}
	Démontrons la première propriété du théorème fondamental:
	\begin{dem}
	Soit la fonction:
	
	Si $f$ est positive et $h>0$ (la démonstration dans le cas où $h<0$ est  similaire) et comme $X>0$, nous pouvons nous représenter $A(X)$ comme l'aire sous la courbe de $f$ de $t=a$ jusqu'à $t=X$.
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/area_primitive_representation.jpg}
		\caption{Représentation graphique de l'aire}
	\end{figure}
	Pour démontrer que $A$ est une primitive de $f$, nous allons prouver que $A'=f$. Selon la définition de la dérivée:
	
	Etudions ce quotient: $A(x+h)-A(x)$ est représentée par l'aire de la bande de largeur $h$, prise en sandwich entre deux rectangles de largeur $h$.
	
	Soit $M$ le maximum de $f$ sur l'intervalle $[X,X+h]$ et $m$ le minimum de $f$ sur ce même intervalle. Les aires respectives des deux rectangles sont $Mh$ et $mh$.
	
	Nous avons alors la double inégalité suivante:
	

	Comme $h$ est positif, on peut diviser par $h$ sans changer le sens des inégalités:
	
	Lorsque $h\rightarrow 0^+$ et si $f$ est une fonction continue, alors $M$ et $m$ ont pour limite $f(X)$, et le rapport:
	
	qui est compris entre $m$ et $M$, a bien pour limite $f(X)$.
	
	Comme $A'(X)=f(X)$ pour tout $X$, ceci nous montre que la dérivée de la fonction aire est $f$. Ainsi $A$ est une primitive de $f$. Comme $A(a)=0$, $A$ est bien la primitive de f qui s'annule en $a$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Avant de commencer la démonstration de la deuxième propriété du théorème fondamental, donnons et démontrons le théorème suivant qui va nous être indispensable: Si $F_1(x)$ et $F_2(x)$ sont deux primitives de la fonction $f(x)$ sur le segment $[a, b]$, leur différence est une constante (ce théorème est très important en physique pour ce qui est de l'étude de ce que nous appelons les "conditions initiales").
	\begin{dem}
	Nous avons en vertu de la définition de la primitive:
	
	Nous avons en vertu de la définition de la primitive: $\forall x \in [a,b]$.
	
	Posons:
	
	Nous pouvons écrire:
	
	Il vient donc de ce que nous avons vu pendant notre étude des dérivées que:
	
	Nous avons alors:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Il résulte de ce théorème que si nous connaissons une primitive quelconque $F(x)$ de la fonction $f(x)$, toute autre primitive de cette fonction sera de la forme:
	
	où $c^{te}$ est nommée la "\NewTerm{constant d'intégration}\index{constant d'int\'egration}\label{constant of integration}".
	
	Donc finalement, nous appelons "\NewTerm{intégrale indéfinie}\index{int\'egrale ind\'efinie}" de la fonction $f(x)$ et nous notons:
	
	toute expression de la forme où  $F (x)+c^{te}$ est une primitive de $f(x)$. Ainsi, par convention d'écriture:
	
	si et seulement si $F'(x)=f(x)$.
	
	Dans ce contexte, $f(x)$ est également appelée "\NewTerm{fonction à intégrer}\index{fonction \`a int\'egrer}" et $f(x)\mathrm{d}x$, la "\NewTerm{fonction sous le signe somme}".
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Une "\NewTerm{antidérivée}\index{antid\'eriv\'ee}" (primitive) d'une fonction $f$ est une fonction $F$ dont la dérivée est $f$. L'intégrale indéfinie de $f$ est l'ensemble de TOUTES les antidérivées (primitives) de $f$. Donc l'intégrale indéfinie et l'antidérivée (primitive) ne sont pas les mêmes chosese car la première est l'ensemble de toutes les secondes ! Si $f$ et $F$ sont tels que décrits tout à l'heure, l'intégrale indéfinie de $f$ a la forme $\{F+c^{te}|c^{te}\in\mathbb{R}\}$ quand une antidérivée (primitive) n'est qu'un élément de cet ensemble !! Mais tous les enseignants ne sont pas d'accord sur la définition des antidérivées (primitives)...
	\end{tcolorbox}
	
	Géométriquement, nous pouvons considérer l'intégrale indéfinie comme un ensemble (famille) de courbes telles que nous passons de l'une à l'autre en effectuant une translation dans le sens positif ou négatif de l'axe des ordonnées.
	
	Revenons-en à la démonstration du point (2) du théorème fondamental de l'analyse:
	\begin{dem}
	Soit $F$ une primitive de $f$. Puisque deux primitives diffèrent d'une constante, nous avons bien:
	
	ce que nous pouvons écrire aussi:
	
	pour tout $X$  dans $[a, b]$. Le cas particulier $X=a$ donne $\int\limits_a^a f(t)\mathrm{d}t$ et donc $F(a)+c^{te}=0$ et nous otenons trivialement $c^{te}=-F(a)$. En remplaçant, nous obtenons:
	
	Comme cette identité est valable pour tout $X$ de l'intervalle $[a,b]$, elle est vraie en particulier pour $X=b$. D'où:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Ce dernier résulétat montre aussi quelque chose d'utile!: Il n'est pas nécessaire lorsque nous évaluaons une intégrale de prendre en compte la constante de la primitive générale puisque celle-ci s'annule de par la différence des deux primitives!!
	\begin{tcolorbox}[title=Remarques,colframe=black,arc=10pt]
	\textbf{R1.} Le théorème fondamental qui montre le lien entre primitive et intégrale a conduit à utiliser le même symbole $\int$ pour écrire une primitive (introduit par Leibniz à la fin du 17ème siècle), qui est une fonction, et une intégrale, qui elle, est un nombre.\\
	
	\textbf{R2.} Nous avons également démontré dans le chapitre de Mécanique Analytique comment calculer à l'aide d'une intégrale la longueur d'une courbe dans le plan si la fonction $f(x)$ est explicitement connue.
	\end{tcolorbox}
	Voici quelques propriétés triviales de l'intégration qu'il est bon de se rappeler car souvent utilisées ailleurs sur le site (si cela ne vous semble pas évident, contactez-nous et nous le détaillerons):
	\begin{enumerate}
		\item[P1.] La dérivée d'une intégrale indéfinie est égale à la fonction à intégrer:
		
		
		\item[P2.] La différentielle d'une intégrale indéfinie est égale à l'expression sous le signe somme:
		
		
		\item[P3.] L'intégrale indéfinie de la différentielle d'une certaine fonction est égale à la somme de cette fonction et d'une constante arbitraire:
		
		
		\item[P4.] L'intégrale indéfinie de la somme (ou soustraction) algébrique de deux ou plusieurs fonctions est égale à la somme algébrique de leurs intégrales (ne pas oublier que l'on travaille avec l'ensemble des primitives et non des primitives particulières!):
		
		\begin{dem}
		Pour démontrer cela nous allons prouver que la dérivée du membre de gauche permet de trouver le membre de droite et inversement (réciproque) à l'aide des propriétés précédentes.
		
		D'après P1 nous avons:
				
		Vérifions s'il en est de même avec le membre de droite (nous supposons connues les propriétés des dérivées que nous avons démontrées au début de ce chapitre):
		
		\end{dem} 
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		
		\item [P5.] Nous pouvons sortir un facteur constant de sous le signe somme, c'est-à-dire:
		
		Nous justifions cette égalité en dérivant les deux membres (et d'après les propriétés des dérivées):
		
		
		\item[P6.] Nous pouvons sortir un facteur constant de l'argument de la fonction intégrée (plutôt rarement utilisée):
		
		En effet, en dérivant les deux membres de l'égalité nous avons d'après les propriétés des dérivées:
		
		
		\item[P7.] L'intégration d'une fonction dont l'argument est sommé (ou soustrait) algébriquement est la primitive de l'argument sommé (respectivement soustrait):
		
		Cette propriété se démontre également identiquement à la précédente à l'aide des propriétés des dérivées.
		
		\item[P8.] La combinaison des propriétés P6 et P7 nous permet d'écrire:
		
		
		\item[P9.] Soit $f$ une fonction continue sur $[a,b]$, nous avons pour tout $c$ appartenant à cet intervalle:
		
		Ce théorème, appelé parfois "\NewTerm{relation de Chasles}\index{relation de Chasles}" (de par son pendant vectoriel), découle immédiatement de la définition de l'intégrale indéfinie. $F$ étant une primitive de $f$ sur $[a, b]$ nous avons:
		
		
		\item[P10.] Voilà une propriété souvent utilisée dans la section de Statistiques du site (nous ne trouvons pas de moyen d'exprimer cette propriété par le langage courant donc...):
		
		 Voyons deux propriétés qui nous seront parfois utiles pour calculer des intégrales difficiles:
		 
		\item[P11.] Si une fonction est paire (\SeeChapter{voir section Analyse Fonctionnelle page \pageref{even function}}), l'intégrale sur des bornes symétriques équivaut à:s
		
		
		\item[P12.] Si une fonction est paire (\SeeChapter{voir section Analyse Fonctionnellea page \pageref{odd function}}), l'intégrale sur des bornes symétriques équivaut à:
		
		
		\item[P13.] L'intégrale d'une fonction périodique est invariante sous un décalage de son intégration. C'est une propriété que nous utiliserons plus loin pour finaliser la preuve de la représentation intégrale de la fonction de Bessel d'ordre zéro du premier type.
	
		Si $f$ est une fonction périodique de période $T$ nous savons que pour toute valeur $a$:
		
		Considérons donc maintenant :
		
		Nous faisons le changement de variable $y=t-T$, alors nous avons pour la dernière intégrale :
		
		Dès lors:
		
	\end{enumerate}
	
	\pagebreak
	\subsubsection{Intégrale Double}\label{double integral}
	L'idée des intégrales doubles est de mesurer le volume de la zone délimitée par le graphe d'une fonction de deux variables, au-dessus d'un domaine $D$ du plan (ci-dessous $D$ est rectangulaire).
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/double_integral_square_domain.jpg}
		\caption{Exemple d'une fonction à deux variables au-dessus d'un domaine}
	\end{figure}
	Il va sans dire que les intégrales doubles sont extrêmement importantes aussi dans tout le domaine des mathématiques appliquées!
	
	Là encore, l'idée est la même que l'intégrale définie. Si nous adaptons une approche simpliste, nous décomposons la fonction continue en un escalier et le volume à calculer se réduit alors à faire la somme des volumes de parallélépipèdes:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/double_integral_square_domain_decomposition_into_big_parallelepipeds.jpg}
		\caption{Décomposition du volume en parallélépipèdes grossiers}
	\end{figure}
	Dès lors nous avons la double somme:
	
	Pour une fonction continue, nous procèdons par approximations successives: nous calculons des sommes de Riemann pour des subdivisions de plus en plus fines du domaine $D$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/double_integral_square_domain_decomposition_into_thin_parallelepipeds.jpg}
		\caption{Décomposition du volume en parallélépipèdes de plus en plus fins}
	\end{figure}
	et donc à la limite : 
	
	Par contre, quand on veut intégrer sur un domaine qui n'est pas rectangulaire, les choses se compliquent à priori... Voyons comment contourner le problème.
	
	Pour cela, nous allons construire le domaine $D$ fermé borné de la façon suivante.
	
	où le lecteur aura remarqué que le support de $y$ est la variable $x$ par l'intermédiaire de deux fonctions $u$ et $v$. C'est ce que nous appelons alors un "\NewTerm{domaine du type I}\index{domaine de d\'efinition!domaine du type I}" (et donc si c'est $y$ qui paramétrise $x$ alors il s'agit "\NewTerm{domaine du type II}\index{domaine de d\'efinition!domaine du type II}").

	Ce qui peut s'illustrer par la figure ci-dessous:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/double_integral_type_I_domain_example.jpg}
		\caption{Exemple d'un domaine de type I }
	\end{figure}
	où nous remarquons que cette approche simpliste (il existe d'autres approches possibles mais qui nécessitent de faire appel à la théorie de la mesure) nécessite que le domaine soit simplement connexe (qu'il n'y ait pas de trous hors du domaine $D$ entre $u (x)$ et $v (x)$) ou décomposé en sous-domaines disjointes simplement connexes.
	
	Bref, nous pouvons donc intégrer de la manière suivante:
	
	Donc nous transformons l'intégrale double en deux intégrales simples emboîtées.
	
	\paragraph{Théorème de Fubini}\label{fubini theorem}\mbox{}\\\\
	Nous allons voir un théorème important utilisé à de nombreuses reprises dans différents chapitres du site et qui permet d'inverser l'ordre d'intégration.

	En se rappelant que:
	
	nous pouvons aussi utiliser:
	
	Ainsi avec cette paramétrisation nous pouvons écrire:
	
	Nous pouvons ainsi changer l'ordre d'intégration, le calcul est différent, mais le résultat est le même. Mais ce n'est pas cela qui nous intéresse en réalité ici.
	
	Considérons une fonction telle que (nous disons que la fonction est à "variables séparables"):
	
	Alors:
	
	Supposons que le domaine est un rectangle (nous faisons cette simplification sinon la démonstration se complique nettement). C'est-à-dire:
	
	Dès lors par la propriété de linéarité des intégrales:
	
	
	\subsubsection{Intégration par changement de variables}
	Lorsque nous ne pouvons facilement déterminer la primitive d'une fonction donnée, nous pouvons nous débrouiller par un changement de variable astucieux (parfois même très subtil) pour contourner la difficulté. Cela ne marche pas à tous les coups (car certaines fonctions ne sont pas intégrables formellement) mais il vaut la peine d'essayer avant d'avoir recours à l'ordinateur.

	À nouveau, nous ne donnons que la forme générale de la méthode. C'est le rôle des professeurs dans les écoles d'entraîner les élèves à comprendre et maîtriser ce genre de techniques. De plus, les chapitres traitant des sciences exactes sur le site (physique, informatique, astrophysique, chimie, ...) regorgent d'exemples utilisant cette technique et servent ainsi implicitement d'exercices de style.

	Soit à calculer l'intégrale (non bornée pour l'instant):
	
	bien que nous ne sachions pas calculer directement la primitive de cette fonction $f(x)$ (en tout cas nous imaginons être dans une telle situation) nous savons (d'une manière ou d'une autre) qu'elle existe (nous ne traitons pas encore des intégrales impropres à ce niveau).

	La technique consiste alors dans cette intégrale à effectuer le changement de variable:
	
	où $\varphi (t)$ est une fonction continue ainsi que sa dérivée, et admettant une fonction inverse. Alors $\mathrm{d}x=\varphi' (t)\mathrm{d}t$, démontrons que dans ce cas l'égalité:
	
	est satisfaite.
	\begin{dem}
	Nous sous-entendons ici que la variable $t$ sera remplacée après intégration du membre droit par son expression en fonction de $x$. Pour justifier l'égalité en ce sens, il suffit de montrer que les deux quantités considérées dont chacune n'est définie qu'à une constante arbitraire près ont la même dérivée par rapport à $x$. La dérivée du membre gauche est:
	
	Nous dérivons le membre droit par rapport à$x$ en tenant compte que $t$ est une fonction de $x$. Nous savons que:
	
	Nous avons par conséquent:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Les dérivées par rapport à x des deux membres de l'égalité de départ sont donc égales.
	
	Bien évidemment, la fonction $x=\varphi (t)$ doit être choisie de manière à ce que nous sachions calculer l'intégrale indéfinie figurant à droite de l'égalité.
	
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Il est parfois préférable de choisir le changement de variable sous la forme $t=\varphi (x)$ au lieu de $x= \psi(t)$ car cela à une large tendance à simplifier la longueur de l'équation au lieu de l'allonger.
	\end{tcolorbox}
	Il est évident que ce théorème s'écrira plus explicitement :
	
	
	\pagebreak
	\paragraph{Jacobien}\label{jacobian}\mbox{}\\\\
	Considérons un domaine $D$ du plan $u,v$ limité par une courbe $L$. Supposons que les coordonnées $x, y$ soient des fonctions des nouvelles variables $u, v$ (toujours dans le cadre d'un changement de variables donc) par les relations de transformations:
	
	où les fonctions $\varphi (x,y)$ et $\phi(u,v)$ sont univoques, continues et possèdent des dérivées continues dans un certain domaine $D'$ que nous définirons par la suite. Il correspond alors d'après les relations précédentes à tout couple de valeurs $u, v$ un seul couple de valeur $x, y$ et réciproquement.
	
	Il résulte de ce qui précède qu'à tout point $P(x,y)$ du plan $\text{O}xy$ correspond univoquement un point $P '(u, v)$ du plan $\text{O}uv$ de coordonnées $u, v$ définies par les relations précédentes. Les nombres $v$ et $u$ seront appelées "\NewTerm{coordonnées curvilignes}\index{coordonn\'ees curvilignes}\label{curvilinear coordinates}" de $P$ et nous verrons des exemples concrets et schématisés de ceux-ci dans la section de Calcul Vectoriel.
	
	Si dans le plan $\text{O}xy$ le point $P$ décrit la courbe fermée $L$ délimitant le domaine $D$, le point correspondant décrit dans le plan $\text{O}uv$ un certain domaine $D'$. Il correspond alors à tout point de $D'$ un point de $D$. Ainsi, les relations de transformations établissent une correspondance biunivoque entre les points des domaines $D$ et $D'$.
	
	Considérons maintenant dans $D'$ une droite d'équation $u=c^{te}$. En général, les relations de transformation lui font correspondre dans le plan $\text{O}xy$ une ligne courbe (ou inversement). Ainsi, découpons le domaine $D'$ par des droites d'équations $u=c^{te}$ et$v=c^{te}$  en de petits domaines rectangulaires (nous ne prendrons pas en compte dans la limite, les rectangles empiétant sur la frontière de $D'$). Les courbes correspondantes du domaine $D$ découpent alors ce dernier en quadrilatère (définis par des courbes donc). Évidemment, l'inverse est applicable.
	
	Considérons dans le plan $\text{O}uv$ le rectangle $\Delta s'$ limité par les droites:
	
	et le quadrilatère curviligne $\Delta s$ correspondant dans le plan $\text{O}xy$. Nous désignerons les aires de ces domaines partiels également par $\Delta s'$ et $\Delta s$. Nous avons évidemment:
	
	Les aires $\Delta s$ et $\Delta s'$ peuvent être en général différentes.
	 \begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/algebra/non_linear_map.jpg}
		\caption{Une application non linéaire $f : \mathbb{R}^2\mapsto \mathbb{R}^2$ envoie un petit carré à un parallélogramme déformé}
	\end{figure}
	Supposons donc dans $D$ une fonction continue $z=f(x,y)$. Il correspond à toute valeur de cette fonction du domaine $D$ la même valeur $z=F(u,v)$ (ce qu'il faut vérifier) dans $D'$, où:
	
	Considérons les sommes intégrales de la fonction $z$ dans le domaine $D$. Nous avons évidemment l'égalité suivante:
	
	Calculons $\Delta s$, c'est-à-dire l'aire du quadrilatère curviligne $P_1,P_2,P_3,P_4$ dans le plan $\text{O}xy$:
	
	Déterminons les coordonnées de ses sommets:
	
	Nous assimilerons dans le calcul de l'aire du quadrilatère $P_1,P_2,P_3,P_4$ les arcs $P_1P_2,P_2P_3,P_3P_4,P_4P_1$ à des segments de droites parallèles. Nous remplacerons en outre les accroissements des fonctions par leurs différentielles. C'est dire que nous faisons abstraction des infiniment petits d'ordre plus élevé que $\Delta u$ et $\Delta v$. Les relations précédentes deviennent alors:
	
	Sous ces hypothèses, le quadrilatère curviligne $P_1P_2P_3P_4$ peut être assimilé à un parallélogramme. Son aire est approximativement égale au double de l'aire du triangle  $P_1P_2P_3$, aire que nous pouvons calculer en utilisant les propriétés du déterminant (comme nous le démontrerons dans le chapitre d'Algèbre Linéaire, le déterminant dans $\mathbb{R}^2$ représente un parallélogramme alors que dans $\mathbb{R}^3$ celui-ci représente le volume d'un parallélépipède):
	
	Tel que (c'est là qu'il faut faire le meilleur choix pour que l'expression finale soit la plus simple et la plus esthétique, nous procédons par essais successifs et faisons enfin le choix ci-dessous):
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/graphical_representaton_of_determinant.jpg}
		\caption{Représentation graphique du déterminant}
	\end{figure}
	Ainsi, nous avons:
	
	Par conséquent la relation suivante (contenant ce qu'il est d'usage d'appeler le "\NewTerm{déterminant fonctionnel}\index{d\'eterminant fonctionnel}):
	
	avec:
	
	qui est la "\NewTerm{matrice jacobienne}\index{matrice jacobienne}" (alors que son déterminant est appelé le "jacobien" (tout court)) de la transformation de coordonnées de $\mathbb{R}^2 \rightarrow \mathbb{R}^3$. En appliquant exactement le même raisonnement pour $\mathbb{R}^3$, la matrice jacobienne s'écrit alors (en changeant un peu les notations car sinon cela devient illisible):
	
	Bref, à quoi cela sert-il concrètement ? Eh bien revenons à notre relation:
	
	qui n'est finalement qu'approximatiion étant donné que dans les calculs de l'aire $\Delta s$ nous avons négligé les infiniment petits d'ordre supérieur. Toutefois, plus les dimensions des domaines élémentaires $\Delta s$ et $\Delta s'$ sont petites, et plus nous nous approchons de l'égalité. L'égalité ayant finalement lieu quand nous passons à la limite (finalement en maths aussi on fait des approximations... hein !), les surfaces des domaines élémentaires tendant vers zéro:
	
	Appliquons maintenant l'égalité obtenue au calcul de l'intégrale double (nous pouvons faire de même avec la triple bien sûr). Nous pouvons donc finalement écrire (c'est la seule manière de poser la chose qui ait un sens):
	
	Passant à la limite, nous obtenons l'égalité stricte:
	
	Telle est la relation de transformation des coordonnées dans une intégrale double. Elle permet de ramener le calcul d'une intégrale double dans le domaine $D$ au domaine $D'$, ce qui peut simplifier le problème. De même, pour une intégrale triple, nous écrirons:
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemples:}\\\\
	Déterminons maintenant le Jacobien pour les systèmes de coordonnées les plus courants (nous renvoyons à nouveau le lecteur au chapitre de Calcul Vectoriel pour plus d'informations concernant ces systèmes):\\
	
	E1. Coordonnées polaires\label{jacobian in polar coordinates} $x=r\cos(\phi),y=r\sin(\phi)$:
	
	Comme $r$ est toujours positif, nous écrivons simplement:
	
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	E2. En coordonnées cylindriques $x=r\cos(\phi),y=r\sin(\phi),z=z$ (voir section Algèbre Linéaire page \pageref{determinant} pour le calcul détaillé d'un tel déterminant):
	
	Comme $r$ est toujours positif, nous écrivons simplement:
	
	
	E3. \label{jacobian spherical coordinates}En coordonnées sphériques  $x=r\sin(\theta)\cos(\phi),y=r\sin(\theta)\sin(\phi),z=r\cos(\theta)$ (voir section Algèbre Linéaire page \pageref{determinant}  pour le calcul détaillé d'un tel déterminant):
	
	Comme $r^2$ est toujours positif, nous écrivons simplement:
	
	\end{tcolorbox}
	La définition de la différentiabilité dans le calcul multivariable est un peu technique. Il y a des subtilités à surveiller, car il faut se rappeler que l'existence de la dérivée est une condition plus stricte que l'existence de dérivées partielles. Mais, au final, si notre fonction est assez sympa pour qu'elle soit dérivable, alors la dérivée elle-même n'est pas trop compliquée. C'est une généralisation assez simple de la dérivée à variable unique.
	
	Dans le calcul univaré, nous avons appris que la dérivée d'une fonction $f : \mathbb{R} \rightarrow \mathbb{R}$ en un seul point n'est qu'un nombre réel, le taux d'augmentation de la fonction (c'est-à-dire la pente du graphique) à ce stade. Nous pourrions considérer ce nombre comme une matrice de $1 \times 1$, donc si nous le souhaitons, nous pourrions désigner la dérivée de $f(x)$ à $x=a$ comme :
	
	Pour une fonction à valeur scalaire de plusieurs variables, telles que $f(x, y)$ ou $f(x, y, z)$, nous pouvons considérer les dérivées partielles comme les taux d'augmentation de la fonction dans la directions des coordinées. Si la fonction est dérivable, alors la dérivée est simplement une matrice ligne contenant toutes ces dérivées partielles, que nous appelons la "matrice des dérivées partielles" (qui correspond à la matrice jacobienne). Pour $f : \mathbb{R}^{n} \rightarrow \mathbb{R}$, vu comme un $f(\vec{x})$, où $\vec{x}=\left(x_{1 }, x_{2}, \ldots, x_{n}\right)$, la matrice $1 \times n$ des dérivées partielles à $\vec{x}=\vec{a}$ est :
	
	La dernière généralisation concerne les fonctions vectorielles, $f : \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$. Ici, $f(\vec{x})$ est une fonction du vecteur $\vec{x}=\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ dont la sortie est un vecteur de $m$ composants. Nous pourrions écrire $f$ en termes de ses composants comme :
	
	(Rappelez-vous que lorsque nous considérons les vecteurs comme des matrices, nous les considérons comme des matrices de colonnes, de sorte que les composants sont empilés les uns sur les autres.)
	
	Pour former la matrice des dérivées partielles, nous considérons $f(\vec{x})$ comme une matrice de colonnes, où chaque composant est une fonction à valeur scalaire. La matrice des dérivées partielles de chaque composant $f_{i}(\vec{x})$ serait une matrice de lignes de $1 \times n$, comme ci-dessus. Nous empilons simplement ces matrices de lignes les unes sur les autres pour former une matrice plus grande. Nous obtenons que la matrice $m \times n$ complète des dérivées partielles à $\vec{x}=\vec{a}$ est :
	
	Bien que nous devrions probablement nous référer à la dérivée de $f$ comme la transformation linéaire associée à la matrice $\mathrm{D}f(\vec{a})$, c'est bien à ce niveau de se référer à la "\ NewTerm{matrice des dérivées partielles}" $Df(\vec{a})$ comme "la dérivée" de $f$ au point $\vec{a}$ (en supposant que $f$ est dérivable en $\vec{ a}$, bien sûr).
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Exemple:}\\\\
	Soit $f(x, y)=x^{2}+y^{2}$. Nous voulons trouver $\mathrm{D} f(1,2)$ et l'équation pour le plan tangent (en utilisant le développement de Taylor multivarié tel que démontré à la page \pageref{multivariate taylor series}) à $(x, y)=(1,2)$.\\\\
		
	Nous avons dans un premier temps:
	
	Ainsi $\mathrm{D} f(1,2)=\left[2\quad 4\right]$.\\
	
	Puisque les deux dérivées partielles $\frac{\partial f}{\partial x}(x, y)$ et $\frac{\partial f}{\partial y}(x, y)$ sont des fonctions continues, nous savons que $f(x, y)$ est dérivable. Par conséquent, $\mathrm{D}f(1,2)$ est la dérivée de $f$, et la fonction y a un plan tangent.\\
	
	Pour calculer l'équation du plan tangent, le seul calcul supplémentaire est la valeur de $f$ à $(x, y)=(1,2)$, soit $f(1,2)=1^{2} +2^{2}=5$. L'équation pour le plan tangent est donc (toujours en utilisant l'approximation multivariée de Taylor) :
	
	\end{tcolorbox}

	\pagebreak
	\subsubsection{Intégration par parties}\label{integration by parts}
	Lorsque nous cherchons à effectuer des intégrations, il est très fréquent que nous ayons à utiliser un outil (ou méthode de calcul) appelé "\NewTerm{int\'egration par parties}\index{int\'egration par parties}". Il existe différents degrés d'utilisation de cet outil et nous allons commencer par le plus simple et qui est le plus utilisé dans tous les chapitres traitant de physique sur le présent site.

	Nous partons d'abord de la dérivée du produit de deux fonctions démontrée plus haut:
	
	nous avons donc:
	
	et il vient:
	
	après une dernière simplification nous avons enfin la fameuse relation très importante:
	
	Mais nous allons parfois avoir besoin de la généralisation de cette dernière relation. Nous pouvons démontrer que si $f$ et $g$ sont deux applications (fonctions) de classe $\mathcal{C}^n$ (dérivables $n$ fois) sur $[a, b]$ dans $\mathbb{C}$, alors :
	
	\begin{dem}
	Procédons par récurrence sur $n$ (attention ce n'est pas forcément facile à comprendre comme souvent avec les démonstrations par récurrence!).
	
	Tout en sachant la relation vraie pour $n = 1$, nous la supposons vraie pour $n$ (comme donnée dans la relation précédente!) et nous la démontrons pour $n + 1$ (donc nous devons retomber sur la relation précédente mais avec $n + 1$ au lieu de $n$):
	
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	L'astuce (proposée par un internaute) dans cette démonstration est de bien voir que $-(-1)^n$ donne un signe moins quand $n$ est pair et un plus quand $n$ est impair et de même $+(-1)^{n+1}$ donne un signe moins quand $n$ est pair et un plus quand $n$ est impair.
	\end{tcolorbox}
	Pour $n = 1$ nous retrouvons la formule bien connue et qui sera très très souvent utilisée dans tout le présent livre:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}

	\pagebreak
	\subsubsection{Primitives usuelles}\label{usual primitives}
	Il existe en mathématique et en physique un grand nombre de primitives ou de fonctions définies sur des intégrales que nous retrouvons assez fréquemment (mais pas exclusivement). Par ailleurs, toutes les primitives démontrées ci-dessous seront utilisées dans les sections relatives à la Mécanique, l'Ingénierie, l'Atomistique, les Mathématiques Sociales, etc. Donc, comme dans n'importe quel formulaire, nous vous proposons les primitives connues mais avec les démonstrations! 

	Cependant, nous omettrons les primitives qui découlent déjà des dérivées que nous avons démontrées plus haut. Ce qui signifie par exemple que nous supposerons connues les deux primitives très importantes (certainement les plus utilisées dans l'ensemble des pages de ce livre): 
		
	Sinon voici déjà une liste de quelques intégrales fréquentes (le lecteur en rencontrera de toute façon bien d'autres - développées dans les détails - lors de son parcours du présent livre): 
	\begin{enumerate}
		\item Primitive de $f(x)=\tan (x)$:
		
		Par définition nous avons donc:
		
		Nous utilisons le changement de variable $u=\cos(x),\mathrm{d}u=-\sin(x)\mathrm{d}x$ et dès lors:
		
		Donc:
		
		
		\item Primitive de $f(x)=\cot(x)$:
		
		Par définition nous avons donc:
		
		Nous utilisons le changement de variable $u=\sin(x),\mathrm{d}u=\cos(x)\mathrm{d}x$ et dès lors:
		 
		Donc:
		
		
		\item Primitive de $f(x)=\arcsin(x)$:
		
		Nous intégrons par parties:
		
		Si nous posons $u=1-x^2$, ce qui nous donne $\mathrm{d}u=-2x\mathrm{d}x$, nous obtenons:
		
		Donc:
		
		
		\item Primitive de $f(x)=\arccos(x)$:
		
		Nous intégrons à nouveau par parties:
		
		Si nous posons $u=1-x^2$, ce qui nous donne $\mathrm{d}u=-2x\mathrm{d}x$, nous obtenons:
		
		Donc:
		
		
		\item Primitive de $f(x)=\arctan(x)$:
		
		Nous intégrons encore une fois par parties:
		
		Si nous posons $u=1+x^2$, ce qui nous donne $\mathrm{d}u=2x\mathrm{d}x$, nous obtenons:
		
		Donc:
		
		
		\item Primitive de $f(x)=\text{arccot}(x)$:
		
		Encore une fois... nous intégrons par parties:
		
		Si nous posons  $u=1+x^2$, ce qui nous donne $\mathrm{d}u=2x\mathrm{d}x$, nous obtenons:
		
		Donc:
		
		
		\item Primitive de $f(x)=xe^{ax}$ avec $a\in \mathbb{R}\left\lbrace 0 \right\rbrace$:
		
		Une intégration par parties nous donne:
		
		Donc:
		
		La généralisation est de notre point de vue immédiate et on obtient la formule de réduction suivante :
		
		\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
		Une autre intégrale très importante avec l'exponentielle en physique est celle que nous avions démontrée lors de notre étude de la loi de Gauss-Laplace en statistiques et probabilités (détermination de la moyenne).
		\end{tcolorbox}
		
		\item Primitive de $f(x)=\ln(x)$:
		
		Nous écrivons:
		
		En intégrant par parties nous trouvons:
		
		Finalement:
		
		
		\item Primitive de $f(x)=x\ln(ax)$ avec $a\in \mathbb{R}\left\lbrace 0 \right\rbrace$:
		
		Une intégration par parties nous donne:
		
		Donc:
		
		
		\item Primitive de $f(x)=a^x$ pour $a>0,a\neq 1$:
		
		Pour commencer nous écrivons:
		
		Ainsi il vient:
		
		et:
		
		Finalement:
		
		
		\item Primitive de $f(x)=\log_a(x)$:
		
		Pour $a>0,a\neq 1$ sachant que (voir les propriétés des logarithmes dans la section d'Analyse Fonctionnelle):
		
		nous avons en utilisant la primitive de  $\ln(x)$:
		
		
		\item Primitive de $f(x)=\tanh(x)$:
		
		Nous avons:
		
		Nous utilisons le changement de variable $u=\cosh(x),\mathrm{d}u=\sinh(x)\mathrm{d}x$ et obtenons:
		
		Donc:
		
		
		\item Primitive de $f(x)=\coth(x)$:
		
		Nous avons donc:
		
		Nous utilisons le changement de variable $u=\sinh(x),\mathrm{d}u=\cosh(x)\mathrm{d}x$ et obtenons:
		
		Donc:
		
		\item Primitive de $f(x)=\text{arcsinh}(x)$:
		
		Nous intégrons par parties:
		
		Si nous posons $u=1+x^2,\mathrm{d}u=2x\mathrm{d}x$ nous obtenons:
		
		Donc:
		
		
		\item Primitive de $f(x)=\text{arccosh}(x)$:
		
		Nous intégrons par parties:
		
		Si nous posons $u=x^2-1,\mathrm{d}u=2x\mathrm{d}x$, nous obtenons:
		
		Donc finalement:
		
		
		\item Primitive de $f(x)=\text{arctanh}(x)$:
		
		Nous intégrons par parties:
		
		Si nous posons $u=1-x^2,\mathrm{d}u=-2x\mathrm{d}x$, nous obtenons:
		
		Donc finalement:
		
		
		\item Primitive de $f(x)=\text{arccoth}(x)$:
		
		Nous intégrons par parties:
		
		Si nous posons  $u=1-x^2,\mathrm{d}u=-2x\mathrm{d}x$, nous obtenons:
		
		Donc finalement:
		
		
		\item Primitive de $f(x)=\sin ^{n}(x)$ avec $n \geq 2$:
		
		Posons $I_n=\int \sin ^n(x)\mathrm{d}x$. Une intégration par partie donne:
		
		en remplaçant $\cos ^2(x)$ par $1-\sin ^2{x}$ dans la dernière intégrale, nous obtenons:
		
		et donc:
		
		Toutes les primitives de même forme (relation de récurrence) sont nommées "\NewTerm{formules de réduction}\index{formules de r\'eduction}".
		
		\item Primitive de $f(x)=\cos ^{n}(x)$ avec $n \geq 2$:
		
		Dans ce cas nous avons la formule de récurrence:
		
		cela se démontre exactement de la même manière que la relation récursive précédente (le lecteur peut demander les détails s'il le souhaite).
		
		\item Primitive de $f(x)=\tan ^{2}(x)$:
		
		Sachant que $\tan'(x)=1+\tan^2(x)$, nous avons:
		
		Donc:
		
		
		\item Primitive de $f(x)=\cot ^{2}(x)$:
		
		Sachant que $\cot'(x)=-1-\cot^2(x)$, nous avons:
		
		Donc:
		
		
		\item Primitive de $f(x)=\sin ^{-2}(x)$:
		
		En utilisant les relations trigonométriques remarquables (\SeeChapter{voir section Trigonométrie page \pageref{remarkable trigonometric identities}}), , nous avons: 
		
		selon la primitive $\cot^2(x)$. Donc:
		
		
		\item Primitive de $f(x)=\cos ^{-2}(x)$:
		
		En utilisant encore une fois les relations trigonométriques remarquables (\SeeChapter{voir section Trigonométrie page \pageref{remarkable trigonometric identities}}), nous avons:
		
		grâce à la primitive de $\tan^2(x)$. Donc:
		
		
		\item Primitive de $f(x)=\sin^{-1}(x)$:
		
		Nous faisons la substitution $x=2\arctan(t),t=\tan(x/2)$. Sachant que: (\SeeChapter{voir section Trigonométrie page \pageref{remarkable trigonometric identities}}):
		
		nous obtenons alors:
		
		Donc:
		
		Finalement:
		
		
		\item Primitive de $f(x)=\cos^{-1}(x)$:
		
		Sachant que $\cos(x)=\sin(x+\pi/2)$ (\SeeChapter{voir section Trigonométrie page \pageref{remarkable angles}}), nous avons:
		
		Nous faisons le changement de variable $x+\pi/2=u, \mathrm{d}u=\mathrm{d}x$:
		
		grâce à la connaissance de la primitive de $\sin^{-1}(x)$. Finalement:
		
		
		\item Primitive de $f(x)=\dfrac{1}{1+\cos(x)}$:
		
		Nous faisons la substitution $x=2\arctan(t),t=\tan(x/2)$, sachant que (\SeeChapter{voir section Trigonométrie page \pageref{remarkable trigonometric identities}}):
		
		nous obtenons:
		
		Dès lors:
		
		Finalement:
		
		
		\item  Primitive de $f(x)=\dfrac{1}{1-\cos(x)}$:
		
		Nous faisons à nouveau la substitution $x=2\arctan(t),t=\tan(x/2)$. Dès lors nous trouvons:
		
		Tel que finalement:
		
		
		\item  Primitive de $f(x)=\dfrac{1}{1+\sin(x)}$:
		
		Sachant que:
		
		Nous avons alors:
		
		En faisant le changement de variable:
		
		nous obtenons:
		
		Finalement:
		
		
		\item  Primitive de $f(x)=\dfrac{1}{1-\sin(x)}$:
		
		Par le même raisonnement que précédemment en utilisant le cosinus nous obtenons:
		
		
		\item Primitive de $f(x)=\sinh^n(x)$ avec $n \geq 2$:
		
		Posons:
		
		Une intégration par partie donne (nous avons démontré lors des dérivées usuelles que la primitive du sinus hyperbolique était le cosinus hyperbolique):
		
		en remplaçant $\cosh^2(x)$ avec $1+\sinh^2(X)$ ans la dernière intégrale, nous obtenons:
		
		et donc:
		
		Ainsi nous pouvons obtenir pour ce cas particulier:
		 
		
		\item Primitive de $f(x)=\cosh^n(x)$ avec $n \geq 2$:
		
		Dans ce cas nous avons aussi la relation de récurrence:
		
		qui se démontre de la même façon que ci-dessus. Ainsi:
		
		
		\item Primitive de $f(x)=\tanh^2(x)$:
		
		Sachant que (démontré lors des dérivées usuelles):
		
		nous avons:
		
		Donc:
		
		
		\item Primitive de $f(x)=\coth^2(x)$:
		
		Sachant que (démontré lors des dérivées usuelles):
		
		nous avons:
		
		Donc:
		
		
		\item Primitive de $f(x)=\dfrac{1}{\sinh^2(x)}$:
		
		Nous avons en utilisant la primitive de  $f(x)=\coth^2(x)$:
		
		Donc:
		
		
		\item Primitive de $f(x)=\dfrac{1}{\cosh^2(x)}$:
		
		Nous avons en utilisant la primitive de $f(x)=\tanh^2(x)$:
		
		Donc:
		
		
		\item Primitive de $f(x)=\dfrac{1}{\sinh(x)}$:
		
		Nous faisons la substitution: 
		
		Nous obtenons en utilisant la dérivée de $\text{arctanh}(x)$:
		
		
		et finalement:
		
		
		\item Primitive de $f(x)=\dfrac{1}{\cosh(x)}$:
		
		Nous faisons la substitution:
		
		Nous avons en utilisant la primitive de $\arctan(x)$:
		
		et finalement:
		
		
		\item Primitive de $f(x)=\dfrac{1}{1+\cosh(x)}$:
		
		Nous faisons la substitution:
		
		Nous obtenons:
		
		Finalement nous obtenons:
		
		
		\item Primitive de $f(x)=\dfrac{1}{1-\cosh(x)}$:
		
		Nous faisons la substitution: 
		
		Nous obtenons:
		
		Finalement:
		
		
		\item Primitive de $f(x)=\dfrac{1}{1+\sinh(x)}$:
		
		Nous faisons la substitution:
		
		Nous obtenons: 
		
		D'où:
		
		Donc:
		
		Finalement:
		
		
		\item Primitive de $f(x)=\dfrac{1}{1-\sinh(x)}$:
		
		Nous faisons la substitution habituelle:
		
		Nous obtenons: 
		
		Or:
		
		D'où:
		
		Finalement:
		
		
		\item Primitive de $f(x)=e^{ax}\sin(bx)$ avec $a,b\in \mathbb{R},a^2+b^2\neq 0$:
		
		Une première intégration par parties donne:
		
		Une deuxième intégration par parties donne:
		
		d'où l'égalité:
		
		Ainsi en redistribuant la relation précédente: 
		
		
		\item Primitive de $f(x)=e^{ax}\cos(bx)$ avec $a,b\in \mathbb{R},a^2+b^2\neq 0$:
		
		Un raisonnement analogue à celui d'avant montre que:
		
		
		\item Primitive de $f(x)=x\sin(ax)$ avec $a \in \mathbb{R}^*$:
		
		Une intégration par parties nous donne:
		
		
		\item Primitive de $f(x)=x\cos(ax)$ avec $a \in \mathbb{R}^*$:
		
		Une intégration par parties nous donne:
		
		
		\item Primitive de $f(x)=\dfrac{1}{(x-a)(x-b)}$ avec $a \neq b$:
		
		Nous avons la relation suivante (en calcul intégral, nous appelons une telle décomposition "\NewTerm{décomposition en fraction partielle}\index{d\'ecomposition en fraction partielle}"):
		
		Dès lors:
		
		
		Finalement:
		
		
		\item Primitive de $f(x)=\dfrac{1}{a^2-x^2}$ avec $a \neq 0$:
		
		Nous avons en utilisant le résultat précédent:	
		
		Dès lors:
		

		\item Primitive de $f(x)=\dfrac{1}{a^2+x^2}$ avec $a \neq 0$:
		
		En faisant le changement de variable:
		
		Nous obtenons en utilisant la dérivée de $\arctan (x)$:
		
		
		\item Soit:
		
		avec $n \in \mathbb{N}$. Nous obtenons:
		
		Or cette dernière intégrale se résout par parties:
		
		Dès lors:
		
		Or cette dernière intégrale se résout par parties:
		
		Identiquement au développement suivant, nous avons pour (le signe change):
		
		la relation suivante:
		
		Vous pourrez trouver une application de ces deux primitives dans le modèle cosmologique newtonien de l'univers dans le chapitre d'Astrophysique ainsi que dans le chapitre de Relativité Générale dans le cadre de l'étude de l'effet Shapiro!
		
		\item Primitive de $f(x)=\dfrac{1}{(1-x^2)^2}$:
		
		Nous avons en utilisant les primitives de $\dfrac{1}{(1-x^2)^n}$ (vue avant) et de  $\dfrac{1}{1-x^2}$ (vue plus haut):
		
		
		\item Primitive de $f(x)=\dfrac{1}{(1+x^2)^2}$\label{black hole primitive}:
		
		Nous avons en utilisant les primitives de $\dfrac{1}{(1+x^2)^n}$ (vue avant) et de $\dfrac{1}{1+x^2}$ (vue plus haut):
		
		Vous pouvez trouver une application de cette primitive dans la section de  Relativité Générale pour le temps de chute propre dans un Trou Noir non rotatif.
		
		\item Primitive de $f(x)=\sqrt{x^2-a^2}$ avec $a\in \mathbb{R}^*$ (cas relatif à la surface sous une hyperbole):
		
		Nous pouvons sans perte de généralité supposer  $a>0$. Remarquons que le domaine de définition de $f$ est $]-\infty,-a] \cup [a,+\infty[$.
		
		Nous allons déterminer une primitive de $f$ uniquement sur l'intervalle $[a,+\infty[$ (car c'est celle dont nous aurons besoin dans certains chapitres).
		
		Faisons le changement de variable:
		
		avec donc:
		
		où nous considérons la fonction $\cosh: \mathbb{R}^+ \rightarrow [1,+\infty[$ avec pour réciproque la fonction  $\text{arccosh}:[1,+\infty[ \rightarrow \mathbb{R}^+$ donnée par (\SeeChapter{voir section Trigonométrie page \pageref{inverse hyperbolic to logarithm}}):
		
		Nous obtenons alors en utilisant la primitive de $\sinh^2(x)$:
		
		mais (\SeeChapter{voir section Trigonométrie page \pageref{hyperbolic trigonometry}}) comme:
		
		Donc:
		
		et en utilisant un autre résultat de la section de Trigonométrie:
		
		nous avons alors:
		
		étant donné que les primitives sont données à une constante près, nous pouvons écrire:
		
		pour $x\geq a$. $F$ est donc une primitive de $\sqrt{x^2-a^2}$ sur l'intervalle $[a,+\infty[$.
		
		\item Primitive de $f(x)=\sqrt{a^2-x^2}$ avec $a\in \mathbb{R}^*$:
		
		Nous pouvons sans perte de généralité supposer  $a>0$. Remarquons que le domaine de définition de $f$ est $[-a, a]$.
		
		Nous faisons les substitutions:
		
		nous obtenons:
		
		où nous avons utilisé la primitive de $\cos^n (x)$ avec $n=2$ démontrée plus haut. Maintenant nous avons:
		
		Donc:
		
		et:
		

		\item Primitive de $f(x)=\sqrt{x^2+a^2}$ avec $a\in \mathbb{R}^*$:
		
		Nous pouvons sans perte de généralité supposer $a>0$.
		
		Faisons le changement de variable:
		
		avec:
		
		Nous obtenons:
		
		Ainsi:
		
		Mais comme nous avons vu dans la section de Trigonométrie page \pageref{hyperbolic trigonometry}:
		
		et:
		
		Donc nous avons finalement:
		
		où le $ln (a)$ a encore une fois été omis car les primitives sont données à une constante près.
		
		\item Primitive de $f(x)=\left(\sqrt{a^2-x^2}\right)^{-1}$ avec $a\in \mathbb{R}^*$:
		
		Nous pouvons sans perte de généralité supposer $a>0$.
		
		Nous faisons la substitution:
		
		Nous obtenons:
		
		
		\item Primitive de $f(x)=\dfrac{1}{\sqrt{a^2+x^2}}$ avec $a\in \mathbb{R}^*$:
		
		Nous pouvons sans perte de généralité supposery $a>0$.
		
		Faisons le changement de variable:
		
		Nous obtenons de la même manière que pour les intégrales usuelles précédentes:
		
		et sachant que (\SeeChapter{voir section Trigonométrie page \pageref{inverse hyperbolic to logarithm}}):
		
		Nous obtenons alors au final la primitive importante suivante:
		
		En procédant de même, mais en utilisant le cosinus hyperbolique au lieu du sinus hyperbolique, nous avons bien évidemment:
		
		Nous réutiliserons ces deux dernières relations dans des cas pratiques importants des sections de Mécanique Analytique, Génie Civil (où la constante $a$ valant $1$, $\ln(a)$ est de toute façon nul!) et de Relativité Générale (où $a$ sera non nul et donc il ne sera pas possible d'omettre la constante $\ln(a)$).
		
		\item En mathématiques, il existe plusieurs intégrales connues sous le nom de "\NewTerm{intégrales de Dirichlet}\index{intégrale de Dirichlet}\label{Dirichlet integral}". L'une d'entre elles est l'intégrale impropre de la fonction sinc sur la droite réelle positive :
	
	De par le théorème de Fubini (voir \pageref{fubini theorem}):
	
	Puis en utilisant la primitive de $e^{ax}$:
	
	et en utilisant la primitive de $e^{ax}\sin(bx)$:
	
	Dès lors:

	En utilisant la primitive de $\frac{1}{x^2+a^2}$ vu plus tôt:
	
	Comme:
	
	Alors:
	
		
		\item Considérons une intégrale de la forme suivante que nous pouvons utiliser pour améliorer la formule de Stirling (\SeeChapter{voir section Méthodes Numériques page \pageref{stirling}}) et aussi dont nous avons absolument besoin pour l'étude de la diffraction de Fresnel circulaire (\SeeChapter{voir section Optique Ondulatoire page \pageref{wave optics}}):
		
		où:
		\begin{itemize}
			\item $\lambda$ est grand;
			\item $g(y)$ est une fonction lisse qui a un minimum local à $y^*$ à l'intérieur de l'intervalle $[a, b]$ ;
			\item $h(y)$ est lisse.
		\end{itemize}
		L'intégrale peut être la fonction génératrice caractéristique de la distribution de $g(Y)$ lorsque $Y$ a une densité $h$ (\SeeChapter{voir section Statistiques page \pageref{charactertistic function}}), cela pourrait être une espérance postérieure de $h(Y)$, ou juste une intégrale "simple".
		
		Lorsque $\lambda$ est grand, la contribution à cette intégrale est essentiellement entièrement par construction provenant d'un voisinage autour de $y^*$.
		
		Nous formalisons cela par le développement de Taylor de la fonction $g$ autour de $y^*$ :
		
		Puisque $y^*$ est un minimum local, nous avons :
		
		et donc:
		
		Dès lors:
		
		Le fait que ci-dessus les limites aient changé de $[a,b]$ à $]-\infty,+\infty[$ est du au fait que nous supposons que la zone d'intérêt est au voisinage de $y^*$ et cela parce que $\lambda $ est tellement grand que  quand on s'éloigne rapidement un peu de $y^*$, on peut considérer la courbe comme négligeable !
		
		Si on approxime $h(y)$ linéairement autour de $y^*$, soit :
		
		tel que:
		
		Dès lors:
		
		Le lecteur ne doit pas oublier qu'ici $\lambda g''(y^*)$ est une constante et que si on pose :
		
		Dès lors, pour la première intégrale ci-dessus, nous voyons que nous nous rabattons sur l'intégrale de quelque chose de très similaire à la distribution de Gauss (avec une moyenne $y^*$ et un écart type $\lambda g''(y^*)$) et ensuite il vient presque immédiatement (\SeeChapter{voir section Statistiques page \pageref{Gauss integral}}) :
		
		Pour la deuxième intégrale :
		
		le changement de variable $y-y^*=x$ nous donne :
		
		Il est immédiat que primitive est de la forme :
		
		Donc par symétrie la seconde intégrale est nulle ! On a enfin :
		
		Ce calcul est nommé "\NewTerm{méthode d'intégration de Laplace}\index{m\'ethode d'int\'egration de Laplace}\label{laplace method of integration}" ou simplement "\NewTerm{intégration de Laplace}".
	\end{enumerate}
	
	\begin{tcolorbox}[title=Remarque,colframe=black,arc=10pt]
	Les personnes intéressées par une lecture plus approfondie sur des intégrales communes et belles peuvent jeter un oeil au très bon livre de P.J. Nahin : \textit{Inside Interesting Integrals} \cite{nahin2014inside}.
	\end{tcolorbox}	
	
	\subsubsection{Règle d'intégration de Leibniz}
	En algèbre, la "\NewTerm{règle de l'intégrale de Leibniz}\index{règle intégrale de Leibniz}\label{Leibniz's integral rule}", du nom de Gottfried Leibniz, indique que pour une intégrale de la forme :
	
	où $-\infty <a(x),b(x)<+\infty$, la dérivée de cette intégrale s'exprime par :
	
	Cette relation peut être utile pour évaluer certaines intégrales définies. Lorsqu'elle est utilisée dans ce contexte, la règle de Leibniz pour différencier sous le signe intégral est également connue sous le nom de "\NewTerm{astuce de Feynman}\index{astuce de Feynman}".
	
	Notons que si $a(x)$ et $b(x)$ sont des constantes plutôt que des fonctions de $x$, nous avons un cas particulier de la règle de Leibniz :
	
	Par ailleurs, si $a(x)=a$ et $b(x)=x$, ce qui est aussi une situation courante (par exemple, dans la preuve de la formule d'intégration répétée de Cauchy), nous avons :
	
	\begin{dem}
	Dans un premier temps, autorisons-nous à écrire:
	
	Nous partons de la définition même de la dérivée :
	
	Maintenant, puisque les limites d'intégration dépendent (en général) de $\alpha,$ alors un $\Delta \alpha$ provoquera un $\Delta a$ et un $\Delta b$ et nous devons donc écrire :
	
	Comme $\Delta \alpha \rightarrow 0$ nous avons $\Delta a \rightarrow 0$ et $\Delta b \rightarrow 0,$ aussi, et ainsi :
	
	où les deux derniers termes découlent du fait que comme $\Delta a \rightarrow 0$ et $\Delta b \rightarrow 0$ la valeur de $x$ sur tout l'intervalle d'intégration reste pratiquement inchangée à $x=a$ ou à $x= b$ respectivement. Ainsi:
	
	ou, en prenant le $1/\Delta \alpha$ à l'intérieur de l'intégrale (l'intégrale de Riemann elle-même est définie comme une limite, donc ce que nous faisons est d'inverser l'ordre de deux opérations limites, ce qu'un pur mathématicien voudrait justifier mais, comme d'habitude dans ce livre, nous ne nous en soucierons pas !):
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	La différenciation sous le signe intégral est mentionnée dans les mémoires à succès du regretté physicien Richard Feynman \textit{Surely You're Joking, Mr. Feynman!} dans le chapitre \textit{A Different Box of Tools}. Il décrit l'avoir appris, alors qu'il était au lycée, à partir d'un ancien texte, \textit{Advanced Calculus} (1926), de Frederick S. Woods (qui était professeur de mathématiques au Massachusetts Institute of Technology). La technique n'était pas souvent enseignée lorsque Feynman a reçu plus tard son éducation formelle en Algèbre, mais en utilisant cette technique, Feynman a pu résoudre des problèmes d'intégration autrement difficiles à son arrivée à l'école supérieure de l'Université de Princeton :
	
	«\textit{Une chose que je n'ai jamais apprise était l'intégration des contours. J'avais appris à faire des intégrales par diverses méthodes décrites dans un livre que mon professeur de physique au lycée, M. Bader, m'avait donné. Un jour, il m'a dit de rester après les cours. "Feynman, dit-il, tu parles trop et tu fais trop de bruit. Je sais pourquoi. Tu t'ennuies. Alors je vais te donner un livre. Tu montes là-bas au fond, dans le coin, et étudies ce livre, et quand vous saurez tout ce qu'il y a dans ce livre, vous pourrez reparler." Donc à chaque cours de physique, je ne prêtais aucune attention à ce qui se passait avec la loi de Pascal, ou quoi qu'ils fassent. J'étais à l'arrière avec ce livre : "Advanced Calculus", de Woods. Bader savait que j'avais un peu étudié "Calculus for the Practital Man", alors il m'a donné les vrais exercices. C'était pour un cours junior ou senior au collège. Il y avait des séries de Fourier, des fonctions de Bessel, des déterminants, des fonctions elliptiques, toutes sortes de trucs merveilleux dont je ne connaissais rien. Ce livre montrait aussi comment différencier des paramètres sous le signe intégral, c'est une certaine opération. Il s'avère que cela n'est pas très enseigné dans les universités; ils ne le soulignent pas. Mais j'ai compris comment utiliser cette méthode, et j'ai utilisé ce foutu outil encore et encore. Donc, parce que j'étais autodidacte en utilisant ce livre, j'avais des méthodes particulières pour faire des intégrales. Le résultat était que lorsque les gars du MIT ou de Princeton avaient du mal à faire une certaine intégrale, c'était parce qu'ils ne pouvaient pas le faire avec les méthodes standard qu'ils avaient apprises à l'école. Si c'était l'intégration des contours, ils l'auraient trouvée ; s'il s'agissait d'une simple extension en série, ils l'auraient trouvée. Ensuite, je viens et j'essaie de différencier sous le signe intégral, et souvent cela a fonctionné. J'avais donc une excellente réputation pour faire des intégrales, uniquement parce que ma boîte à outils était différente de celle des autres, et ils avaient essayé tous leurs outils dessus avant de me donner le problème.}»
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us compute the following integral with variables limits:
	
	As both $a(x)$ and $b(x)$ are constants rather than functions of $x$, we will use the special case of Leibniz's integral rule:
	
	Therefore differentiating under the integral with respect to $\alpha$ , we have (at the end we use one of the previous usual primitive and trigonometric relations):
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	Therefore:
	
	But $I(\pi/2)=0$ by definition, so we must have $c^{te}=\pi^2/8$ and finally:
	
	\end{tcolorbox}
	
	\subsubsection{Integral representation of first kind Bessel's function}\label{integral representation of first kind Bessel's function}
	A particularly useful and powerful way of treating Bessel functions employs their integral representation as we will see in the section of Wave Optics page \pageref{fresnel circular aperture}.
	
	Remember that in the section of Sequences and Series we have proved that the generating function of Bessel's function was (see page \pageref{generating function for bessel function of first kind}):
	
	That is:
	
	Now remember that (\SeeChapter{see section Numbers page \pageref{euler formula}}):
	
	So if we return to the generating function, and substitute $t=e^{\mathrm{i}\theta}$, we get:
	
	In which to condensate the result we have used first the property proved during our study of the generating function of Bessel's functions:
	
	and also:
	
	and so on...
	Now remember that:
	
	Therefore identifying real and imaginary part, we get:
	
	Remember also that we have proved during our study of Fourier series in the section of Sequences and Series that:
	
	\begin{center}
	\begin{tabular}{ccc}
	$\text{with }n,k\in \mathbb{N}\text{ and }n\ne k$
	&$\qquad$&
	$\text{with }n,k\in \mathbb{N}\text{ and }n = k$
	\end{tabular}
	\end{center}
	That is:
	
	where $\delta_{nm}$ is the Kronecker symbol (\SeeChapter{see section Tensor Calculus page \pageref{kronecker symbol}}).
	
	Now let us write:
	
	Let us focus on the first integral:
	
	So we see above that whatever the value for any $n$, excepte of $n=0$:
	
	Therefore in only remains for $n>0$:
	
	and we see above that if $n>0$ is odd all the integrals vanish but if $n>0$ is even, only the corresponding $J_n(x)$ remains!
	
	Exactly the same analysis can be done for can be done:
	
	We have therefore for each of the integrals above, especially for each the left term that (recall that $n=0,2,4,\ldots$ is even and $n=1,3,5,\ldots$ is odd):
	
	If these two equations are added together we have using trigonometric identities (\SeeChapter{see section Trigonometry page \pageref{remarkable trigonometric identities}}):
	
	for $n=0,1,2,3,\ldots$.
	
	If we put $n=0$ in the above relation, we get:
	
	If we plot $\cos(x\sin(\theta))$ we see that it repeats itself in all four quadrants (it's an even function):\\\\
	\texttt{>plot([cos(sin(theta)),cos(2*sin(theta)),cos(3*sin(theta)),cos(5*sin(theta))]\\
	,theta=-2*Pi..2*Pi);}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.55]{img/algebra/cos_sin_maple.jpg}
	\end{figure}
	So we can write:
	
	This is the real integral representation of the zero order Bessel function of the first type.
	
	But in many developments we don't use the above expression as there is not phasor that is visible. So the trick is to notice that $\sin(x\sin(\theta))$ reverses its sign in the third an fourth quadrant (it's and odd function):\\\\
		\texttt{>plot([sin(sin(theta)),sin(2*sin(theta)),sin(3*sin(theta)),sin(5*sin(theta))],\\
	theta=-2*Pi..2*Pi);}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/algebra/sin_sin_maple.jpg}
	\end{figure}
	So we have:
	
	Adding the both relations by multiplying the second by $\mathrm{i}$,  we have:
	
	Finally we get the complex representation of the zero order Bessel function of the first type:
	
	Let us do a change of variable $\theta=\varphi+\pi/2$, then:
	
	But we have proved earlier that for any periodic function:
	
	Therefore:
	
	This integral representation my be obtained in various ways but this one seems the most easy one to us. Many other integral representation exists.

	\subsubsection{Dirac Function}
	The Dirac function, also named "\NewTerm{Dirac peak}\index{Dirac peak}\label{dirac function}" or "\NewTerm{delta function}\index{delta function}", plays a very important practical role both in electronic and computer also  wave mechanics and quantum field theory (this allows to discretize a continuum!).
	
	Before going further we could notice that it is wrong to speak about a "function" because a function is an application of a start set (usually the set of real or complex number with one or more dimensions) in an arrival set (usually the set of real or complex numbers in one or more dimensions). While the domain of definition of the Dirac function is not a set of numbers but strictly speaking a set of functions!
	
	More technically the Dirac delta function, or $\delta$ function, is a generalized function, or distribution, on the real number line that is zero everywhere except at zero, with an integral of one over the entire real line. The delta function is sometimes thought of as an infinitely high, infinitely thin spike at the origin, with total area one under the spike, and physically represents the density of an idealized point mass or point charge. It was introduced by theoretical physicist Paul Dirac. In the context of signal processing it is often referred to as the unit impulse symbol (or function). Its discrete analogue is the Kronecker delta function, which is usually defined on a discrete domain and takes values $0$ and $1$.
	
	As always in this book we will focus here only on the properties we will need to study Applied Mathematics stuffs of other sections of the book.
	
	To represent mentally in an easy way this function, first consider the function defined by:
	
	The representation of $y=f(x)$ above is a rectangle of width $a$, and of height $1/ a$ and unit surface. The Dirac function can be considered as the boundary when $a\leftarrow 0$ of the  $f (x)$. So we have:		
	
	That is to say:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/algebra/common_dirac_peak_representation.jpg}
		\caption[Schematic representation of the Dirac delta function by a line surmounted by an arrow]{Schematic representation of the Dirac delta function by a line surmounted by an arrow (source: Wikipedia)}
	\end{figure}
	with:
	
	where $\varepsilon$ is a number greater than $0$ and as small as we want.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		As the reader will have probably noticed it when we introduced the initial function $f (x)$, the resulting delta Dirac function has therefore the dimension of the inverse of a length!
	\end{tcolorbox}
	For a function $g (x)$ continues in $x = 0$ we have:
	
	By extension we have:
	
	and for a function $g (x)$ coninue on $x_0$:
	
	It is then relatively easy to define the Dirac function in 3-dimensional space by:
	
	As as already mentioned we will prove properties of the Dirac function only if we will need them in other sections of this book.
	
	To see now the integral representation of the Dirac function (very useful in Quantum Field Theory) let us recall the Fourier transform of a function $f(t)$ (\SeeChapter{see section Sequences and Series page \pageref{fourier transform}}) using another common notation and convention used by physicists and mathematicians (some of them invert the definition... the Fourier transform is designated as the Inverse Fourier transform...) :
	
	This transform is reversible, i.e., you can go back from $\tilde{f}(s)$ to $f(t)$ by:
	
	If we set $f(t)=\delta(t)$ in the above equations, we find:
	
	In other words, the delta function and a constant $1 / \sqrt{2 \pi}$ are Fourier-transform of each other.
	
	Another way to see the integral representation of the delta function is again using the limits. For example, using the limit of the Gaussian (and the fact the the Fourier transform of a Gaussian is another Gaussian as prove at page \pageref{fourier transform gaussian function}):
	
	Or more generally:
	
	
	\subsubsection{Gamma Euler Function}\label{gamma euler function}
	We define the Euler Gamma function (Eulerian integral of the second kind) by the following integral:
	
	with $x$ belonging to the set of complex numbers whose real part is positive and non-zero (thus the positive real number are also included in the domain of definition)! Indeed, if we take complex numbers with a zero or negative real part, the integral diverges and is then undefined!
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		We have already met this integral and some of its properties (which will be proved here) in our study of the Beta, Gamma, Chi-square, Fisher and Student statistical distribution functions (\SeeChapter{see section Statistics page \pageref{statistical distributions}}). We will also use this integral in maintenance (\SeeChapter{see section Industrial Engineering page \pageref{preventive maintenance}}), in String theory and other engineering and physics fields (see the corresponding chapters) as for the canonical negative binomial generalized linear regression (\SeeChapter{see section Numerical Methods page \pageref{regression techniques}}).
	\end{tcolorbox}
	Consider we want to solve a quite common related practical case:
	
	Set:
	
	Hence:
	
	Arranging we get:
	
	That is:
	
	
	Here is a graphical plot of the module of the Euler Gamma function $\Gamma_{-1}(x)$ for $x$ browsing an interval of real numbers (take care in Maple 4.00b to write GAMMA capitalized!!!):
	
	\texttt{>with(plots):\\}
	\texttt{>plot(GAMMA(x),x=-Pi..Pi,y=-5..5);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/maple_gamma_euler_2d_plot.jpg}
		\caption{Plot of the Euler Gamma function in Maple 4.00b}
	\end{figure}
	and always the same function with Maple 4.00b but now in the complex plane and always with in ordinate the module of the Gamma Euler function:
	
	\texttt{>with(plots):\\}
	\texttt{>plot3d(abs(GAMMA(x+y*I)),x=-Pi..Pi,y=-Pi..Pi,view=0..5, grid=[30,30],orientation=[-120,45],axes=frame,style=patchcontour);}
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/maple_gamma_euler_3d_plot.jpg}
		\caption{Plot of the Euler Gamma function in the complex plane with Maple 4.0}
	\end{figure}
	This function is interesting if we impose the variable $x$ to belong to the set of integer numbers and that we write it as follows:
	
	Let us integrate by part the latter function:
	
	Since the exponential function decreases much faster than $t^x$ then we have:
	
	In literature, we frequently find the following notations (there are confusing):
	
	Which brings us to write the result in a more traditional form:
	
	From the relation $\Gamma_{0}(x)=x\Gamma_{0}(x-1)$, it comes by induction:
	
	But:
	
	That gives:
	
	Therefore:
	
	or written in another way for $x\in \mathbb{N}^*$
	
	Another interesting and useful result of the Euler gamma function is obtained when we replace $t$ by $y^2$ and calculate this latter for $x=0.5$.
	
	First we have:
	
	and after:
	
	But, as we have proved it in the section Statistics during our study of the Normal distribution (see page \pageref{Gauss integral}), we recognize here the Gauss integral that is equal for recall to:
	
	Various kinds of relations can be derived using the recurrence relation :
	
	and from the previous result. For example the Gamma function for $n+\frac{1}{2}$, where $n$ in an integer will be very useful to us in the section Statistics for the derivation of the noncentral chi-square distribution and can be derived as follows:
	
	
	\subparagraph{Incomplete regularized Gamma function}\label{incomplete regularized Gamma function}\mbox{}\\\\
	The "\NewTerm{(upper) incomplete regularized Gamma function}\index{incomplete regularized Gamma function}", useful in the study of the A/B testing for count data (\SeeChapter{see section Statistics page \pageref{A/B testing for count datas}}), is defined by:
	
	Obviously the non-regularized version is defined as:
	
	And factoring one $\beta$ it is also sometimes defined as (with the conventional notation) and putting $t=\beta\lambda$:
	
	So whatever which version we choose for the next developments, the results remains the same!
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We also define the (lower) incomplete regularized Gamma function by:
	
	These functions were first investigated by the mathematician Friedrich Prym in 1877, and $Q(\alpha,\beta\lambda)$ has also been named "\NewTerm{Prym's function}\index{Prym's function}".
	\end{tcolorbox}
	Before continuing consider the following special case that will be useful to further below:
	
	And we need also to prove as intermediate result a special recursive relation. For this thanks to an integration by part we prove the following relation:
	
	Indeed:	
	
	and multiplying by $\alpha$ in both side of the equal we proved the previous equality!
	
	Knowing that $\Gamma(\alpha+1) = \alpha\Gamma(\alpha)$ and that by definition $Q(\alpha,t)=\Gamma(\alpha,t)/\Gamma(\alpha)$ and dividing:
	
	 by $\Gamma(\alpha + 1)$ we get:
	
	From the above relation, we can calculate $Q(\alpha+2,t)$ :
	
	and $Q(\alpha+3,t)$ for the fun...:
	
	We show easily by recurrent that the following power recursive relation:
	
	Using the identity $Q(1,t)=e^{-t}$ and the previous recursive relationship we can express $Q$ as:
	
	Indeed, let us put first $\alpha=1$:
	
	We put $n=n+1$ (hence we required now that $n>1$), therefore:
	
	We do another change of variable $k=k+1$:
	
	
	\pagebreak
	\paragraph{Euler-Mascheroni Constant}\mbox{}\\\\
	This small text is a just curiosity regarding to Euler's constant $e$ and to almost every Differential and Integral calculus tools that we have seen until now. This is a very nice example (almost artistic) of what we can do with mathematics as soon as we have enough tools at our disposal.
	
	Moreover, this constant is useful in some differential equations which we see later.
	
	Remember that we saw in the section of Functional Analysis that the Euler constant $e$ is defined by the limit:
	
	
	In a more general case we can easily demonstrate in the same way that (you can ask us the details if needed):	
	
	This obviously suggests:
	
	by a change of variable $t=nu$ we write:
	
	And we use the definition of the Beta function (\SeeChapter{see section Statistics page \pageref{beta function}}):
	
	Therefore:
	
	To transform this expression we can write:
	
	But the quantity:
	
	tends to the limit $\gamma=0.5772$, named "\NewTerm{Euler-Mascheroni constant}\index{Euler-Mascheroni constant}" or also "\NewTerm{Euler Gamma constant}\index{Euler Gamma constant}" when $n$ tends to infinity.
	
	Therefore:
	
	We divide each term of the product $(x+1)...(x+n)$ by the corresponding integer taken into $n!$, so we get (according to a reader request we have put a maximum of details!):
    
	
	\pagebreak
	\subsubsection{Curvilinear Integrals}\label{curvilinear integral}
	The line integrals (curvilinear integrals) are also very important in physics. The reader will thus find see them again in the section of Classical Mechanics, Electrodynamics Magnetostatics and to calculate the work of a force or the "flow field", or in the chapter of Euclidean Geometry to calculate the center of gravity of curves (functions), or in the section of Geometric Shapes to calculate the surface of some bodies of revolution but also in Corpuscular Quantum Physics for the famous "path integral" (which is only the term used by physicists to say "line integral") or for the specific calculation of integrals using the residue theorem proved in the section of Complex Analysis or for transformation states in the section of Thermodynamics. This is why there will not be here examples of line integrals because they are already so many applications in the other chapters of this book.
	
	With the definition of these integrals, we can prove two very important results detailed in section of Vector Calculus that are respectively the Green's theorem and Stokes' theorem or even the theorem of residues proved in the section of Complex Analysis and already mentioned in the preceding paragraph (this is important enough to mention it twice!).
	
	More technically a line integral is an integral where the function to be integrated is evaluated along a curve. The terms "\NewTerm{path integral}\index{path integral}", "\NewTerm{curve integral}\index{curve integral}", and "\NewTerm{curvilinear integral}\index{curvilinear integral}" are also used; "\NewTerm{contour integral}\index{contour integral}" as well, although that is typically reserved for line integrals in the complex plane.
	
	\paragraph{Curvilinear Integral of a scalar field}\mbox{}\\\\
	Consider a parametrized curve $C$ (\SeeChapter{see section Differential Geometry page \pageref{parametric curves}}) by a vector function $\vec{r}(t)$ with $t \in [a,b]$ of class $\mathcal{C}^1$ piecewise (this condition is necessary so that we can integrate the curve without problems).
	
	\textbf{Definitions (\#\mydef):}
	 \begin{enumerate}
	 	\item[D1.] The curve is said to be a "\NewTerm{closed curve}\index{closed curve}" if $\vec{r}(a)=\vec{r}(b)$
	 	
	 	\item[D2.] The curve is said to be a "\NewTerm{smooth curve}\index{smooth curve}" if $\forall t \in [a,b]\; \vec{r}'(t)\neq 0$
	 \end{enumerate}
	 Recall that a parametric curve can be written as follows (all vector function can be written in this form):
	 
	Consider a function or a "\NewTerm{scalar field}\index{scalar field}" $f(x,y)$ defined in a neighborhood of $C$. We subdivide $[a,b]$ into $n$ subintervals of equal length $\Delta t$ as:
	
	We choose on each subinterval a point $t_i^*\in [t_i,t_{i+1}]$. Given $\delta s_i$ the length of the arc $C$ connecting the point $(x(t_i),y(t_i))$ and $(x(t_{i+1}),y(t_{i+1})$, the integral of $f$ along $C$ is defined as the "\NewTerm{line integral}\index{line integral}" or "\NewTerm{path integral}\index{path integral}":
	
	Which as we know, can be written (see section of Differential Geometry page \pageref{curvilinear abscissa helix} of Analytical Mechanics page \pageref{parametric curve length} and many others):
	
	and that can obviously be immediately extended to the case to 3 variables and more.
	
	Or in vector form:
	
	The line integral is linear, that is to say, if $C=C_1 \cup C_2$ and that $C_1 \cap C_2$ is a point, then (without going into the strict definition of the union of two curves...):
	
	
	\paragraph{Curvilinear Integral of a vector field}\mbox{}\\\\
	Consider a vector field (e.g. a force field) as:
	
	and an infinitesimal element of a curve (path) $\mathcal{C}^1$ piecewise as:
	
	The idea is then to consider that the dot product (vector field projection on the path element) represents the work along the differential element:
	
	Therefore the work on all the path will be given by (using the property of linearity of the curvilinear integral):
	
	This can obviously be generalized to $n$ dimensions. Let us indicate that when the line integral (path integral) of a vector field is extended to a closed curve, then we speak of "\NewTerm{circulation of the vector field}\index{circulation of the vector field}".
	
	As:
	
	We then have write a fairly common notation:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In physics many times problems are often in the plane and require the transition to polar coordinates because many academic physic problems are centro-symmetric, which also facilitates the calculations of path integrals.
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us calculate the work of the force of gravity moving a mass $M$ of the point $M_1(a_1,b_1,c_1)$ to the point $M_2(a_2,b_2,c_2)$ along an arbitrary path $C$. The projections of the force of gravity on the coordinate axes are:
	
	The work accomplish is then:
	
	so we find a very known result of the section of Classical Mechanics.
	\end{tcolorbox}
	A line integral of a vector $\vec{F}$ field along a curve $C_1$ is independent of the path of integration if:
	
	for any non-null curve $C_2$ having only the same points of departure and arrival. Furthermore if the vector field satisfied (where $U$ in physics is typically a potential):
	
	as (the reader will recognize an exact total differential form):
	
	Then the integral path on an arbitrary curve only depends only on the difference of the values the function $U$ at the two ends! This is the "\NewTerm{fundamental theorem for line integrals}\index{fundamental theorem for line integrals}" or "\NewTerm{gradient theorem for line integrals}\index{gradient theorem for line integrals}".
	\begin{dem}
	If the differential form of the vector field satisfies an total exact differential, we have:
	
	That is:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	So the line integral of an exact total differential does not depend on the path of integration but only the ends! We also conclude that if $\vec{F}$ isderived from a scalar potential and $A = B$, the line integral is then zero.
	
	In physics this result is interpreted by saying that the work provided by a force $\vec{F}$ derived from a scalar potential acting on an elementary particle in a finite displacement does not depend on the path followed.
	
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] When the curve (path) $C$ is closed and the path integral has a result independent result of the direction in which this path is traveled, we use the following notation (the letter below the symbol representing the path can of course vary...):
		
		If this closed integral is always zero, we say that the integrated vector field is a "\NewTerm{conservative vector field}\index{conservative vector field}" and "\NewTerm{derives from a scalar potential}" (and therefore satisfies the Schwarz's theorem to be written as exact total differential) since this stems from the proof given already just above.
		
		\item[D2.] \label{closed path orientation}When the value of the integral of a closed path depends on the orientation (clockwise not equal to counterclockwise) we use the following notation (the letter below the symbol representing the path can of course vary...):
		
		Thus if the direction is direct (that is to say "counterclockwise" or "trigonometric") as the notation on the above, its sign will be positive; if on the contrary the direction is clockwise his sign will be negative (see the proof in the section of Complex Analysis). Therefore we often speak about respectively "negative direction" or "positive direction".
		
		Thus, to summarize, a line integral (path integral) is fully defined by the expression under the symbol of the integral, the form of integration path and the direction of the integration.
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The reader will find some the proofs of very important properties of curvilinear integrals  in section of Vector Calculus as the Green-Riemann theorem or a particular application to study holomorphic functions in the section of Complex Analysis.
	\end{tcolorbox}
	
	\subsubsection{Integrals involving parametric equations}
	Now that we have seen how to calculate the derivative of a plane curve, the next question is this: How do we find the area under a curve defined parametrically? 
	
	To derive an expression for the area under a parametric curve defined by the functions:
	
	with $a\leq t\leq b$.

	We assume that $x(t)$ is differentiable and start with an equal partition of the interval $a\leq t\leq b$. Suppose:
	
	and consider the following figure: $x(t_0),x(t_1),x(t_n)$
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/integral_parametric_curve.jpg}
	\end{figure}
	We use rectangles to approximate the area under the curve. The height of a typical rectangle in this parametrization is $y(x(\overline{t_i}))$ for some value $\overline{t_i}$ in the $i$-th subinterval, and the width can be calculated as $x(t_i)-x(t_{i-1})$. Thus the area of the $i$-th rectangle is given by:
	
	Then a Riemann sum for the area is:
	
	Multiplying and dividing each area by $t_i-t_{i-1}$ gives:
	
	Taking the limit as $n$ approaches infinity gives:
	
	And it is obvious that applying Pythagoras's theorem on an infinitesimal length of the parametric curve we have:
	
	with $x=x(t)$, $y=y(t)$ and $t_1<t<t_2$. This gives the arc length\index{arc length} of the parametric curve between two points on the curve.
	
	It comes also immediately:
	
	The chain rule gives:
	
	Therefore:
	
	We will meet again this relation in the section of Analytical Mechanics.
	
	In astronomy we often have to deal with closed curved and calculated the distance travel on that curve. But working in cartesian coordinates is not always is the best. This is why it is better to change in polar coordinates.

	The idea is to suppose that we are able to express our curve of interest in the following form:
	
	where $\alpha\leq \theta \leq \beta$. In order to adapt the arc length relation for a polar curve, we use the relations:
	
	and we replace the parameter $t$ by $\theta$ Then:
	
	we replace $\mathrm{d}t$ by $\mathrm{d}\theta$, and the lower and upper limits of integration are $\alpha$ and $\beta$ respectively. Then the arc length formula becomes:
	
	So finally in polar coordinates:
	
	
	\subsubsection{Improper Integrals}
	Improper integrals are definite integrals where one or both of the boundaries is at infinity, or where the integrand has a vertical asymptote in the interval of integration. As crazy as it may sound, we can actually calculate some improper integrals using some clever methods that involve limits.
	
	By abuse of notation, improper integrals are often written symbolically just like standard definite integrals, perhaps with infinity among the limits of integration. When the definite integral exists (in the sense of either the Riemann integral or the more advanced Lebesgue integral), this ambiguity is resolved as both the proper and improper integral will coincide in value and this is what will the most occur through all applications of integrals in physics, chemistry and engineering through this book!
	
	For the Riemann integral (or the Darboux integral, which is equivalent to it as we have seen earlier above), improper integration is necessary both for unbounded intervals (since one cannot divide the interval into finitely many subintervals of finite length) and for unbounded functions with finite integral (since, supposing it is unbounded above, then the upper integral will be infinite, but the lower integral will be finite)!
	
	An "\NewTerm{improper integral}\index{improper integral}\label{improper integral}" of a function $f(x) > 0$ is given basically by:
	
	And we say that the improper integral converge if this limit exists and diverges otherwise.
	
	More generally, improper integrals are given by the following pairs of possibilities:
	
	or:
	
	in which one takes a limit in one or the other (or sometimes both) endpoints.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	By abuse of notation, improper integrals are often written symbolically just like standard definite integrals, perhaps with infinity among the limits of integration. When the definite integral exists (in the sense of either the Riemann integral or the more advanced Lebesgue integral), this ambiguity is resolved as both the proper and improper integral will coincide in value.
	\end{tcolorbox}
	
	Geometrically then the improper integral represents the total area under a curve stretching to infinity. If the integral $\int_a^{+\infty} f(x)\mathrm{d}x$ converges the total area under the curve is finite; otherwise it's infinite.
	
	How can an area that extends to infinity be finite?  Obviously the area between $a$ and $N$ (i.e. $\int_a^N f(x)\mathrm{d}x$) is finite.  As $N$ goes to infinity this quantity will either grow without bound or it will converge to some finite value. 
	
	The domains where improper integrals are the most used, without even be noticeable most of time by students or engineering practitioners, are respectively:
	\begin{itemize}
		\item Statistics (see corresponding section) when we normalize or check the condition of covergence to a cumulated probability of $1$ of a density function (most of time in statistics one or the both bounderies are equal to infinity)

		\item Wave Quantum Physics (see corresponding section) where we deal sometimes with free propagating particles from infinity to infinity (this also happens sometimes in General Relativity)

		\item Differential equations, especially when we solve them by using Fourier Transform or Laplace transform (we have many examples using this transforms accross the book) for practical application in physics and high level financial engineering.

		\item In astrophysics or electrostatics when dealing with any punctual potential field source of the type $1/r^2$  for which we want to calculate the work necessary to bring an object from infinity to that source (calculation of the type $\int_{+\infty}^{r} f(r)\mathrm{d}r$).
	\end{itemize}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. We want compute the very important integral (to introduce Laplace Transforms):
	
	with $a\in\mathbb{R}$.\\
	
	Following the definition above we need to first compute a definite integral and the take a limit. So from the definition:
	
	We first compute the definite integral. We start with the case $a=0$:
	
	therefore for $a=0$ the improper integral $I$ does not exist. When $a\neq 0$ we have:
	
	In the case $a<0$, that is $a=-|a|$, we have that:
	
	therefore for $a<0$ the improper integral $I$ does not exist. In the case $a>0$ we have:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	E2. We want to compute:
	
	The integrand is not continuous at $x=0$ and so we’ll need to split the integral up at that point:
	
	Now we need to look at each of these integrals and see if they are convergent
	
	At this point we're done.  One of the integrals is divergent that means the integral that we were asked to look at is divergent.  We don't even need to bother with the second integral.
	\end{tcolorbox}
	On a side note, notice that the area under the curve of this infinite interval was not infinity as the reader may have suspected it to be (perhaps).  In fact, it was a surprisingly small number.  Of course this won't always be the case, but it is important enough to point out that not all areas on an infinite interval will yield infinite areas.
 
	Let's now get some definitions out of the way.  We will name these integrals "\NewTerm{convergent integrals}\index{convergent integral}" if the associated limit exists and is a finite number (i.e. it's not plus or minus infinity) and "\NewTerm{divergent integrals}\index{divergent integral}" if the associated limit either doesn't exist or is (plus or minus) infinity.
	
	Notice that the similar integral:
	
	cannot be assigned a value in the previous way, as the integrals above and below zero do not independently converge. However there is another possibility that we will see in the section of Analysis during our study of the Hilber transform (see page \pageref{hilbert transform}).
	
	\pagebreak
	\subsubsection{Elliptic Integrals}\label{elliptic integrals}
	"\NewTerm{Elliptic integrals}\index{elliptic integrals}" originally arose in connection with the problem of giving the arc length of an ellipse. They were first studied by Giulio Fagnano and Leonhard Euler. Modern mathematics defines an "elliptic integral" as any function $f$ which can be expressed in the form:
	
	where $R$ is a rational function of its two arguments, $P$ is a polynomial of degree $2$ to $4$ with no repeated roots, and $c$ is a constant.

	In general, integrals in this form cannot be expressed in terms of elementary functions. Exceptions to this general rule are when $P$ has repeated roots, or when $R(x,y)$ contains no odd powers of $t$. However, with the appropriate reduction formula, every elliptic integral can be brought into a form that involves integrals over rational functions and the three Legendre canonical forms (i.e. the elliptic integrals of the first, second and third kind).

	Let us recall that and inform you that we have, and we will in this book, encounter the following Elliptic Integrals:
	\begin{itemize}
		\item Non-Euclidean Geometry page \pageref{elliptic integral riemann space} (complete elliptic integral of second kind):
		
		
		\item Geometric shapes page \pageref{elliptic integral ellipse perimeter} (second-order elliptic integral):
		
		
		\item Classical Mechanics Pendulum \pageref{elliptic integral pendulum} (elliptic integral of the first kind):
		
	\end{itemize}
	
	\paragraph{Incomplete Elliptic Integrals}\label{incomplete elliptic integrals}\mbox{}\\\\
	It is common first to consider three types integrals that we name "\NewTerm{incomplete\footnote{The term "incomplete integral" seems to be an old fashioned way to call a primitive function} elliptic integrals}":
	
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] The "\NewTerm{incomplete elliptic integral of the first kind}" depends of two parameters, that are the amplitude $\phi$ and the angle $\alpha$:
		
		We can do the following simple change of variables:
		
		Hence:
		
		We then use also sometimes instead the parameter $k=\sin(\alpha)$, ($\leq m \leq 1$), by writing:
		
		named the "\NewTerm{Jacobi's form}" of the elliptic integral.
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Using (\SeeChapter{see section Trigonometry page \pageref{remarkable trigonometric identities}}):
		
		Then:
		
		\end{tcolorbox}
		
		\item[D2.] The "\NewTerm{incomplete elliptic integral of the second type}" is define we the same both parameters:
		
		We can also do the following simple change of variable:
		
		and $k=\sin(\alpha)$ to get same as previously:
		
	
		\item[D3.] The "\NewTerm{incomplete elliptic integral of the third kind}":
		
		or:
		
		for $n>0$. We can do the same type of change of variable, and therefore:
		
	\end{enumerate}
	
	\paragraph{Complete Elliptic Integrals}\label{complete elliptic integrals}\mbox{}\\\\
	When the amplitude is equal to $\pi/2$ (hence $t=1$), we write the following "\NewTerm{complete elliptic integrals}":
	
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] The "\NewTerm{complete elliptic integral of the first kind}" may thus be defined from the incomplete elliptic integral of the first kind:
		
	
		\item[D2.] The "\NewTerm{complete elliptic integral of the second kind}" may thus be defined from the incomplete elliptic integral of the second kind also:
		
	
		\item[D3.] The "\NewTerm{complete elliptic integral of the third kind}" may thus be defined from the incomplete elliptic integral of the third kind also:
		
	\end{enumerate}
	
	\pagebreak
	\subsection{Differential Equations}
	\textbf{Definition (\#\mydef):} In mathematics, a "\NewTerm{differential equation}\index{differential equation}" (D.E.)  is a relationb etween one or more unknown functions and their derivatives up to order $n$. The "\NewTerm{order}\index{order of a differential equation}" of a differential equation corresponding to the maximum degree of differentiation which one of the functions is subjected.
	
	Compared to our goal of trying to see how the math describes the sensible reality, differential equations are a great success but are also the source of many troubles. First there are modeling difficulties (see for example the differential equation system of General Relativity in the corresponding section of the book...) resolution difficulties (there is no general method even with numerical computer methods as you can see in the corresponding section!), then their are proper mathematical difficulties (that's why some D.E. have million dollar price in case or resolution), finally difficulties related to the fact that certain differential equations are unstable by nature and give chaotic solutions (see the section Population Dynamics or Meteorology for flagrant simple examples!).
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The differential equations are used to construct mathematical models of physical orbiological phenomena, such as for the study of radioactivity, celestial mechanics, electronic circuits, populatin development or even financial stochastic process. Therefore, differential equations represent a vast field of study, both in pure and Applied Mathematics.
	\end{tcolorbox}
	\begin{fquote}[Steven Strogatz]Since Newton, mankind has come to realize that the laws of physics are always expressed in the language of differential equations.
 	\end{fquote}
	The differential equation of order $n$ the more general can always be written as:
	
	We consider in this book only the cases where $x$ and $y$ have their values in $\mathbb{R}$. A solution to such a D.E. on the interval $I \in \mathbb{R}$ is a function $y \in \mathcal{C}^n (I,\mathbb{R})$ (a function $y:I \rightarrow \mathbb{R}$ which is $n$ times continuously differentiable) such that for any $x \in I$, we have:
	
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} For reasons that will be developed later, we also say "integrate a D.E." instead of saying "finding a solution to the D.E.". The first expression is particularly found in the American literature.\\
	
	\textbf{R2.} Since all this book is full examples of differential equations with initial conditions (we speak then about a "\NewTerm{Cauchy problem}\index{Cauchy problem}") and methods of resolutions in the section of Classical Mechanics, Atomic Physics, Cosmology, Econometry, Sequences and series, Industrial Engineering, Statistics, etc., we will not give any application examples here and will focus only on the minimum theoretical useful aspect.
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{First order differential equations}\label{first order differential equations}
	A differential equation of the first order is therefore a D.E. which involves only the first derivative $y'$.
	
	\textbf{Definition (\#\mydef):} A first order differential equation is named "D.E. of order 1 with separate variables" if it can be written as:
	
	Such a differential equation can be easily integrated. Indeed, we write:
	
	Then symbolically:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We write explicitly here the arbitrary integration constant $c^{te} \in \mathbb{R}$ (which is normally implicitly present in the indefinite integrals) to not forget it!
	\end{tcolorbox}
	So the purpose is first to find the primitives $F$ and $G$ of $f$ and $g$, and then to express it in terms of $x$:
	
	The integration constant is fixed when asked for a given $x=x_0$, we get a particular value of $y(x)=y(x_0)=y_0$. We speak then of "\NewTerm{initial value problem}\index{initial value problem}" or of "\NewTerm{initial conditions}\index{initial conditions}\label{initial conditions}" also abbreviated: IC. So, in other words, initial conditions are values of the solution and/or its derivative(s) at specific points.  As we will see eventually, solutions to ... nice enough ... differential equations are unique and hence only one solution will meet the given conditions. The number of initial conditions that are required for a given differential equation will depend upon the order of the differential equation as we will see.
	
	\subsubsection{Linear differential equations}\label{linear differential equations}
	\textbf{Definition (\#\mydef):} A differential equation of order $n$ is named "\NewTerm{linear differential equation L.D.E.}\index{linear differential equation}" if and only if it is of the form:
	
	with:
	
	Let us now see a property that may seem insignificant at first glance but which will become very important later!
	
	We will prove now that $L$ is a linear application:
	
	and for all $\lambda \in \mathbb{R}$
	
	Then we say that the linear D.E. represent a linear model if the multiples of this function (or any linear combination) are also a solution. Thus, in physics, for a linear system, the amplification of the cause involves an amplification of the effect (the systems are often linear in high-school problems but in reality they are rather the exception!).
	
	For example, the ordinary differential equation of order $2$ of the simple pendulum proved in the section of Classical Mechanics is not linear because it contains a sine term that is not separable.

	\textbf{Definition (\#\mydef):} The linear differential equation (which is the most common in physics):
	
	is named "\NewTerm{homogeneous equation H.E.}\index{homogeneous equation}" or " \NewTerm{equation without second member E.W.S.M.}\index{equation without second member}" (and sometimes "\NewTerm{complementary equation}\index{complementary equation}") associated to:
	
	\begin{theorem}
	We will now prove an important property of H.E.: the set $\left\lbrace S_0 \right\rbrace$ of solutions of the H.E. is the kernel of the linear application $L$ (which means for refresh: $L(S_0)=0$) and the set  $\left\lbrace S \right\rbrace$ of solutions to $L(y)=f(x)$ is given by:
	
	that is to say that the solutions of the form:
	
	where $y_p$ is a "particular/specific solution" to $L(y)=f(x)$ and $y_h$ the "\NewTerm{homogeneous solution}\index{homogeneous solution}" give all the D.E. solutions.
	\end{theorem}
	\begin{dem}
	The first statement will be assumed obvious.
	
	As regards to the second part, any function of the form $y_p+y_h$ is solution of $L(y)=f(x)$.
	
	Indeed it is trivial and it follows from the definition of the kernel concept (\SeeChapter{see section Set Theory page \pageref{kerr}}):
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	What is important also to understand with the linear D.E. with second member, it is that if we find solutions to $L(y)$ with a second given member and solutions to the same D.E. with another different second member, then the sum of all these solutions will be a solution of the D.E. with the sum of the second members!!!\label{sum solutions of differential equations}
	
	\subsubsection{Resolution methods of differential equations}
	There are many ways to solve accurately or approximately linear or non-linear differential equations. Let us give the list of the few methods we will analyze further below by the example (but who are already many, many times in the chapters of Mechanics, Cosmology, Social Sciences and Quantum Physics):
	\begin{itemize}
		\item The "\NewTerm{method of characteristic polynomial of D.E.}\index{Differential equations!method of characteristic polynomial}" (see below) used a bit in every section of the chapter of Mechanics/Quantum Physics/Cosmology/Chemistry and Social Sciences of this book.
		
		\item The "\NewTerm{method of integrating factor}\index{Differential equations!method of integrating factor}" (see also below) for general knowledge but used to this date for practical cases in this book.
		
		\item The "\NewTerm{method of variation of the constant}\index{Differential equations!method of variation of the constant}" (see below) and used to this date only in the section of Industrial Engineering in this book.
		
		\item The "\NewTerm{method of disturbances of D.E.}\index{Differential equations!method of disturbances}" (see below) useful for the wave quantum physics and quantum field theory.
	\end{itemize}
	
	Note also other widely used methods (classical high-school technics) but that are treated case by case in the individual sections of this book because solving approaches are too numerous and specific to each problem:
	
	\begin{itemize}
		\item The "\NewTerm{separation of variables method of D.E.}\index{Differential equations!separation of variables}" (the heat equation in the section of Thermodynamics, wave equation in the section Marine \& Weather Engineering, Schrödinger evolution equation in the section of Wave Quantum Physics, vibration of a drum in the section of Wave Mechanics, etc.), whose we will see a very specific and simple case lower but for which it is best to refer to the sections mentioned for concrete examples.
		
		\item The "\NewTerm{matrix method for solving D.E.}\index{Differential equations!matrix method}" and "\NewTerm{trivial solution of D.E.}" (Lotka-Volterra model in the section of Populations Dynamics, electron or nuclear spin resonance in the section of Relativistic Quantum Physics, Lorenz model in the section of Marine \& Weater Engineering, etc.).
		
		\item The "\NewTerm{spectral method}\index{Differential equations!spectral method}" using the spectral theorem proved in the section of Linear Algebra page \pageref{spectral theorem} (see the section of Industrial Engineering for the calculation of system reliability by Markov chains for a concrete example).
		
		\item The "\NewTerm{method of the Fourier transform of the D.E.}\index{Differential equations!Fourier transform method}" or "\NewTerm{method of the Laplace transform of the D.E.}\index{Differential equations!Laplace transform method}" (heat equation in the section Thermodynamics, resolution of the Black \& Scholes equation in the section of Economy, beam equation  under point load in the section of Civil Engineering).
		
		\item "\NewTerm{Numerical methods for D.E.}\index{Differential equations!numerical method}" to solve the differential equations using computer when the D.E. have no known analytic solutions or when they have but we need a visual three dimensional view of the solutions (heat equation in the section of Theoretical Computing).
		
		\item The "\NewTerm{Frobenius method}\index{Differential equations!Frobenius method}" named after Ferdinand Georg Frobenius and  also "\NewTerm{power series solutions}\index{Differential equations!power series solutions}", that is a way to find an infinite series solution for a second-order ordinary differential equation of a special form. We will use this technique in the section of Sequences and Series for our study of the Bessel series and also introduce Bessel series by solving in the section of Mechanical Engineering the probel of the self-buckling column with the power series solutions.
	\end{itemize}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The first differential equations were solved around the end of the 17th century and beginning of the 18th century. By the middle of the 18th century people realized that the first methods we listed above had reached a dead end. One reason was the lack of functions to write the solutions of differential equations. The elementary functions we use in calculus, such as polynomials, quotient of polynomials, trigonometric functions, exponentials, and logarithms, were simply not enough. People even started to think of differential equations as sources to find new functions. It was matter of little time before mathematicians started to use power series expansions to find solutions of differential equations. Convergent power series define functions far more general than the elementary functions from calculus.
	\end{tcolorbox}
	
	\paragraph{Method of characteristic polynomial}\label{method of characteristic polynomial}\mbox{}\\\\
	Solving simple differential equations (with constant coefficients and without second member most of the time...) uses a technique using a characteristic polynomial of the differential equation which we will see the details in the developments that follow on  few special cases very frequent in physics.
	
	It is a relatively simple method to implement when we seek solutions to the homogeneous differential equation without second member (E.W.S.M.). In the contrary case, the presence of a second member, we add the solutions of the homogeneous equation to the particular solutions.
	
	\subparagraph{Resolution of the H.E. of the first order L.D.E. with constant coefficients}\label{first order lde with constant coefficients}\mbox{}\\\\

	Consider the following L.D.E. with constant coefficients:
	
	which is a simplified version of the following general L.D.E.  with constant coefficients:
	
	where:
	
	We write its associated homogeneous equation (E.W.S.M.):
	
	Which can be written:
	
	Therefore:
	
	There is behind this homogeneous solution infinite solutions: to each value given to the constant $C$ there is a solution.
	
	We still need to add to this homogeneous solution the particular solution $y_p$ and for that we have a collection of recipes, depending on the type of the function $f (x)$ of the second member of the differential equation. We will see in each case in the various chapters this book as already mentioned.
	
	\subparagraph{Resolution of the H.E. of the first order L.D.E. with non-constant coefficients}\mbox{}\\\\
	The general solution of homogeneous linear differential equations (E.W.S.M.) of order $1$ with non constant coefficients:
	
	can always be reduced to the following form:
	
	where:
	
	Well obviously there is the solution $y=0$... but let us try to do better. So we have:
	
	It comes therefore:
	
	where $G (x)$ is a primitive of $g (x)$. Since then:
	
	It is also common to find these developments in another notation a little bit more explicit.
	
	So we start again of the differential equation without second member with non-constant coefficients:
	
	after rearrangement:
	
	And then:
	
	Therefore:
	
	This result will be very useful to calculate the Fourier transform of a Gaussian function (\SeeChapter{see section Sequences and Series page \pageref{fourier transform gaussian function}}), Fourier transform, which is essential to resolve in a fairly general way the heat equation (\SeeChapter{see section Thermodynamics page \pageref{heat equation}}) resolution that will finally allow us to prove the Black \& Scholes equation (\SeeChapter{see section Economy page \pageref{solving black and scholes}}). 
	
	\subparagraph{Resolution of the H.E. of the second order L.D.E. with constant coefficients}\mbox{}\\\\\label{second order differential equations}
	Consider the L.D.E. with constant coefficients:
	
	which is a simplified version of the following general L.D.E. with constant coefficients:
	
	where:
	
	We write is homogeneous associated equation (E.W.S.M.):
	
	wherein the function of the second member is zero. We can quite quickly consider a solution of the type (inspiring of the form of the solutions of the first order differential equations):
	
	where $\tau$ is a constant. Which give us therefore:
	
	What we can simplify by:
	
	If our starting assumption is correct, we only have to solve in $K$ this "\NewTerm{characteristic equation (CHARE)}\index{characteristic equation}\label{characteristic equation}"  or "\NewTerm{characteristic polynomial}\index{characteristic polynomial}" of the homogeneous equation to find the homogeneous solution:
	
	whose solutions depend on the sign of the discriminant of the characteristic polynomial\label{discriminant differential equation}:
	
	\begin{itemize}
		\item If the discriminant is strictly positive, that is to say $\Delta>0$:
		
		So we know that the characteristic polynomial has two distinct roots and then we have:
		
		where $K_1\tau=c^{te}$ and $K_2\delta=c^{te'}$. Then we say that the solution is "\NewTerm{delayed}\index{Differential equations!delayed solution}" or "\NewTerm{advanced}\index{Differential equations!advanced solution}" by the values of these constants. But the key is to note that if $y_h(x)$ is a solution, then $y_h(x\pm \Delta x)$ is always a solution!
		
		We then speak of "\NewTerm{general solution of the homogeneous equation}\index{general solution of the homogeneous equation}". There is behind this result an infinity of solutions: to each value given to the constants $A, B$ corresponds a solution.
		
		Physicists also write sometimes this in a particular form by putting
		
		with then:
		
		And using the hyperbolic trigonometric functions (\SeeChapter{see section Trigonometry page \pageref{hyperbolic trigonometry}}):
		
		where finally the possibility to write the homogeneous solution in the form (when we omit the advance or delay $\delta=\tau=0$):
		
		In addition, let us show that the solutions of the E.W.S.M. form a vector space of dimension $2$ (corresponding to the order of our differential equation)!
		
		Indeed:
		\begin{itemize}
			\item The zero function: $y=0$ is a solution of the E.W.S.M. (this is unnecessary to prove because obvious...!)
			
			\item The sum or subtraction of solutions remains a solution (this we have already proved it before)
			
			\item The elements of the basis of a vector space (the solutions of the E.W.S.M.) are linearly independent (that's an interesting property that we will need later!)
		\end{itemize}
		Let us put:
		
		Then:
		
		These relations injected into the E.W.S.M. in generalized form:
		
		Then gives:
		
		We do have indeed a vector space structure.
		
		Let us recall that conversely two functions are linearly dependent if:
		
		\item If the discriminant is strictly positive, that is to say $\Delta=0$: 
		
		The characteristic equation has a real double root $K$.
		
		By going a little fast we would say then:
		
		and that it is over... but that it is forget that the vector base must be formed of two independent solutions!
		
		So the second option is probably... of the form:
		
		Then:
		
		If we inject it into the E.W.S.M. in generalized form:
		
		Then:
		
		That is to say in our case:
		
		But, both actual real values of $K$ are precisely solutions of:
		
		The prior-previous relation is reduced to:
		
		and as we are in the case of study where the discriminant is zero, we have:
		
		Therefore and finally the prior-previous relation reduces to:
		
		We deduce from it that:
		
		Therefore finally:
		
		Which gives for the general solution of the E.W.S.M.:
		
		
		\item If the discriminant is strictly positive, that is to say $\Delta<0$:
		The characteristic equation has two complex conjugate roots (\SeeChapter{see section Calculus page \pageref{second order polynomial roots}}):
		
		Therefore:
		
		But if we look instead for real solutions, we can always put $A$ and $B$ as being equal such as:
		
		And if we set the delay and advance respectively as being zero ($\delta=\tau=0$), so we find the relation available in most books without proof:
		
		where $A'$ and $B'$ are any two real constants. There is another important form of this last relation (often used in electronics, for example). Indeed, it is possible for any $A'$ and $B' \in \mathbb{R}$ , to find $C'$ and $\phi$ also in $\mathbb{R}$ such as the following equality holds:
		
		We put:
		
		Then:
		
		It is then possible to find $\phi$ such as:
		
		Therefore our initial expression (proposition) can be written as:
		
		Finally:
		
	\end{itemize}
	So we can make the following summary\label{summary LDE with constant coefficients}:
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|c|c|c|}
					\hline
					\multicolumn{1}{c}{\cellcolor{black!30}\textbf{Discriminant}} & 
	  \multicolumn{1}{c}{\cellcolor{black!30}\textbf{Roots}}  & 
	  \multicolumn{1}{c}{\cellcolor{black!30}\textbf{Homogeneous solution}} \\ \hline
					$\Delta>0$ & \centering\arraybackslash\ $K_{1,2}=\dfrac{-a\pm \sqrt{a^2-4b}}{2}$ & $y_h=Ae^{K_1(x+\tau)}+Be^{K_2(x+\delta)}$ \\ \hline
					$\Delta=0$ & \centering\arraybackslash\ $K_{1,2}=\dfrac{-a\pm \sqrt{a^2-4b}}{2}$  & $y_h=(C_1 x+C_2)e^{K(x+\tau)}$ \\ \hline
					$\Delta<0$ & \centering\arraybackslash\ $K_{1,2}=\dfrac{-a\pm \sqrt{a^2-4b}}{2}$  & $y_h=e^{\alpha x}(A' \cos(\beta x)+B'\sin(\beta x))$ \\ \hline
			\end{tabular}
		\end{center}
		\caption{Typical solutions of H.E. of The L.D.E. with constant coefficients}
	\end{table}
	
	\paragraph{Integrating Factor Method (Euler's Method)}\mbox{}\\\\
	The technique of integrating factor is useful when it comes to solve differential equations of the form:
	
	We have not to this day practical application of this technique in the other chapters of this book. You must therefore see this as a presentation for general culture.
	
	The basic idea is to find a function $M(x)$, named "\NewTerm{integration factor}\index{integration factor}", by which can be multiplied by our differential equation to bring the left-hand side of equality to a simple derivative. For example, for a linear differential equation as the one above, we choose often the following integration factor (but this is by far not the only possibility and this choice does not solve everything!):
	
	Therefore we have:
	
	or by distributing:
		
	Which can therefore be seen as:
	
	or even better (and therein lies the whole trick)...:
	
	We can then take the primitive with respect to $x$:
	
	We can then take the primitive with respect to $x$:
	
	and trivially (!) we have the left primitive that is immediate:
	
	Which is sometimes written as:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider the following differential equation:
	
	That we will rewrite as:
	
	We see then that (assuming $x$ is positive):
	
	Then we have:
	
	Hazard making  sometimes things good (the example is purposely very simple), we have this equality that simplified as:
	
	in:
	
	Which may be condense in:
	
	By integrating:
	
	It then comes immediately:
	
	Finally:
	
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Method of separation of variables}\label{separation vaiables method}\mbox{}\\\\
	The method of separation of variables (also known as the "\NewTerm{Fourier method}\index{Fourier method}") is any of several methods for solving ordinary and partial differential equations, in which algebra allows one to rewrite an equation so that each of two variables occurs on a different side of the equation.
	
	In mathematics, a "\NewTerm{partial differential equation PDE}\index{partial differential equation}" is a differential equation that contains unknown multivariable functions and their partial derivatives (a special case are ordinary differential equations, which deal with functions of a single variable and their derivatives). PDEs are used to formulate problems involving functions of several variables, and are either solved by hand, or used to create a relevant computer model.
	
	PDEs can be used to describe a wide variety of phenomena such as sound, heat, electrostatics, electrodynamics, fluid dynamics, elasticity, or quantum mechanics. These seemingly distinct physical phenomena can be formalized similarly in terms of PDEs. Just as ordinary differential equations often model one-dimensional dynamical systems, partial differential equations often model multidimensional systems. PDEs find their generalization in stochastic partial differential equations.
	
	A partial differential equation (PDE) for the function $U(x_{1},\cdots ,x_{n})$ is an equation of the form:
	
	If $f$ is a linear function of $U$ and its derivatives, then the PDE is named a "\NewTerm{linear partial differential equation}\index{linear partial differential equation}". Common examples of linear PDEs include the heat equation, the wave equation, Laplace's equation, Helmholtz equation, Klein–Gordon equation, and Poisson's equation (see the chapters of Mechanics, Electrodynamics and Quantum Physics for the study of most of them!).
	
	The method of separation of variables is a very common technique used in physics when we have second-order differential equations. Many useful examples and very detailed are already in the various chapters already mentioned above. Here we will just present a special case just for doing things good but with the minimum subsistence level!
	
	Consider the common case of physical partial differential equation of the type:
	
	The solution of this equation therefore requires finding a function $U$ which depends on $x$ and $y$ such that:
	
	In physics, the idea is then to put that we can always find a said a "separable" solution of the form:
	
	Thus, the differential equation can be written as:
	
	Which can be rewritten as:
	
	Or:
	
	After rearrangement it is use in physics to note this last equality in condensed form:
	
	This equality can only take place if each term is a constant since $X$ depends only on $x$ and $Y$ only on $y$. It comes then:
	
	And each differential equation can then be solved independently of the other and once the solutions found we multiplied them determine the expression of $U$.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Probably the most beautiful example is the section of Quantum Chemistry.
	\end{tcolorbox}
	How do we know that this technique of separation of variables is valid? When the differential operators in various variables are additive in the partial differential equation, that is, when there are no products of differential operators in different variables, the separation method usually works. We are proceeding in the spirit of \textit{let’s try and see if it works}... If our attempt succeeds, then this technique will be justified. If it does not succeed, we shall find out soon enough and then we shall try another attack, such as Green’s functions, integral transforms, or brute-force numerical analysis!
	
	\paragraph{Method of constant variation}\mbox{}\\\\
	The variation of constants, is a general method to solve inhomogeneous linear ordinary differential equations.

	For first-order inhomogeneous linear differential equations it is usually possible to find solutions via integrating factors or undetermined coefficients with considerably less effort, although those methods leverage heuristics that involve guessing and don't work for all inhomogeneous linear differential equations.

	Variation of parameters extends to linear partial differential equations as well, specifically to inhomogeneous problems for linear evolution equations like the heat equation, wave equation, and vibrating plate equation. In this setting, the method is more often known as "\NewTerm{Duhamel's principle}\index{Duhamel's principle}", named after Jean-Marie Duhamel who first applied the method to solve the inhomogeneous heat equation.
	
	The idea of the method of variation of the constant is as follows: if we have a particular solution affected by constants, we know that depending of the initial conditions thereof are well determined. The idea is then to generalize by putting that these constants are functions. In some cases obviously mathematical developments will show that the functions are necessarily constant.
	
	The idea behind this method is to say that the solutions of the (linear) differential equation with the second member will look like the solutions of the homogeneous equation. As the term on the right will disrupt this solution, we vary only constants (which will therefore no longer be constants), but we remain on the "base" of homogeneous solutions, to seek close solutions. After we check that this "physicist reasoning" gives out all the solutions of the differential equation.
	
	Let us see before studying to the general case a simple example by considering the following differential equation:
	
	for which the particular solution of the homogeneous equation (E.W.S.M.) is (if you need the details not hesitate to ask!):
	
	The method of variation of the constant consist then to put that:
	
	and therefore:
	
	But by the differential equation with the second member, we have:
	
	
	and it follows that:
	
	where we eliminated the integration constant because what we want is a particular solution! The particular general solution (pg) is then the sum of the particular solution and the homogeneous one and this with the variation of the constant:
	
	Thus, generalizing the previous example, so we have a differential equation of the form:
	
	General particular solution will be:
	
	Then we have:
	
	hence injected into the original differential equation:
	
	Therefore after factoring similar terms:
	
	So we have the above relation and the particular solution to the homogeneous differential equation (therefore without second member):
	
	Therefore we find:
	
	and it sufficient then to integrate this equation to find $C_0(t)$. Then, the particular general solution (pg) will be the sum of the particular homogeneous solution and that with the variation of the constant.
	
	\pagebreak
	\subsubsection{Classification of partial differential equations}
	Before we begin, the reader wonders what classifying differential equations can be used for, well, here are the two main arguments for the usefulness of a classification in the order of importance:
	\begin{itemize}
		\item Many books have authors who systematically speak of a differential equation by categorizing it, so it is more pleasant to know what they are talking about

		\item Some finite element modeling software (including MATLAB™) requires that the differential equation category be chosen before anything else can be done as a calculation.
	\end{itemize}
	So this being said, formally, we name "\NewTerm{partial differential equation PDE}\index{partial differential equation}" of order less than or equal to $2$ in a domain $\Omega \subset \mathbb{R}^n$ and of unknown:
	
	an equation of the following general type:
	
 	where $s(x)$ is often named a "\NewTerm{source term}\index{source term}" in analogy with the main situation in physics concerned.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/algebra/ode_vs_pde.jpg}
	\end{figure}
	It is now important to generalize the latter equation in vector form. Thus, we introduce the following notations:
	\begin{itemize}
		\item $A=[a_{ij}]$ the $n\times n$ symmetric matrix of the coefficients in front of the terms of order $2$

		\item $B=(f_i(x))$ The vector of size $n$ of the coefficients in front of the terms of order $1$

		\item $[H\Phi(x)]$ the $n\times n$ symmetric Hessian matrix (\SeeChapter{see section Sequences and Series page \pageref{hessian matrix}}) of $\Phi$:
		

		\item $\vec{\nabla}(\Phi(x))$ the vector of size $n$ of $\Phi$:
		

		\item The notation (named "\NewTerm{Frobenius (matrix) dot product}\index{Frobenius (matrix) dot product}"):
		
		also sometimes denoted as the classical dot product: $\langle A,B\rangle$.
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		The "\NewTerm{Frobenius norm}\index{Frobenius norm}" of a matrix $\mathbb{R}^{m\times n}$ is given by:
		
		\end{tcolorbox}
	\end{itemize}
	With this, the previous relation:
	
	can be rewritten as:
	
	What is often (abusively) written in a very condensed way:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The following PDE:
	
	First, it is easy to determine (because it is a definition) that:
	
	It is also trivial that:
	
	and that:
	
	It is also trivial that:
	
	Finally, the minor difficulties are to find:
	
	\end{tcolorbox}
	For linear PDE of order $2$, the matrix $A$ is non-zero and is symmetric. It is therefore diagonalizable with real eigenvalues (\SeeChapter{see section Linear Algebra page \pageref{eigenvector}}) and their study provides elements of classification of linear PDE systems of order $2$ under the denomination of "Elliptic", "hyperbolic" or "parabolic" PDE (\SeeChapter{see section Analytical Geometry page \pageref{classification of conical by the determinant}}).

	It is often customary to say in physics that the elliptics PDE characterize problems of equilibrium or stationarity, that hyperbolics PDE characterize problems of wave propagations and finally that parabolic PDE characterize diffusion problems (see examples further below).
	
	That latter terminology comes from the fact that when the matrix $A$ is constant, the curves:
	 
	are respectively ellipsoids, hyperboloid and paraboloid (\SeeChapter{see section Analytical Geometry page \pageref{type of conics matrix approach}}).

	Indeed, we have proved in the section of Analytic Geometry that following the determinant of $A$, we had:
	\begin{itemize}
		\item If $\det(A)=ac-b^2>0$, the curve $\Gamma$ is either empty, reduced to a point, or an ellipse.

		\item If $\det(A)=ac-b^2<0$, the curve $\Gamma$ is either the union of two intersecting lines, that is to say an hyperbola.

		\item If $\det(A)=ac-b^2=0$, the curve $\Gamma$ is either empty, a line, or two distinct parallel lines, or a parabola.	
	\end{itemize}
	More explicitly what we have seen above can be reformulated as following. If we have the following PDE:
	
	where $a$, $b$, $c$, $d$, $e$ and $f$ are real constants is said to be:
	\begin{itemize}
		\item An "\NewTerm{elliptical partial differential equation}\index{elliptical partial differential equation}" if $ac-b^2>0$ that is to say if the eigenvalues are all positive or all negative (\SeeChapter{see section Analytical Geometry page \pageref{type of conics determinant}}).

		\item An "\NewTerm{hyperbolic partial differential equation}\index{hyperbolic partial differential equation}" if $ac-b^2<0$ that is to say there only one negative eigenvalue and all the rest are positive, or there is only one positive eigenvalue and all the rest are negative (\SeeChapter{see section Analytical Geometry page \pageref{type of conics determinant}}).

		\item A "\NewTerm{parabolic partial differential equation}\index{parabolic partial differential equation}\label{parabolic partial differential equation}" if $ac-b^2=0$ that is to say if the eigenvalues are all positive or all negative, save one that is zero (\SeeChapter{see section Analytical Geometry page \pageref{type of conics determinant}}).
	\end{itemize}

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The Laplace's equation:
	
	is an elliptic PDE.\\

	The wave equation:
	
	is a hyperbolic PDE.\\

	The heat equation:
	
	is a parabolic PDE.
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Systems of Differential Equations}
	Let us study now special developments that will also be useful for the study of quantum physics or for the resolution of particular systems of differential equations (see corresponding section in this book for the details on these examples) and especially one which is well known in chaos theory!
	
	Let us first indicate to the reader before going further that more complex inhomogeneous case (with second member) and with unknown coefficients is treated directly by an example in the section of Industrial Engineering during the study of the reliability of a repairable system as a Markov chain with resolution using the determinants and eigenvalues/eigenvectors.
	
	To start this first approach, we will have to introduce the concept of exponentiation of a matrix:

	The set of matrices $n \times n$ with coefficients in $\mathbb{C}$ denoted $M_n(\mathbb{C})$ is a vector space for the addition of matrices and multiplication by a scalar. We will as always denote by $\mathds{1}_n$ the identity matrix of dimension $n$.
	
	We will admit that a sequence of matrices $A_n$ converges to a matrix $A$ if and only if the sequences of coefficients of the matrices $A_n$ converge towards the corresponding coefficients of $A$.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	In $M_2(\mathbb{C})$ the sequence of matrice:
	
	converge to:
	
	when $n\rightarrow +\infty$.
	If $x\in \mathbb{C}$, we saw in our study of complex numbers (\SeeChapter{see section on Numbers page \pageref{taylor expansion complex exponential}}) that the series:	
	
	does converge and its limit is denoted by $e^x$. In fact here there is no difficulty in replacing $x$ by a matrix $A$ since we know (we have proved it during our study of complex numbers) that any complex number can be written as follows (the body of complex numbers is isomorphic to the field of real matrices of square dimensions $2$ having this form):
	
	\end{tcolorbox}
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	and that a complex number is equivalent to put his matrix form also to the square:
	
	Indeed:
	
	\end{tcolorbox}
	We then define the "\NewTerm{exponential of a matrix}\index{exponential of a matrix}\label{exponential of a matrix}" $A\in M_n(\mathbb{C})$ as the matrix limit of the sequence:
	
	If the matrix $A$ is diagonal obviously its exponential is easy to calculate. Indeed, if:
	
	It follows:
	
	This property of diagonal matrices will also be very useful to us in the section of General Relativity during our study of the variation of the metric determinant.
	
	However, it is clear that a non-diagonal matrix will be much more complicated to deal with! We will then use a diagonalization technique or an endomorphisms reduction\index{endomorphism reduction}\footnote{In linear algebra, an "endomorphism reduction" aims to express matrices and endomorphisms in a simpler form, for example to facilitate calculations. This consists essentially in finding a decomposition of the vector space into a direct sum of stable subspaces on which the induced endomorphism is simpler. Less geometrically, this corresponds to finding a basis of the space in which the endomorphism is simply expressed like for example the spectral theorem (\SeeChapter{see section Linear Algebra page \pageref{spectral theorem}}).} (\SeeChapter{see section Linear Algebra page \pageref{matrix endomorphism}}) to simplify our problem.
	
	So notice that if $S\in M_n(\mathbb{C})$ is reversible and if $A\in M_n(\mathbb{C})$ then:
	
	This follows from the fact that (think to the change of basis of a linear application as it has been studied in the section of Linear Algebra page \pageref{change of basis}... perhaps it may help):
	
	Therefore:
	
	This development will enable us to bring the computing of the exponential of a diagonalizable matrix in search of its eigenvalues and its eigenvectors.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	Let us calculate $e^A$ where:
	
	The eigenvalues of $A$ are $\lambda_1=-3,\lambda_2=7$, and associated eigenvectors are:
	
	Indeed:
	
	By putting:
	
	We get:
	
	with:
	
	Therefore:
	
	\end{tcolorbox}
	Now, let us recall that in the case of real numbers we know that if $x,y\in \mathbb{R}$ then:
	
	In the case of matrices we can prove that if $A,B\in M_n(\mathbb{C})$ are two matrices that commute with one another, that is to say such that $AB=BA$, then:
	
	The condition of commutativity comes from the fact that the addition in the exponential is itself commutative. The proof is therefore intuitive.
	
	An important corollary of this proposition is that for any matrix $A\in M_n(\mathbb{C})$, $e^A$ is reversible. Indeed the matrices $A$ and $-A$ commute and therefore:
	
	We recall that a matrix $A$ with complex coefficients is unitary if:
	
	The following theorem will serve us later:
	\begin{theorem}
	Let us prove that if $A$ is a Hermitian matrix (also named "self-adjoint") (\SeeChapter{see section Linear Algebra page \pageref{self-ajdoint matrix}}) then for any $t\in\mathbb{R}$, $e^{\mathrm{i}tA}$ is unitary.
	\end{theorem}
	\begin{dem}
	
	Therefore:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Remember that this condition for a self-adjoint matrix is linked to the definition of linear group of order $n$ (\SeeChapter{see section Set Algebra page \pageref{unitary linear group}}).
	
	One of the first applications of the exponentation of matrices is the resolution of ordinary differential equations. Indeed, from the linear differential equation below using as initial condition $y(0)=0$ and where $A$ is a matrix:
	
	the solution is given by (as seen previously):
	
	We frequently find that kind of systems of differential equations in biology (population dynamics), astrophysics (study of plasmas) or in fluid mechanics (chaos theory) and in classical mechanics (coupled systems), astronomy (coupled orbits), in electrical engineering, etc.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Suppose we have the following homogeneous system of differential equations (without constant terms):
	
	The associated matrix is then:
	
	and its exponential (see developments made above):
	
	The general solution of the system is:
	
	So we have:
	
	By calculating the derivative of the previous relations and comparing to:
	
	we easily determine the constants to get:
	
	which finally gives us:
	
	\end{tcolorbox}

	\subsection{Regular Methods of Perturbations}\label{regular methods of perturbations}
	Very frequently in physics (high level physics) or financial engineering, a mathematical problem can not be solved exactly. Even if the solution is known sometimes there are such a dependency of parameters that the solution is difficult to use as such.
	
	Sometimes, however, it happens that an identified parameter of the differential equation, which we denote by tradition with the Greek letter $\varepsilon$, is such that the solution is available and reasonably simple for $\varepsilon=0$.
	
	The problem then is to know how the solution is altered for a non-zero $\varepsilon$ but still small. This study is the center of "\NewTerm{perturbation theory}\index{perturbation theory}" that we will use, for example in the section of General Relativity to calculate the precession of the perihelion of Mercury.
	
	As the perturbation theory within the general framework is too complex for this book purpose, we propose an approach by example with first a simple algebraic equation and then with what interests us: a differential equation.
	
	\subsubsection{Perturbation theory for algebraic equations}
	Consider the following polynomial equation:
	
	We know from our study of the section Functional Analysis, that this polynomial equation has two roots that are trivially:
	
	For small $\varepsilon$, these roots can be approximated by the first term of the Taylor series expansion (\SeeChapter{see section Sequences and Series page \pageref{taylor series}}):
	
	The question is whether we can get the two previous relation without a priori knowledge of the exact solution of the initial polynomial equation? The answer is obviously: YES with the help of perturbation theory.
	
	The technique is based on four steps:
	\begin{enumerate}
		\item In the first step, we assume that the solution of the polynomial equation is an expression of the type Taylor series on $\varepsilon$. Then we have:
		
		where $X_1,X_2,X_3$ are obviously to be determined.
		\item In the second step, we inject the solution in our hypothetical polynomial equation:
		
		As:
		
		and:
		
		It finally comes that the polynomial equation can be written as:
		
		
		\item In the third step we successively equalize the terms with 0 such as:
		
		\item Fourth and last step, we solve successively the polyinomial equations above to get:
		
		By injecting these results in the hypothetical solution:
		
		it is obvious to observe that we fall back on the certain solution:
		
	\end{enumerate}
	
	\pagebreak
	\subsubsection{Perturbation theory of differential equations}
	Perturbation theory is therefore also often used to resolve numerous differential equations. This is the case for example in fluid mechanics, in General Relativity or quantum physics.
	
	Again, rather than doing a super abstract and general theory, we will see the concept with an example as previously.
	
	Consider the following ordinary differential equation with second member and constant coefficients:
	
	Or written in another way?
	
	with the boundary conditions:
	
	The exact resolution is relatively easy to obtain:
	
	First we start with the homogeneous equation:
	
	So it is a linear differential equation of order 2 with constant coefficients, equation that it is relatively easy to solve in the general case. Given the equation:
	
	Assume that the function $y$ which satisfies this differential equation is the of the form $y=e^{Kx}$ where $K$ may be a complex number. Then we have:
	
	provided, of course, that $e^{Kx}\neq 0$. This last relation is the auxiliary quadratic equation of the differential equation (characteristic polynomial in other words). It has two solutions/roots (it's a simple resolution of a polynomial of the second degree) which we denote in the general case: $K_1,K_2$. Which means that:
	
	are satisfied for the two roots. If we do the sum, since both are equal to the same constant:
	
	Thus, it is immediate that the general solution of the homogeneous equation is of the type:
	
	where $A, B$ are obviously constants to be determined. Now we solve the characteristic polynomial:
	
	It comes immediately that:
		
	Therefore:
	
	Now a particular solution to:
	
	is relatively trivially a solution of the type:
	
	where $B$ is of course a constant to be determined and which is equal (once injected into the differential equation):
	
	Therefore:
	
	Hence finally the general solution:
	
	Then, with the initial conditions that are a for reminder:
	
	it is very easy to find A:
	
	We also have:
	
	We are free to choose that $c^{te}=0$ which gives us:
	
	Then:
	
	becomes:
	
	Now that we have the general solution if $\varepsilon$ is small we can take the development of order $4$ in Maclaurin series of the exponential (\SeeChapter{see section Sequences and Series page \pageref{euler maclaurin expansion}}). Such as:
	
	Injected into $y$ this gives (you will notice that we sometimes express explicitly... the term of order 5 by anticipation...):
	
	Now that we have this development, what we want to show is that from a perturbative expansion we can find the same result in series and this without any prior knowledge on the solution.
	
	Again, the development is done in 4 steps:
	\begin{enumerate}
		\item In the first step, we assume that the solution of differential equation is an expression of the type Taylor series on  $\varepsilon$. Then we have:
		
		where $y_0,y_1,y_2$ are obviously to be determined.
		
		\item In the second step, we inject the hypothetical solution of our differential equation in itself with the initial conditions and we develop the whole.
		
		then the initial conditions:
		
	
		\item In the third step we equalize successively the terms with $0$ such as:
		
		
		\item In the fourth step we solve the differential equations listed above (if you do not see how we solve them do not hesitate to contact us!):
		
		By injecting relations in the supposed solution developed in Taylor series and injected into the differential equation:
		
		We fall back on:
		
	\end{enumerate}
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{95} & \pbox{20cm}{\score{4}{5} \\ {\tiny 119 votes,  75.45\%}} 
	\end{tabular} 
	\end{flushright}
	
	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Sequences and Series}
	
\lettrine[lines=4]{\color{BrickRed}S}equences and series have a great importance in Applied Mathematics and that is why we devote to them a whole section. We will also see them often in various sections of the Mechanics chapters when we need to make some minor approximations (...) as well as in the sections of Economy and Quantitative Management Techniques. The reader should try to not confuse in what follows the concept of "sequences" with that of "series" which, while being similar in substance, are not always analyze mathematically in the same way.

We wanted to study in this section simple things without going to far within the topological concepts of sequences and series. However, those interested in more rigorous definitions can read the sections Fractals (\SeeChapter{see chapter of Theoretical Computing page \pageref{fractals}}) and Topology where many concepts about series are  (supremum, infimum, subsequence, Bolzano-Weierstrass' theorem, etc.).

\subsection{Sequences}

\textbf{Definition (\#\mydef):} A "\NewTerm{sequence}\index{sequence}" of a set is a family of elements indexed by the set of natural numbers (\SeeChapter{see section Numbers page \pageref{natural numbers}}) or by a part of it. In a vulgarized way we say that a sequence is a list of objects put in order, each with a order number. We typically write a sequence as:
	
where indexing is sometimes (by tradition...) without the 0.

For some sequences, we provide the first term $u_1$ (if indexing starts with 1 instead of 0), and a formula for any term $u_{n+1}$ from the previous term $u_n$ regardless $n \geq 1$. We call such a formulation a "\NewTerm{recurring definition}\index{recurring definition}" and the sequence is defined "\NewTerm{recursively}\index{recursively defined sequence}" (and even if it is indexed from 0 instead of 1).

Before seeing some examples of sequences families that will be used in the various sections of the book (Population Dynamic, Economy, Nuclear Physics, etc.) let us see a small set of definitions as it is the tradition in mathematics...

\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] Numbers (as sequence) are in "\NewTerm{arithmetic progression}\index{Sequence!arithmetic progression}" if the difference of two consecutive terms is equal to a constant $r$ named the "\NewTerm{reason}".
		
		\item[D2.] Numbers (as sequence) are in "\NewTerm{geometric progression}\index{Sequence!geometric progression}" if the ratio of two consecutive terms is equal to a constant $r$ also named the "\NewTerm{reason}".
		
		\item[D3.] Numbers (as sequence) are in "\NewTerm{harmonic progression}\index{Sequence!harmonic progression}" if the inverse of two consecutive terms are in arithmetic progression.
	\end{enumerate}

	Therefore, a number $b$ is respectively the arithmetic mean, geometric, harmonic of $a$ and $c$ if the numbers $a, b, c$ are respectively in  an arithmetic, geometric or harmonic progression.

	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
For the definitions of the averages listed above see the section Statistics.
	\end{tcolorbox}
	
\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] A "\NewTerm{majorated sequence}\index{majorated sequence}" is a sequence such that there is a real number $M$ such that:\\ $\forall n \in \mathbb{N}, \; u_n \leq M$
		
		\item[D2.] A "\NewTerm{minorated sequence}\index{minorated sequence}" is a sequence such that there is a real number $m$ such that:\\ $\forall n \in \mathbb{N}, \; u_n \geq m$
		
		\item[D3.] A "\NewTerm{bounded sequence}\index{bounded sequence}" is a sequence that is both majorated and minorated.
		
		\item[D4.] A sequence $(u_n)$ is named  "\NewTerm{increasing sequence}\index{increasing sequence}" if $\forall n \in \mathbb{N}, \; u_{n+1}-u_n > 0$
		
		\item[D5.]  A sequence $(u_n)$ is named  "\NewTerm{decreasing sequence}\index{decreasing sequence}" if $\forall n \in \mathbb{N}, \; u_{n+1}-u_n < 0$
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	If a sequence is increasing or decreasing, we sometimes just say it is a "\NewTerm{monotonous sequence}" (without specifying if its increasing or decreasing).
		\end{tcolorbox}
		
		\item[D6.]  A sequence $(u_n)$ is named  "\NewTerm{constant sequence}\index{constant sequence}" if $\forall n \in \mathbb{N}, \; u_{n+1}=u_n$
	\end{enumerate}
	
	We will now see some practical important arithmetic and geometric sequences that will be used later in other sections of this book.
	
	But keep in mind during the lecture that we can express sequences mot of time in two ways:
	\begin{itemize}
		\item A "closed formula" or "closed relation" for a sequence $(u_n)_{n\in\mathbb{N}}$ is a relation for $u_n$ using a fixed finite number of operations on $n$ only (and nothing else!). This is what we normally think of as a function of $n$ only,
	
		\item A "recursive definition" for a sequence $(u_n)_{n\in\mathbb{N}}$ consists of a recurrence relation: a relation relating a terme of the sequence to previous terms (terms with smaller index $n$) and an initial condition.
	\end{itemize}
	
	\pagebreak
\subsubsection{Arithmetic Sequences}

\textbf{Definition (\#\mydef):} We say that numbers or "\NewTerm{terms}\index{term of a sequence}" are in an "\NewTerm{arithmetic sequence}" when the difference of their sequential value is equal to a constant $r$ named the "\NewTerm{reason}\index{reason of an arithmetic sequence}" of the sequence so that (recursive relation):
	
where $r$ is the "reason" of the progression. We then obviously have if the indexing starts from $0$:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Examples:}\\\\
E1. The sequence:
	
where $n$ is a constant and the reason is equal $1$.\\\\
E2. The sequence:
	
	is an arithmetic sequence of reason $x$.
	\end{tcolorbox}
	Thus, if we write $u_n$ any term of the sequence $(u_n)$ of reason $r$, we have:
	
	We have the following properties for this type of sequences:
	\begin{enumerate}
		\item[P1.] A term whose rank is the average of the ranks of the other two terms is the arithmetic mean of these two terms.
		\begin{dem}
		Consider now $(u_n)$ an arithmetic sequence of reason $r$ given by the previous development:
			
and $a,b,k \in \mathbb{N}$ such as $a+b=2k$, then we have:
			
and so:
		
with $k=\dfrac{a+b}{2}$.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
\end{dem}
	\item[P2.] For three consecutive terms $u_n,u_{n+1},u_{n+2}$ in an arithmetic sequence of reason $r$, the second term is the arithmetic mean of the other two.
		\begin{dem}
			Let us write:
				
				\begin{flushright}
					$\blacksquare$  Q.E.D.
				\end{flushright}
		\end{dem}
	\item[P3.] If $u_1,u_2,u_3,...,u_n,...$ is an arithmetic sequence of ratio $r$, then the $n$-th partial sum $S_n$ (that is to say, the sum of the first $n$ terms to the power of $1$) is given by:
		
when indexing is starts from $1$.
		\begin{dem}
			We can write the sequence:
				
		Playing with the second line, we get:
			
What can be simplified even more:
			
Considering that we will prove a little bit later that the simple following Gauss series:
			
is equal to:
			
We then have for:
			
the following relation:
			
We thus get:
			
We see with the latter relation that if $u_1=r=1$ we fall back on the simple Gauss series.

As:
			
when the indexation starts from 1 we thus get:
			
			\begin{flushright}
				$\blacksquare$  Q.E.D.
			\end{flushright}
		\end{dem}
We will see other types of summations a little bit further below during our study of series!
	\end{enumerate}
	
	\subsubsection{Harmonic Sequences}
	\textbf{Definition (\#\mydef):} We say that numbers $\dfrac{1}{a}, \dfrac{1}{b}, \dfrac{1}{c},...$ generates an "harmonic progression" when their inverses are in arithmetical progression (also with a "reason" $r$. We represent this progress by:
	
We then obviously if the indexing starts from 0:
	
Moreover, we assume, in what follows, that there is no zero denominator.

By sharing this type of sequences successively in groups containing $2^n$ terms, we observe that each of them is bigger than the last of his group. For example:
	
And we can see that the sum of the terms of each group is larger than 1/2.

We can also see that each term is the harmonic mean of the previous and consecutive one:

\begin{dem}
	
Thus:
	
So finally:
		
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
\end{dem}

	\subsubsection{Geometric Sequences}
\textbf{Definition (\#\mydef):} A "\NewTerm{geometric sequence}\index{geometric sequence}\label{geometric sequence}" is a sequence of numbers such that each of them is equal to the previous $n$ multiplied by a constant number $q$ that we also name the "\NewTerm{reason}\index{term of a geometric sequence}" or "\NewTerm{common ratio}\index{common ratio}" of the sequence. We will denote by:
	
Thus, if we denote by $u_n$ any term of the sequence $u_n$, we have (trivial):
	
Here are some properties for such a type of sequence (without proof until now... except if some readers ask for them because most are really trivial):
	\begin{enumerate}
		\item[P1.] (trivial) The quotient of two terms of the same sequence is a power of the reason $q$ whose exponent equals the difference in rank of the two terms chosen (simple ratio of two same bases with different powers).
		\item[P2.] (trivial) If we multiply or divide term by term two geometric sequences, we get a third geometric sequence whose raison equal the product (respectively the quotient) of the reasons of the two chosen sequences (simple operation with the reasons of the two original sequences).
		\item[P3.] In a geometric sequence, a term whose rank is the average of the ranks of the other two terms is the geometric mean (\SeeChapter{see section Statistics page \pageref{geometric mean}}) of these two terms (reread many times if needed...).
	\end{enumerate}
Let us prove the property P3:
\begin{dem}
	Given a geometric sequence with real positive reason $q$, we have for recall:
	
Let $a, b$ be the ranks of two terms of the geometric sequence, then we have:
	
and thus:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
\end{dem}

	\begin{corollary}
	We have as corollary that for three consecutive terms $n,n+1,n+2$in a geometric progression, the second term is the geometric mean of the other two.
	\end{corollary}
	\begin{dem}
		We have:
		
Thus:	
		
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
However, there are some special sequences that have special properties that we find very frequently in mathematical or theoretical physics in this book. Without going into  too much detail, here's a partial list of with the important proofs that we will have to use later:

	\subsubsection{Cauchy Sequence}\label{cauchy sequence}

It is often interesting for the mathematician, as much as for the physicist, to know the properties of a sequence with a given type of progression. The most important property is the limit to which it tends.

	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
The reader who is not comfortable with topology can skip the text that follows... and whoever wants to know more about Cauchy sequences may read the section Topology or also particularly the section on Fractals (\SeeChapter{chapter Theoretical Computing}).
	\end{tcolorbox}

\textbf{Definition (\#\mydef):} Let $(X, d)$ a metric space (\SeeChapter{see section Topology page \pageref{metric space}}), we say that the sequence:
	
converges to $x \in X$ if (by definition!):
	
In other words, more we go far in the sequence, the more points are close (in the sense of the metric $d$) to each other.

If we chose a particular metric (the Euclidean one for example) and a discrete sequence the above definition will look like this:
	
	Where the convergence point is therefore $a$ and we have:
	
	In the example of the figure below where the sequence seems to converge to $1.13$ we observe that for a given non-zero positive $\varepsilon$, there there is a particular $n$ which we denote $N$ ($n=17$ in the figure below) from which the sequence converges:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/analysis/cauchy_convergence.eps}
		\caption{Illustration of the principle of convergence of a sequence}
	\end{figure}

	However, the above definition of the convergence makes problem because the number $x$ should be known. In most cases of interest $x$ is unfortunately not known. To break this deadlock, Cauchy had the idea to propose the following definition:

	\textbf{Definition (\#\mydef):} We say by definition that a sequence $(x_n)_{n \in \mathbb{N}}$ of elements of $X$ is a "\NewTerm{Cauchy sequence}\index{Cauchy sequence}" if:
	
	The reader must notice that it is not sufficient for each term to become arbitrarily close to the preceding term. This is why require that $|a_{N+1} - a_{N}| < \varepsilon$ is not sufficient!

	It is almost obvious then that any convergent sequence is a Cauchy sequence (well there are some subtleties that we will not reference for now).

	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
This criterion therefore facilitates some proofs because it helps to show the existence of a limit without involving its value, generally unknown.
	\end{tcolorbox}
	
	\begin{theorem}
	Let us now prove that the theorem that asses that any convergent sequence is Cauchy sequence.
	\end{theorem}
	\begin{dem}
	Consider a sequence $u_n$ converging to the value $l$ (which is unknown to us) and $\varepsilon>0$ (randomly selected). Then there exists according to the definition of a convergent sequence, $n \in \mathbb{N}$ such that:
	
	The choice to write $\dfrac{\varepsilon}{2}$ is completely arbitrary but in fact we anticipate the result of the demonstration so that it is more aesthetic...
	
	Therefore for $p,q>N$ (in fact know the value of $N$ is irrelevant, since it should work for any value... well don't forget that $N$ depends on $\dfrac{\varepsilon}{2}$) we have using the triangle inequality (\SeeChapter{see section Vector Calculus page \pageref{triangle inequality}}):
	
	and because $d(u_n,l)\leq\dfrac{\varepsilon}{2}$ we can write:
	
	Therefore:
	
	That may be a bit abstract so let's see an example with the harmonic sequence as an example to close the proof:
	
	First, nothing it is not vorbidden to us to take $n \geq 2$ (otherwise it will be hard to make a difference between two terms...).
	Therefore we take the Euclidean distance:
	
	First, the reader will note that in all cases since $k\leq 2n$ is between $n+1$ and $2n$. Which brings us to write:
	
	So from this inequality it comes automatically that each term of the sum on the left below will be greater than each term of the sum on the right:
	
	With (just do a particular example)
	
	Therefore:
		
	Now the idea is to see if the sum on the left is therefore greater than or equal to $\varepsilon=\dfrac{1}{2}$ and this for any $n$. Thus the suite is not convergent!
	
	Thus, the idea is that we found an $\varepsilon$ for which the Cauchy criterion is deficient.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	So it is not because the points are always closer to each other that they converge to a given point, because this point may not exist.
	
	The best example is probably the following (it is also a little bit stupid example but...):
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
Let us take $X=\mathbb{Q}$ and the absolute difference as distance:
	
	Given $z$ an irrational number and $q_j \in \mathbb{Q}$ with $j \in \mathbb{N}^{*}$ such that:
	
	The idea is that greater is $j$, more the rational number $q_j$ is near the irrational $z$ and we know we can found such a sequence.
	Let us show that the $q_1,q_2$ we could be able to build form a Cauchy sequence! Indeed using triangle inequality:
	
	and therefore is a Cauchy sequence if and only if $\vert q_m-q_n \vert\leq \varepsilon$ if:
	
	We have thus found a $N$ (equal to $\dfrac{1}{2\varepsilon}$) which satisfies our definition of a Cauchy sequence. But this sequence does not converge in $\mathbb{Q}$ otherwise $z$ would be rational.
	You can check this with $\pi$ and the sequence:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
Mathematicians use such results to define the set of irrational and also by using some additional topological concepts.
	\end{tcolorbox}
	We have just seen that a Cauchy sequence is not necessarily a convergent sequence in $X$. The inverse is however true: any convergent sequence is a Cauchy sequence!!
	
	\subsubsection{Fibonacci Sequence}\label{Fibonacci Sequence}
	
	If we calculate a sequence of numbers starting with 0 and 1, such that each term is equal to the sum of the two previous ones, we can form the following sequence:
	
	therefore, if we designate the different terms by:
	
	We build therefore the following sequence law:
	
	More often written in the following form:
	
	The Fibonacci sequence has many interesting strong properties, which will be developed later. However, it seems to be the first "\NewTerm{recurring sequence}\index{recurring sequence}" known in history (hence the fact that we are talking about it in this book). 
	
	The origin of this sequence seem to come from a rabbit problem asked to Fibonacci in 1202. Starting with a couple of rabbits, how much couples of rabbits will we get after a given number of months knowing that each pair produces a new pair every month (and no couples die...), which becomes productive only after two months. Therefore we have:
	\begin{itemize}
		\item Beginning: We have nothing $(0)$
		\item 1st month: We buy a couple of baby rabbits $(1)$.
		\item 2nd month: The couple of rabbits are now adults $(1)$.
		\item 3rd month: We have the couple of rabbits that make a new couple of baby rabbits. We have two couples $(2)$.
		\item 4th month: We have two couple of adults with a new couple of babies. We have three couples $(3)$.
		\item 5th month: We have three couples of adults rabbits and two new couples of baby rabbits. We have five couples $(5)$
		\item and so on...
	\end{itemize}
	
	Let us take now a "in real life" example (this is typically a biased scientific example because you will always finish to find in Nature what your are looking for to argue your theories with a least on particular example...): the heart of some flowers! The scales of a pineapple or pinecone form two families of spirals wound in opposite directions. On a pine cone, you will count 5 spiral in one direction and 8 in the other, on pineapples, 8 and 13, on sunflowers 21 and 34. Each time we get Fibonacci numbers!
	
	A famous illustration of this is to do draw the following simple figure (named"Fibonacci Spiral") which reproduces the Fibonacci numbers on a grid plan with squares and corners connect with arc circles:
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/algebra/fibonacci.jpg}
		\caption{Fibonacci spiral}
	\end{figure}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The Fibonnacci spiral (quarter-circles tangent to the interior of each square) must not be confused with the "Golden Spiral" (a logarithmic spiral whose growth factor is the Golden ratio $\varphi$)!
	\end{tcolorbox}
	Let us now consider the limit of the Fibonacci sequence, let:
	
	by the properties of limits:
	
	As the construction of the Fibonacci sequence is given by:
	
	Using this equation to substitute, we get:
	
	and so we get the equivalent well known equation (see page \pageref{golden ratio}):
	
	and finally solving for $L$ using the quadratic formula yields and keeping the only solution that makes sense we have:
	
	that is the Golden ratio!
	
	We will come back later (see page \pageref{ordinary generating function}) on this sequence to prove another relation it has with the Golden ratio (see page \pageref{golden ratio}) through the direct determination of the $n$ term value!

	\subsubsection{Logic Sequences/Psychologist Sequences}
	\textbf{Definition (\#\mydef):} Psychologists name "\NewTerm{logical sequences}\index{logical sequence}", sequences that they write with an idea in mind, and they call "logical" people who find their idea, although there are other possibilities mathematically speaking (but psychologist don't know anything about real logic).
	
	For example if you have to find the next number $X$ to the logic sequence:
	
	In fact, you make the difference between the last and prior-previous number then you multiply by $10$ therefore the next number is $X=31000$.
	
	From a mathematical point of view, any number is suitable to replace $X$, also exists for each value of $X$, a polynomial in $n$ that takes the values $4, 5, 10, 50, 400, 3500$ for $n = 0, 1 , 2 ..6$.
	
	For the example above we can take for example:
	
	and is such that $P(0)=4, P(1)=5, P(2)=10, ...,P(6)=0$.

	\pagebreak
	\subsection{Series}

The physicists often needs to simply and formally solve problems, to approximate some given "terms" of their equations. For this purpose, they will use the properties of some given series. Also statisticians and financial analysts often face to series they need to simplify.

\textbf{Definition (\#\mydef):} Let be given an infinite number sequence:
	
	The expression:
	
	is named a "numeric series".
	
	\textbf{Definition (\#\mydef):} The partial sum of the first $n$ terms of the series is named "\NewTerm{partial sum}\index{partial sum}\label{partial sum}" and denoted by:
	
	If the following limit denoted $S$ exists and is finite:
	
	we name it "\NewTerm{sum of the series}\index{sum of a series}" and we say that the "\NewTerm{series converges}\index{convergent series}" (it is therefore a  Cauchy series). However, if the limit does not exist, we say that the "\NewTerm{series diverges}\index{divergent series}" and has no sum (for details see further below when we will deal with some empirical convergence criterias).
	\begin{theorem}
		Also let us prove for fun (because it is almost trivial) that if $\displaystyle \sum_{k\geq 0} u_k $ is a convergent numerical series then:
		
		But the opposite is not necessarily true!! In fact remember the example during our study above of Cauchy sequences with the harmonic series $\sum_{k=1}^n \dfrac{1}{k}$ that is not convergent even if the terms tends to zero when $k \rightarrow +\infty$.
	\end{theorem}
	\begin{dem}
		We assume first that $\displaystyle \sum_{k\geq 0} u_k $ is a convergent series and denote its limit by $S$. Let:
		
		Therefore:
		
		However, if the series is really convergent:
		
		So finally:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
	\end{dem}
	Following a reader request, let us prove in detail (not only assuming that it is intuitive) that the harmonic series\index{harmonic series}\label{harmonic series}:
	
	diverges.
	\begin{dem}
		Suppose the series converges to $H$, i.e.:
		
		Then:
		
		Calculating the underbraces, we get:
		
		This contradiction concludes the proof.	
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
	\end{dem}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Many people are struggling understanding intuitively why the harmonic series diverges but the $p$-harmonic series converges. For sure there are methods and applications to prove convergence and divergence, but many people have trouble understanding intuitively why it is. We know we must never trust too much or intuition, but this is hard for many to grasp. In both cases, the terms of the series are getting smaller, hence are approaching zero, but they both result in different answers.
	
	Perhaps a convincing way is to make an analytical prolongation to change the sum into an integral (\SeeChapter{see section Differential and Integral Calculus page \pageref{usual primitives}}):
	
	converges but:
	
	\end{tcolorbox}
	
	In the next subsection we will how to calculate the partial sum of some classic series that are important in physics, statistics and finance. We will start with Gauss arithmetic series that are an expression of the sum of the $n$ first nonzero integers raised to a given power $k$ in a condensed form. The application of this condensed form of a series has an important practical use in physics, statistics and finance when we wish to simplify the expression of certain results.
	
	\subsubsection{Gauss Series}\label{gauss series}\index{Gauss series}
	
	Gauss arithmetic series are an expression of the sum of the $n$ first nonzero integers raised to a given power $k$ in a condensed form $S_k$. The application of this condensed form of a series has an important practical use in physics, statistics and finance when we wish to simplify the expression of certain results.
	
	It is said that Gauss have found an attractive method in 1786 to determine the arithmetic sum of the first $n$ integers at the power of 1 when he was nine years old (...):
	
	To simplify, we find easily the following closed formula (notice that this is not a recursive relation!):
	
	for $n \geq 0$. Let us indicate that each intermediate sum of the series (1, 3, 6, 10, 15, etc.) is named "\NewTerm{triangular number}\index{triangular number}" since it is possible to represent it in the following form:

	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/algebra/triangular_number.jpg}
		\caption{Triangular number}
	\end{figure}

	We can continue with higher powers bit not as exercises because these relations are very useful!

	Now let us calculate the very important case that we find ourselves in a number of other sections (Economy, Quantum wave Physics, etc.) and that  the sum of the first $n$ square integer numbers (still non-zero!).

	Let us write for this\label{sum of squares integers}
	
	We know from Newton's binomial theorem (\SeeChapter{see Section Calculus page \pageref{binomial theorem}}):
	
	so we can write and add a member to member the $n$ following equalities:
	
	And the sum can be simplified as:
	
	After some elementary algebra manipulations we get:
	
	Therefore:
	
	Finally:
	
	We continue with the sum of the first $n$ cubes (non-zero) integers. The principle is the same as before, we write:
	
	We know from Newton's binomial theorem (\SeeChapter{see Section Calculus page \pageref{binomial theorem}}):
	
	We get by varying $k$ from $1$ to $n$, $n$ relations that we can add a member to member
	
	And the sum can be simplified as:
	
	Giving after development:
	
	And after an fist simplification:
	
	And a second simplification:
	
	The result result is therefore:
	
	or written differently:
	
	For sure, we can continue like this during a long time, but from a certain value of the power things get a bit more complicated (furthermore the method is a little bit boring). Thus, one of the members of the Bernoulli family (it was a family of very talented mathematicians... as you can see int the Biographies chapter) founded a general relation working for any power by defining what we name the "Bernoulli polynomial" (see further below).
	
	Let us conclude with one last case we will need during our study of Fourier series. We put:
	
	We want express this expression (series) as rational fraction. To do this, we multiply all by $x^2$. So we have two expressions:
	
	We subtract the first from the second:
	
	Finally:
	
	Most of times, to indicate that this is for odd powers we prefer to write:
	
	Similarly, for the needs of the section of Economy, we have:
	
	Therefore:
	
	Finally:
	
	Most of times, to indicate that this is for even powers we prefer to write:
	
	\paragraph{Bernoulli's Numbers and Polynomials}\mbox{}\\\\
	As we have seen above, it is possible to express the sum of the first $n$ nonzero integers to a given power (the first four have been proved previously) following the below relations where we put now $n:=n+1$ as we want now $n$ to be the number of terms we want the sum including 0 (hence the negative sign in the relations below that we did not have earlier):
	
	It is said that Jacob Bernoulli then noticed that the polynomials $S_p$ had the form:
	
	In this expression, the numbers $(1,-1/2,1/12,0,...)$ seem not to depend on $p$. More generally, after trial and error we see that the polynomial can be written as:
	
	Giving by identification the "\NewTerm{Bernoulli's numbers}\index{Bernoulli's numbers}":
	
	\begin{theorem}
	Thereafter, it seems that mathematicians in their research fell randomly (???) on the fact that the Bernoulli numbers could be expressed by the series:
	
	with $\vert z\vert<2\pi$.
	\end{theorem}
	\begin{dem}
	We have seen during our study of complex numbers (\SeeChapter{see section Numbers page \pageref{taylor expansion complex exponential}}) that:
		
		Therefore:
		
		Let us write now:
		
		Then we must have:
		
		We see (by distributing) that:
		
		for all this to be equal to unity we must have:
		
		From the second equation we get:
		
		and from the third equation we get:
		
		etc. Continuing this way we show that:
		
		It is obvious that this method allows us to calculate by hand only the first terms of this series.
		
		Thus, based on (as we will see later, this is the expression of an "ordinary generating function"):
		
		we find that the first Bernoulli numbers are:
		\begin{table}[H]
			\begin{center}
			\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|p{1cm}|p{2.5cm}|}
					\hline
					\multicolumn{1}{c}{\cellcolor{black!30}\textbf{$k$}} & 
  \multicolumn{1}{c}{\cellcolor{black!30}\textbf{$B_k$}} \\ \hline
					\centering\arraybackslash\ 0 & \centering\arraybackslash\ 1 \\ \hline
					\centering\arraybackslash\ 1 & \centering\arraybackslash\ -1/2 \\ \hline
					\centering\arraybackslash\ 2 & \centering\arraybackslash\ 1/6  \\ \hline
					\centering\arraybackslash\ 3 & \centering\arraybackslash\ 0  \\ \hline
					\centering\arraybackslash\ 4 & \centering\arraybackslash\ -1/30  \\ \hline
					\centering\arraybackslash\ 5 & \centering\arraybackslash\ 0  \\ \hline
					\centering\arraybackslash\ 6 & \centering\arraybackslash\ 1/42  \\ \hline
					\centering\arraybackslash\ 7 & \centering\arraybackslash\ 0  \\ \hline
					\centering\arraybackslash\ 8 & \centering\arraybackslash\ -1/30  \\ \hline
					\centering\arraybackslash\ 9 & \centering\arraybackslash\ 0  \\ \hline
					\centering\arraybackslash\ 10 & \centering\arraybackslash\ 5/66  \\ \hline
					\centering\arraybackslash\ 11 & \centering\arraybackslash\ 0  \\ \hline
					\centering\arraybackslash\ 12 & \centering\arraybackslash\ -691/2730  \\ \hline
					\centering\arraybackslash\ 13 & \centering\arraybackslash\ 0  \\ \hline
					\centering\arraybackslash\ 14 & \centering\arraybackslash\ 7/6  \\ \hline
					\centering\arraybackslash\ ... & \centering\arraybackslash\ $...$  \\ \hline
			\end{tabular}
			\end{center}
			\caption{Example of tabular representation of a game}
		\end{table}	
		The reader will have noticed that $B_k=0$ when $k$ is odd and different from $1$. That why sometimes the above relation is given by (remember that $0$ is an even number!):
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
	\end{dem}
	We see easily that the values of the Bernoulli numbers can not be described in a simple way. In fact, they are essentially values of the zeta Riemann function (see below) for negative integer values of the variable, and these numbers are associated with profound theoretical properties that go beyond the study of this book. Furthermore, the Bernoulli numbers also appear in the Taylor series expansion of  circular and hyperbolic trigonometric tangent functions in the Euler-Maclaurin formula (see below).
	
	With a small modification it is possible to define the "\NewTerm{Bernoulli polynomials $B_k(x)$}\index{Bernoulli polynomials}\label{bernoulli polynomials}" by:
	
	with:
	
	\begin{theorem}
	Furthermore, it is normally easy to observe that:
	
	and therefore it normally easy to deduce that:
	
	\end{theorem}
	\begin{dem}
	On one side we have:
	
	and another we have:
	
	So:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	And by identification of the coefficients we deduce:
	
	and for $k \geq 1$:
	
	It is then easy to deduce that the polynomials $B_k(x)$ are of degree $k$:
	
	Here is a plot of these polynomials:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/bernoulli_polynomials.jpg}
		\caption[Some Bernoulli polynomials]{Some Bernoulli polynomials (source: Wikipedia)}
	\end{figure}
	What is remarkable is that using the Bernoulli polynomials, we see that it is possible to write the $S_p$ as follows after some trials:
	
	
	Some write this relations even otherwise. Indeed, from previous relation, we can write:
	
	using:
	
	We have:
	
	So we just demonstrated in an engineer way that:
	
	However, we can now ask ourselves what happens to the partial sum of arithmetic and geometric sequences as presented earlier in this section. 
	
	\paragraph{Euler-Maclaurin formula}\label{Euler-MacLaurin formula}\mbox{}\\\\
	The Euler–Maclaurin formula is a relation for the difference between an integral and a closely related sum. It can be used to approximate integrals by finite sums, or conversely to evaluate finite sums and infinite series using integrals and the machinery of calculus. For example, many asymptotic expansions are derived from the formula\footnote{It will be useful to us during our study of the electromagnetic effect in the section of Wave Quantum Physics.}.
	
	If $m$ and $n$ are natural numbers and $f(x)$ is a real or complex valued continuous function for real numbers $x$ in the interval $[m, n]$, then the integral:
	
	can be approximated by the sum (or vice versa):
	
	(see rectangle method page \pageref{rectangle integration method}). The Euler-Maclaurin formula provides expressions for the difference between the sum and the integral in terms of the higher derivatives $f^{(k)}(x)$ evaluated at the endpoints of the interval, that is to say when $x=m$ and $x=n$.
	
	Explicitly, for $p$ a positive integer and a function $f(x)$ that is $p$ times continuously differentiable on the interval $[m, n]$, we have:
	
	where $B_{k}$ is the $k$ th Bernoulli number (with $B_{1}=1 / 2$ ) and $R_{p}$, is an error term which depends on $n, m, p$, and $f$ and is usually small for suitable values of $p$ The formula is often written with the subscript taking only even values, since the odd Bernoulli numbers are zero except for $B_{1}$. In this case we have:
	
	or alternatively:
	
	\begin{dem}
	One of the fundamental concepts of calculus is the correspondence between sums and integrals. Given any sufficiently well-behaved continuous function $f(x)$, consider the sum the integral defined by:
	
	These two functions obviously have some similarities, but they also have significant differences. For example, the summation is taken over the values of $f(k)$ at discrete integer arguments $\mathrm{k}=0,1,2, \ldots,n$, whereas the integral is taken over the values of $\mathrm{f}(\mathrm{x})$ for arguments x varying continuously from $0$ to $n$. Strictly speaking, the function $s(n)$ is defined only for integer arguments, while $I(\mathrm{n})$ is defined for any non-negative real value of $n$. Nevertheless, we can derive an interesting and useful relation between these two functions.
	
	First, notice that $I(n)$ can be written in the form:
	
	The Taylor series expansion of each of the individual terms of the integrand is of the form:
	
	where $f^{(j)}$ denotes the $j$th derivative of $f$. The integral of this term from $x=0$ to $1$ is:
	
	Inserting all these expressions into the original integral, we get:
	
	where:
	
	Now, if we had started with the derivative of $f$ in place of $f$, we would have arrived at the same expression:
	
	except that each $f^{(j)}$ would be replaced with $f^{(j+1)}$, and hence each $S^{(j)}$ would be replaced with $S^{(j+1)},$ and $I(n)$ would be replaced with $I^{(1)}(n)$ where:
	
	Thus beginning with successively higher derivatives of $f$, we get the infinite sequence of relations:
	
	Each of these equations has (in general) infinitely many terms, but we can solve the system of equations for any finite numbers of terms by taking just a restricted portion of these equations. For example, taking just the terms up to $\mathrm{S}^{(6)}$ from the first four equations, we have (in matrix form):
	
	Solving this system of equations gives
	
	where:
	
	and so on are the Bernoulli numbers. Thus we have
	
	Inserting the expressions for each $I^{(j)}$ from relation:
	
	this gives:
	
	Recall that $S^{(0)}$ is defined as the sum of $f(k)$ for $k=0,1,2, \ldots, n-1$. Therefore, if we wish to express this in terms of a summation from $k=0$ to $n$, we must add $f(n)$ to both sides. Noting that $B_{1}=-1 / 2$, this leads to the result:
	
	This is named the "\NewTerm{Euler-Maclaurin formula}\index{Euler-MacLaurin formula}", a generalization of Bernoulli's formula for the sum of powers of the first $n$ integers. Often used in the following form:
	
	or using a condensed notation:
	
	We should note that this series is divergent for most applications, but the error is less than the first neglected term, so the Euler-Maclaurin formula is often a useful method of approximation, relating the series summation to the continuous integral of a function.
	\begin{flushright}
		$\blacksquare$ Q.E.D.
	\end{flushright}
	\end{dem}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The Bernoulli numbers from $B_{1}$ to $B_{7}$ are $1/2, 1/6, 0,-1/30, 0, 1/42, 0$. Therefore the low-order cases of the Euler-Maclaurin formula are:
	
	\end{tcolorbox}
	
	\subsubsection{Arithmetic Series}
	We have shown above that the partial sum of a Gauss series (analogous to the sum of the terms of an arithmetic progression of reason $r = 1$) was given by:
	
	if not denote the value of the $n$-th term by $u_n$ instead of $n$, the development that we made for the series of Gauss then brings us to:
	
	and if we denote the first term $1$ of the Gauss series $u_0$ then we have:
	
	which gives us simple the partial sum of the $n$-terms of an arithmetic sequence of reason $r$.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. A simple Gauss series with of reason 1 starting at 4, finishing at 6:
	
	E2. Now an arithmetic partial sum series of reason 2 starting at 4, finishing at 8:
	
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The reader will have observed that the reason $r$ does not appear in the latter relation. Indeed, by taking (always) the same development that for the Gauss series, the term $r$ is simplified and vanish.
	\end{tcolorbox}
	
	\subsubsection{Geometric Series}\label{geometric series}
	Similarly, with a geometrical sum where we have for recall:
	
	we have therefore:
	
	The last relation is written (after simplification):
	
	and if $q\neq 1$ we get:
	
	which can be written by factoring $u_0$:
	
	If $q$ is positive and less than $1$, as $n$ approaches infinity we have the result that will be used extensively in the section Economy:
	
	That we found often in the following form (special case):
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Consider the following geometric series of reason $q = 2$:
	
	to calculate the sum of the first four terms $\left\lbrace 1,2,3,4 \right\rbrace$, we take the power of $2$ equivalent of $n=2$ (zero is not taken into account). We then get well: $S_3=15$.\\
	
	E2. Consider the quite famous geometric series of reason $q=1/2$. We then have:
	
	\end{tcolorbox}
	
	\paragraph{Zeta function and Euler's product formula}\label{zeta function}\mbox{}\\\\
	The German mathematician Riemann named "zeta" a function already studied before him, but that he examined when the value is a complex number (\SeeChapter{see section Numbers page \pageref{complex numbers}}). This function is represented as a series of inverse powers of integers. This how typically the series looks like for a special case:
	
	This may seems to the reader as pure abstract mathematics. However as we will see in the next chapters of this book, this function will appear for example during our study of the Black body (Stefan-Boltzmann law), or during our study of the Casimir force in Quantum Physics, but also during our study of the recombination of the Cosmological Microwave Background.
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} It is traditional to denote by $s$ the variable upon which this series depends.\\
	
	\textbf{R2.} The Riemann zeta function is a special case of the "\NewTerm{zeta function}\index{zeta function}" defined by:
	
	where $s=\Re(s)+\mathrm{i}\Im(s)=\sigma+\mathrm{i}t$ but when $a_n=n$.
	\end{tcolorbox}
	This series has an interesting property but if we remain within the framework of positive integer powers here is how we can introduce its origin:
	
	when $n\longrightarrow +\infty$ then we have:
	
	If we put $x=2^s$, we obtain the sum of the inverse of powers of $2$ and similarity with $x=3^s$ such that:
	
	If we do the product of these two expressions, we obtain the sum of the powers of all fractions whose denominator is a product of $2$ and $3$:
	
	If we take all primes left, we'll get on the right all integers, since every integer is the product of prime numbers according to the fundamental theorem of arithmetic (\SeeChapter{see section Number Theory page \pageref{fundamental theorem of arithmetic}}), and this is Euler fundamental identity: what we now name "\NewTerm{Riemann zeta function}\index{Riemann zeta function}" is both a finished product involving prime numbers and an infinite sum of inverse powers of all integers:
	
	In condensed notation, the "\NewTerm{Euler's product formula}\index{Euler's product formula}" of the Riemann zeta function is given by:
	
	where $p$ as indicated below the product symbol are the prime numbers.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The zeta Riemann function is a special case of "\NewTerm{Dirichlet series}\index{Dirichlet series}" that are defined by:
	
	When the real part of $s$ is greater than $1$, the Dirichlet series converges. On the other hand, the Dirichlet series diverges when the real part of s is less than or equal to $1$, so, in particular, the series $1 + 2 + 3 + 4 + \ldots$ that results from setting $s=-1$ does not converge.
	\end{tcolorbox}
	We now recommend most readers to skip what follows on the Riemann zeta function and return back here once the Fourier series and the Gamma-Euler function presented later in this section are mastered and understood...
	
	We assume in what follows that the Fourier series and Gamma-Euler function are known and mastered and that the Parseval's equality (see below page \pageref{Parseval theorem}) was studied (since it is also proved further below). 
	
	We will seek to determine first the Riemann zeta function for two positive values ($s$ respectively having values $2$ and $4$) that will be useful in the valuation of integrals in some sections of the chapter about Mechanics. As the two values are positive the series converge and there won't be too many issues evaluating the Riemann zeta function\footnote{For the section of Cosmology we should need also the value of the Riemann zeta function for $s=3$ but however the result give an irrational constant (ie the result cannot be written as a fraction $p/q$ where $p$ and $q$ are integers) named the "Apéry's constant". There is a theorem that proves that $\zeta(3)$ is irrational named the "Apéry's theorem".}. 
	
	Once done we will determine the value of the Riemann zeta function for any negative integer value! And here obviously the series seems to be divergent. In mathematics and theoretical physics for that kind of situation, we use the "\NewTerm{zeta function regularization}\label{zeta riemann regularization}" that is a type of regularization or summability method that assigns finite values to divergent sums or products, and that in particular can be used to define determinants and traces of some self-adjoint operators. The technique is now commonly applied to problems in physics, but has its origins in attempts to give precise meanings to ill-conditioned sums appearing in Number Theory. As we will see just below and during our study of Grandi's series (see page \pageref{Grandi series}) there are several different summation methods named "zeta function regularization" for defining the sum of a possibly divergent series!
	
	So let's start!
	
	\begin{itemize}
		\item To determine the value of $\zeta (2)$, we will express the function:
		
		in Fourier series form (see a little further below in this section). During our study of Fourier series we will see that there are two traditional ways to define a Fourier series and we have done here the choice of the definition of the most commonly used among physicists and engineers:
		
		As we prove it in our study of Fourier series, Fourier coefficients $a_0,a_n,b_n$ are obtained by solving:
		
		and using the integration by parts (\SeeChapter{see section Differential and Integral Calculus page \pageref{integration by parts}}) we have:
		
		It comes then:
		
		But the Parseval's theorem that we will prove in our study of Fourier series a little bit further below gives us too (depending on the choice of the definition of the Fourier series and associated coefficients, the Parseval's theorem is expressed a little bit differently!):
		
		Therefore we get immediately:
		
		But we will also see during our proof of Parseval's theorem that:
		
		Therefore it comes in our case:
		
		Therefore:
		
		and finally:
		
		The problem of finding the value this series converges to is known as the "\NewTerm{Basel problem}". Leonhard Euler found the solution in 1735...
		
		\item To determine the value of $\zeta(4)$, we will do the same, but with the function:
		
		in the form of Fourier series:
		
		For this purpose, we will calculate Fourier coefficients using the integration by parts (\SeeChapter{see section Differential and Integral Calculus page \pageref{integration by parts}}):
		Then we have:
		
		Therefore we have:
		
		But the Parseval's theorem that we will prove below gives us also:
		
		It then comes immediately:
		
		But we will see also see later below during our Parseval's theorem proof that:
		
		Then it comes in our case:
		
		Therefore:
		
		That is to say:
		
		Finally:
		
		
		\item Now let us focus on the first divergent case $\zeta(-1)$. 
		
		There are a few ways to prove that:
		
		One method, along the lines of Euler's reasoning, uses the relationship between the Riemann zeta function and what we define as the "\NewTerm{Dirichlet eta function}\index{Dirichlet eta function}" or "\NewTerm{alternating zeta function}\index{alternating zeta function}\footnote{Yes there are a lot of different kind (variations) of the zeta function. You can found a list of the 34 most important of them on Wikipedia.}":
		
		This Dirichlet series is the alternating sum corresponding to the Dirichlet series expansion of the Riemann zeta function, $\zeta(s)$.
		
		Where both Dirichlet series converge, one has the identities:
		
		The identity$(1-2^{1-s})\zeta (s)=\eta (s)$ continues to hold when both functions are extended by analytic continuation to include values of s for which the above series diverge. Substituting $s=-1$, one gets $-3\zeta(-1) = \eta(-1)$. Now, computing $\eta(-1)$ is an easier task, as the eta function is equal to the Abel sum of its defining series, which is a one-sided limit:
		
		Dividing both sides by $-3$, one gets:
		
		 The first example in which zeta function regularization is available appears in the Casimir effect, which is in a flat space the bulk contributions of the quantum field in three space dimensions. In this case we must calculate the value of Riemann zeta function at $-3$:
		
		  which diverges explicitly. However, it can be analytically continued to $s=-3$ where hopefully there is no pole (\SeeChapter{see section Complex Analysis page \pageref{residue theorem}}), thus giving a finite value to the expression.
		 \begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		The reason why we see $\zeta(-1)$ in many YouTube videos and physics textbook, rather than $\zeta(-3)$ is that when you imagine the Casimir effect as happening in one dimension (along a line rather than in $3$D), the energy density you calculate is $\zeta(-1)$ rather than $\zeta(-3)$.
		\end{tcolorbox}
		
		Let us see another and more rigorous way to prove that $\zeta(-1)=-\dfrac{1}{12}$.
		
		We need a result first:
		
		Let us now multiply the above result by $x+1$, then integrate by parts twice, to we get:
		
		Now we can plug in $x=-1$ into the above result to get:
		
		Since $(1-2^2)\Gamma(1)=-3$ the above result can be rearrange to:
		
		We will see a third method just below to get that same result and even a fourth one during our study later of Grandi's series (see \pageref{Grandi series}).
		
		\item Let us prove now that and elegant and fast way to determine the value of the zeta function for all negative integers (there are other methods but involving heavy and boring mathematics for most engineers).
		
		For that purpose we will first need to prove the following lemma:
		\begin{lemma}
		The Riemann zeta function in the critical strip is given by:
		
		where $[x]$ denotes the integer part of $x$.
		\end{lemma}
		For the proof we need first for $\Re(s) > 1$:
		 
		Since $[x]=n$ for any $x$ in the interval $[n,n+1]$, we have:
		
		allowing the following simplification:
		
		Because $0\leq \{x\}<1$, the last integral converges and is holomorphic (\SeeChapter{see section Complex Analysis page \pageref{holomorphic functions}}) on $\Re(s)>0$. But that means the full equation is meromorphic (\SeeChapter{see section Complex Analysis page \pageref{meromorphic function}}) on $\Re(s)>0$, and thus provides an analytic continuation of $\zeta(s)$ on the half plane $\Re(s)>0$. The $s/(s-1)$ term gives a simple pole at $s=1$ with residue $1$ (\SeeChapter{see section Complex Analysis page \pageref{residue theorem}}).
		
		Injecting again back $[x] = x-\{x\} \Leftrightarrow \{x\}=x-[x]$, we get:
		
		That finish the proof of the lemma.
		
		Now remember that we have proved just earlier above that:
		
		where $\{x\}$ denotes the fractional part of $x$. This gives the analytic continuation of $\zeta(s)$ for $\Re(s)>0 .$ We can now proceed inductively. Writing:
		
		and integrating the last integral by parts. we get:
		
		and the latter integral converges for $\Re(s)>-1$. That is:
		
		Thus, inductively, we deduce:
		
		and the infinite sum on the right hand side converges for $\Re(s)>-m$. 
		
		If in the relation above we put $s=1-m,$ and note that for $r=m$ that $\zeta(s+m)$ has a simple pole at $s=1-m,$ we obtain the recurrence:
		
		The first few values at nonpositive integers are immediate and given by:
		
		And that's it!
	\end{itemize}
		
	\subsubsection{Telescoping Series}
	A "\NewTerm{telescoping series}\index{telescoping series}" is a series in which most of terms cancel in each of the partial sums, leaving only some of the first terms and some of the last terms:
	
	For example, the series:
	
	simplifies as:
	
	We will encounter such as series for business purposes (management) in our study of Queuing Theory in the section of Quantitative Management!!!
	
	\subsubsection{Grandi's Series}\label{Grandi series}
	The "\NewTerm{Grandi's series}\index{Grandi's series}" (after Italian mathematician, philosopher, and priest Guido Grandi, who gave a memorable treatment of the series in 1703) is defined as the following arithmetic series:
	
	It is a very famous series in mathematics and physics because:
	\begin{itemize}
		\item It highlights in a very simple way the fact (see below) that it is dangerous to manipulate infinite series
		
		\item Its result seems completes non-intuitive but in fact it opens the door to a more general definition of what is a "sum"
		
		\item It is a beautiful example of a series that seems useless and purely mathematics but that has in fact important application in quantum physics (Casimir Effect as seen in the section of Wave Quantum Physics) and String Theory (number of dimensions as seen in the section of String Theory).
	\end{itemize}
	and this is why we dedicate to it a special subsection in this book!
	
	It seem quite obvious at a first glance that it is a divergent series, meaning that it lacks a sum in the usual sense (the sequence of partial sums of Grandi's series clearly does not approach any number). But the other hand, its Cesàro sum is $1/2$!!?? So what the hell is a Cesàro sum?
	
	\textbf{Definition (\#\mydef):} In mathematical analysis a "\NewTerm{Cesàro sum}\index{Cesàro sum} assigns values to some infinite sums that are not convergent in the usual sense. The Cesàro sum is defined as the limit of the arithmetic mean of the partial sums of the series.
	
	Let $\{a_n\}$ be a sequence, and let:
	
	be the $k$th partial sum of the series:
	
	The series $\sum _{n=1}^{+\infty}a_{n}$ is say to be "Cesàro summable", with Cesàro sum $S\in\mathbb{R}$, if the average value of its partial sums $s_k$ tends to $S$:
	
	In other words, the Cesàro sum of an infinite series is the limit of the arithmetic mean (average) of the first $n$ partial sums of the series, as $n$ goes to infinity. If a series is convergent, then it is Cesàro summable and its Cesàro sum is the usual sum. For any convergent sequence, the corresponding series is Cesàro summable and the limit of the sequence coincides with the Cesàro sum.
	
	One obvious method to attack the Grandi's series:
	
	is to treat it like a telescoping series and perform the subtractions in place:
	
	On the other hand, a similar bracketing procedure leads to the apparently contradictory result:
	
	Thus, by applying parentheses to Grandi's series in different ways, one can obtain either $0$ or $1$ as a "value". It can be shown that it is not valid to perform many seemingly innocuous operations on a series, such as reordering individual terms, unless the series is absolutely convergent. Otherwise these operations can alter the result of summation.

	Treating Grandi's series as a divergent geometric series we may use the same algebraic methods that evaluate convergent geometric series to obtain a third value:
	
	so:
	
	Therefore:
	
	Finally:
	
	The same conclusion results from calculating $-S$, subtracting the result from $S$, and solving $2S = 1$.

	The above manipulations do not consider what the sum of a series actually means. Still, to the extent that it is important to be able to bracket series at will, and that it is more important to be able to perform arithmetic with them, one can arrive at two conclusions:
	\begin{itemize}
		\item The series $1-1 + 1-1 + \ldots$ has no sum

		\item ...but its sum should be $1/2$ (see further below)
	\end{itemize}
	In fact, both of these statements can be made precise and formally proven, but only using well-defined mathematical concepts that arise in the 19th century. After the late 17th-century introduction of calculus in Europe, but before the advent of modern rigor, the tension between these answers fueled what has been characterized as an "endless" and "violent" dispute between mathematicians. The funnies is that the violent discussions still continue today... a YouTube video on this subject have more than $5,000$ comments... and blog post more than $200$ comments and a forum thread more than $600$... (and by the way the comments show the worst of what the human behavior and respect can be...) So this is quite a hot topic...
	
	Let us also recall that at the beginning of our study of Geometric series we have proved that:
	
	Therefore if $u_0=1$ this reduce to:
		
	where as $n$ goes to infinity, the absolute value of $|q|$ must be less than one for the series to converge!

	Now notice that if $q=-1$ we fall back on Grandi's series and therefore that latter is a special case of the geometric series $1+q^1+q^2+q^3+\ldots$ and then we would perhaps write a bit too quick:
	
	But as we have just mention it, we are not authorized to write the latter fraction if $q=\pm 1$ otherwise the series diverge excepted... if we work in a special axiomatic framework. This is almost as chocking as when imaginary numbers were introduced in the 16th century, as well as negative numbers that were poorly understood and regarded by some as fictitious or useless, much as zero once was...
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	There have been very interesting studies about the reaction of high-school level students to Grandi's series presentation. The reactions and analysis are very interesting and I can personally only recommend every teacher to introduce this series in classes but without giving the result in a first time!
	\end{tcolorbox}
	But there is a "one more thing"... We will now calculate a sum, thinking it really gives infinity as a result:
	
	To do this, let's do another trick of mathematical magician:
	
	Therefore:
	
	So we can compute:
	
	Our first concrete result, squared, can be rewritten as follows:

	
	Or well:
	
	Explicitly:
   \begin{eqnarray*}
	   (-1 + 1-1 + 1-1 + \ldots)\\
	   \underline{\times (-1 + 1-1 + 1-1 + \ldots)} \\
	    =1-1 + 1-1 + 1-1 + 1-1 + \cdots \\ -1 + 1-1 + 1-1 + 1-1 + \cdots\\ + 1-1 + 1-1 + 1-1 + \cdots\\ \cdots
	\end{eqnarray*}
	Summing each column we see that we fall back on:
   
	Therefore:
	
   But as $S = 1/2$, then:
   
   Therefore:
   
   That is (to freak out a last time), we have shown that:
   
	Quite interesting! But don't forget that we get this result in a special framework that we have build and that generalize the conventional sum that we learn at high-school. It seems that all generalizations of the conventional sum (as mathematicians do definitions with coffee as we know...) lead to the same result of $-1/12$...
	
	It is also quite chocking that as natural numbers are closed under addition to that the sum of all naturals is $-1/12$...
	
	Furthermore, all this stuff is not new! It was known by many people long time before (few hundred years) and it was especially Srinivasa Ramanujan and later Godfrey Harold Hardy in a book titled \textit{Divergent Series} that formalized the subject in a more elegant and technical way than the one you have here above.
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/ramanujan_series.jpg}
		\caption[]{Piece of Ramanujan's first notebook about $-1/12$}
	\end{figure}
	
	\pagebreak
	\subsubsection{Taylor and Maclaurin Series}\label{taylor series}
	Taylor and Maclaurin series provide a convenient and powerful tool to simplify theoretical models and computer calculations (fluid modeling or fields in space). They are used heavily in all fields of physics but they are also found in the industry including engineering (design of experiments, numerical methods, quality management), statistics (integral approximations), finance (stochastic processes ), complex analysis... We strongly advise the reader to read carefully the developments that follow. There is also a serious funny quote on the subject:
	\begin{fquote}Everything is just some form of first order Taylor Expansion!\end{fquote}
	To start, consider a polynomial (with one variable/univariate):
	
	We trivially have for this latter:
	
	Given now the derivative of the polynomial $P (x)$:
	
	Therefore:
	
	and so on with $P''(x), P'''(x), ...$ such that:
	
	Then:
	
	Therefore:
	
	relation that we name "\NewTerm{limited Maclaurin series}\index{limited Maclaurin series}" or simply "\NewTerm{Maclaurin series}\index{Maclaurin series}" of order $k + 1$.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In practice, as we will see in many other sections of this book, we often use limited developments of order $1$ (also named "\NewTerm{affine approximations}\index{affine approximation}", or "\NewTerm{affine tangent approximations}\index{affine tangent approximations}"), which can facilitate the calculations, when we do not expect too much precision in the final result.
	\end{tcolorbox}
	Now by applying the same reasoning but by centering the value of the polynomial on $x=x_0$, we have:
	
	and so the previous development becomes more general:
	
	which is no other than the general expression of a polynomial expression in a form named "\NewTerm{limited Taylor series}\index{limited Taylor series}" of order $k + 1$. This function can be assimilate to a polynomial as $n$ is finite. But if $n$ is infinite, as we shall see later, this series converges to the function we are seeking the representation in the form of a sum of terms.
	
	Thus, some functions $f (x)$ of class $\mathcal{C}^n$ that can be approximated by a polynomial $P(x)$ (a sum of powers in other words...) centered on the value $x_0$ can be expressed as:
	
	Result often referred to as "\NewTerm{Taylor's theorem}\index{Taylor's theorem}".
	
	But this last relation is not correct for all functions that can not be expressed as a polynomial. Therefore we say that the series is not convergent for them. We will see an example later below.
	
	The latter relations is sometimes also written ... more conventionally:
	
	In finance (and not only!), we will often use the following rearrangement:
	
	Let us return briefly to the approximation of $f (x)$ near and centered in $x_0$:
	
	Another very common notation in physics and financial engineering of the above relation is\label{differential expression of Taylor series}:
	
	Some people do not like using this formulation because we have the risk to forget that the approximation for a few terms is only good as long as we are not too far from $x_0$ with $x$. This is why it often happens that we write:
	
	with $x_0$ fixed and a $h$ variable but small (!) and so it then comes a current form of notation of Taylor series:
	
	with $x_0$ fixed and $h$ variable but small and therefore it comes a common other notation of Taylor series (!):
	
	Let's see an application example with Maclaurin series (with $x_0$ being zero) of the function $\sin (x)$ and Maple 4.00b:
	
	\texttt{>p[n](x) = sum((D@@i)(f)(a)/i!*(x-a)\string^i,i=0..n);\\
	>p11:= taylor(sin(x),x=0,12);\\
	>p11:= convert(p11,polynom);\\
	>with(plots):\\
	>tays:= plots[display](sinplot):\\
	for i from 1 by 2 to 11 do\\
	tpl:= convert(taylor(sin(x), x=0,i),polynom):\\
	tays:= tays,plots[display]([sinplot,plot(tpl,x=-Pi..2*Pi,y=-2..2,\\
	color=black,title=convert(tpl,string))]) od: \\
	>plots[display]([tays],view=[-Pi..2*Pi,-2..2]);}

	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/maclaurin_sinus_serie.jpg}
		\caption{Approximation of the sine function by a Maclaurin development Maple 4.00b}
	\end{figure}
	We see well in this example that the Maclaurin series only allows to approach a function at a point with a limited number of points. But more terms we take (put $100$ terms in the Maple code above) more the validity is big on the whole domain of definition of the function. In fact it is possible to prove that the function $sin (x)$ is exactly expressible in Maclaurin series when the number of terms is infinite. We say then that its "rest" is zero.
	
	But this is not true for all functions! For example the function:
	
	
	\texttt{>p[n](x) = sum((D@@i)(f)(a)/i!*(x-a)\string^i,i=0..n); \\
	>p10:= taylor(1/(1-x\string^2),x=0,10);\\
	>p10:= convert(p10,polynom);\\
	>with(plots):\\
	>tays:= plots[display](xplot):\\
	for i from 1 by 2 to 10 do\\
	tpl:= convert(taylor(1/(1-x\string^2), x=0,i),polynom):\\
	tays:= tays,plots[display]([xplot,plot(tpl,x=-2..2,y=-2..2,
	color=black,title=convert(tpl,string))]) od: \\
	>plots[display]([tays],view=[-2..2,-2..2]);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/maclaurin_nonconvergent_serie.jpg}
		\caption{Example of non-convergent Maclaurin serie Maple 4.00b}
	\end{figure}
	We see above that regardless of the number of terms that we take the Maclaurin series converges only in one area of definition between $] -1,1 [$. This interval is named the "\NewTerm{radius of convergence}\index{radius of convergence}" and it determination (the singularity) is crucial in many areas of engineering, physics and analysis. We will return in mure more detail on this example in the section of Complex Analysis.
	
	But we can shift the Maclaurin series of the previous function to approximate the function with a Taylor series in other non-singular point such as in $x_0$ having for value $2$:
	
	\texttt{>p[n](x) = sum((D@@i)(f)(a)/i!*(x-a)\string^i,i=0..n);\\
	>p10:= taylor(1/(1-x\string^2),x=2,10);\\
	>p10:= convert(p10,polynom);\\
	>with(plots):\\
	>tays:= plots[display](xplot):\\
	for i from 1 by 2 to 10 do\\
	tpl:= convert(taylor(1/(1-x\string^2), x=2,i),polynom):\\
	tays:= tays,plots[display]([xplot,plot(tpl,x=0..5,y=-2..2,\\
	color=black,title=convert(tpl,string))]) od: \\
	>plots[display]([tays],view=[-0..5,-2..2]);}
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/maclaurin_nonconvergent_serie_shifted.jpg}
		\caption{Shift possibility of Maclaurin serie in Maple 4.00b}
	\end{figure}
	
	We will study a generalization to the complex plane of Taylor series in the section of Complex Analysis to get a veeeeery powerful result for physicists to calculate complicated curvilinear integrals.
	
	\pagebreak
	\paragraph{Usual Maclaurin developments}\label{usual maclaurin developments}\mbox{}\\\\
	We will prove here the most frequent Maclaurin developments (about ten) to the second order that we can meet in theoretical and mathematical physics (in fact we heve developed here only use almost everywhere in the book). The list is not exhaustive for the time being but as the proof below are generalized, they can be applied to many other cases (that we will apply/meet throughout this book).
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The Taylor expansions (that is to say elsewhere than on zero) are very rare (there are one or two in this entire book but they are detailed in their respective sections), we will omit them.
	\end{tcolorbox}
	
	\begin{enumerate}
		\item Taylor-Maclaurin development of $f(x)=e^x$:
		
		First remember that we have proved in the section of Differential and Integral Calculus that:
		
		Therefore we have:
		
		More generally:
		
		And therefore we have the famous result for $x=1$\label{euler maclaurin expansion}:
		
		that is sometimes named the "\NewTerm{exponential sequence}\index{exponential sequence}".
		
		\item  Taylor-Maclaurin development of $f(x)=\sin(x)$:
		
		First remember that we have proved in the section of Differential and Integral Calculus that:
		
		Therefore we have:
		
		
		\item  Taylor-Maclaurin development of $f(x)=\cos(x)$:
		
		First remember that we have proved in the section of Differential and Integral Calculus that:
		
		Therefore we have\label{cosine maclaurin dev}
		
		\item  Taylor-Maclaurin development of $f(x)=\tan(x)$:
		
		First remember that we have proved in the section of Differential and Integral Calculus that:
		
		Therefore we have:
		
		
		\item  Taylor-Maclaurin development of $f(x)=\arctan(x)$:
		
		First remember that we have proved in the section of Differential and Integral Calculus that:
		
		Therefore we have:
		
		
		\item  Taylor-Maclaurin development of $f(x)=\displaystyle\frac{1}{1+x}$:
		
		First remember that we have proved in the section of Differential and Integral Calculus that:
		
		Therefore we have:
		
		It then follows immediately another Taylor series we will also meet again many  number of times:
		
		For our study of the CMB in the section of cosmology (especially recombination era), we will need a result related to the two series above named the "\NewTerm{Binomial theorem for negative integer exponents}\index{binomial theorem for negative integer exponents}\label{binomial theorem for negative integer exponents}". Given the binomial coefficient given for recall by:
		
		and $n$ be a positive integer. Then (without proof but using "engineer intuition"):
		
		for $|x|<1$.
		
		\item  Taylor-Maclaurin development of $f(x)=\sqrt{1+x}$:
		
		First remember that we have proved in the section of Differential and Integral Calculus that:
		
		Therefore we have:
		
		It then also follows immediately another Taylor series we will also meet again many  number of times:
		
		
		\item  Taylor-Maclaurin development of $f(x)=\ln(1+x)$\label{maclaurin dev natural logarithm}:
		
		First remember that we have proved in the section of Differential and Integral Calculus that:
		
		Therefore we have:
		
		
		\item  Now consider the important case for the Langevin model of paramagnetism that is approximated Taylor expansion of the hyperbolic cotangent function (\SeeChapter{see section Trigonometry page \pageref{hyperbolic trigonometry}}), that is for refresh defined by the relation:
		
		For this, we will use the Landau notation, with expressions like $\mathcal{O}(x^n)$ remembering that we proved a little before above:
		
		when $x \rightarrow 0$.
		For the hyperbolic cotangent we have then:
		
		Now we must be remember as we have just proved a little earlier that:
		
		for $\vert x \vert < 1$. Therefore:
		
		and finally replacing this in the previous expression we find:
		
		
		\item  Another famous Maclaurin series used thousand of times in the world for business application is the computation of the numerical values of the Normal distribution:
		
		So first to simplify this integral, we typically let:
		
		that we know already (\SeeChapter{see section Statistics page \pageref{reduced centered variable}}) as being the $z$ score of a data value. With this simplification, the integral above becomes:
		
		The Maclaurin series for $e^{-x^2/2}$ is given by:
		
		Therefore:
		
		Therefore:
		
		Is is obvious that the constant will eliminate itself. Therefore!
		
		and in the common case in business where $a=0$ we get (with two terms only):
		
	\end{enumerate}
	
	\pagebreak
	\paragraph{Taylor series of bivariate functions (multivariate Taylor series)}\label{multivariate taylor series}\mbox{}\\\\
	We will see now how to approach a function $f (x, y)$ of two real variables by a sum of powers (Taylor series). This type of approximation is widely used in many fields of engineering (see sections of Industrial Engineering page \pageref{bivariat taylor expansion doe} and Numerical Methods page \pageref{newton raphson method}).
	
	We are looking for an approximation of $f (x, y)$ at point $f(x_0+h,y_0+h)$. For this, let us write (a priori nothing prohibits us from doing so) that:
	
	Then we have:
	
	The value of (the trick is here!):
	
	can be approximated using a Taylor series around the value $0$ such that:
	
	But we have:
	
	and:
	
	According to Schwarz's theorem (\SeeChapter{see section Differential and Integral Calculus page \pageref{Schwarz theorem}}):
	
	Then we have:
	
	and we show by induction that:
	
	Therefore we finally get:
	
	or in another equivalent simplified form:
	
	Or if we define a matrix $H$ named "\NewTerm{Hessian matrix}\index{Hessian matrix}\label{hessian matrix}" given by:
	
	we can also write:
	
	In Maple 4.00b we use the following command to make a development of order $3$ around $0$:
	
	\texttt{>readlib(mtaylor):\\
	>mtaylor(f(x,y), [x,y], 3);}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us see an example with the famous humpback whale:\\
	
	\texttt{>with(plots): with(plottools):\\
	>readlib(mtaylor):\\
	>fct:=x\string^2*(4-2.1*x\string^2+1/3*x\string^4)+x*y+y\string^2*(-4+4*y\string^2);\\
	>poly2 :=mtaylor(fct,[x=1,y=1],6);\\
	>\#Convert all the coefficients to floating point numbers\\
	>poly2n := map(evalf,poly2):\\
	>gr1:= plot3d(poly2n,x=-2..2,y=-1..1,color=red):\\
	>gr2:= plot3d(fct,x=-2..2,y=-1..1,color=blue):\\
	>display3d({gr1,gr2},view=-3..8,axes=framed);
	}
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/taylor_multivariate.jpg}
		\caption{Bivariate Taylor example with Maple 4.00b}
	\end{figure}
	\end{tcolorbox}
	
	The previous relation can also be written in the multivariate case in a point $\vec{x}$ near the origin of the coordinate system $\vec{p}$ as following (common notation in computer science):
	
	where:
	
	
	\pagebreak
	\paragraph{Quadratic Form}\mbox{}\\\\
	Now we will need for the section of Theoretical Computing to state an important property (which would have also its place only in the section of Differential and Integral Calculus):
	
	Let $f$ be a function defined and derived over an interval $I$ and given $a$ an element of $I$. If $f$ is such that $f'(a)=0$ then we say it has a "\NewTerm{local extremum}\index{local extremum}\label{local extremum}" on $a$.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The reciprocal is false, the function $x^3$ is such an example. Its derivative is zero at $0$ but there is no local extreme at this point. So be careful!
	\end{tcolorbox}
	
	However, let $f$ be a function defined and derived over an interval $I$ and given $a$ an element of $I$. If $f$ is such that $f'(a)=0$ and if $f'$ changes sign in $a$ then $f$ has a local extremum at $a$.
	
	To return now to our bivariate development Taylor, we know that if $(x_0,y_0)$ is a local extremum of $f$ then we have in a first time (\SeeChapter{see section Differential and Integral Calculus page \pageref{local extremum}}):
	
	However we have seen that this condition is not sufficient to ensure that $(x_0,y_0)$ is a local extremum.
	
	Reconsider the Taylor expansion of $f$ above taking into account the above condition. Development simplifies then to:
	
	Then we know that by definition so that the $(x_0,y_0)$ is a local minimum (respectively a local maximum) it is sufficient that the expression in brackets is positive (respectively negative). Since the second derivatives of $f$ are continuous, it is sufficient that the expression:
	
	to be positive (negative resp.) regardless of $h$ or $k$ and it is zero only if $h=k=0$. Then we say that $q$ is a "\NewTerm{positive definite quadratic form (resp. negative definite)}\index{positive definite quadratic form}".
	
	To simplify writing and to comply with traditions we put now:
	
	Then we can rewrite $q$ as follows:
	
	where $H$ remains the Hessian matrix of $f$ evaluated on $(x_0,y_0)$.
	
	So we see that $q$ is positive definite (local minimum) if $a>0$ and $\det(H)>0$, negative definite (local maximum) if $a<0$ and $\det(H)>0$.
	
	Returning to the partial derivatives these conditions are described as follows:
	\begin{itemize}
		\item Positive definite (local minimum) if:
		
		
		\item Negative definite (local maximum) if:
		
		
		\item Indefinite if:
		
	\end{itemize}
	Finally we see that the sign of the determinant of Hessian matrix and that of $\dfrac{\partial^2 f}{\partial x^2}(x_0,y_0)$ allow us to obtain a sufficient condition to determine if we are in the presence of a local extremum.
	
	If we now consider a general $2\times 2$ symmetric matrix:
	
	It should be almost obvious to the reader that this matrix induces, in the sens of the Hessian matrix, the quadratic form:
	
	If $y = 0$, then we have $\mathrm{Q}_A(x,0) = ax^2$ (parabola), so we must certainly have $a > 0$ in order for $A$ to be positive definite.
	
	The following examples illustrate that in general, it cannot easily be determined whether a symmetric matrix is positive definite from inspection of the entries.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. As first example, consider the matrix:
	
	Then:
	
	and we have whatever $(x_0,y_0)$:
	
	Therefore, even though all of the entries of $A$ are positive, $A$ is not positive definite.
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	E2. Consider the matrix:
	
	Then:
	
	which can be seen to be always nonnegative. Furthermore, $ \mathrm{Q}_A(x,y) =0 $ if and only if $x = y$ and $y =0$, so for all nonzero vectors $(x,y)$, $ \mathrm{Q}_A(x,y) >0 $ and $A$ is positive definite, even though $A$ does not have all positive entries.\\
	
	E3. Let:
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/algebra/hessian_exp_x_y.jpg}
	\end{figure}
	We have:
	
	which yields the critical points $(x, x)$ for all $x \in \mathds{R}$. We also have:
	
	which yields:
	
	That is,$\mathrm{H}f(x,x)$, is positive semidefinite, making $(x,x)$ a global minimizer of $f(x,y)$.
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	E4. Let:
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/algebra/hessian_x3-12xy.jpg}
	\end{figure}
	We have:
	
	which yields the critical points $(0, 0)$ and $(2, 1)$. We also have:
	
	and therefore:
	
	We see that $\mathrm{H}f(2, 1)$ is positive definite, because its principal minors are positive, but $\mathrm{H}f(0, 0)$ is not, as $\Delta_1 = 0$ and $\Delta_2 = -144$. That is, $\mathrm{H}f(0, 0)$ is indefinite, so $(0, 0)$ is a saddle point.
	\newline
	Furthermore:
	
	so $f(x, y)$ has no global minimizer on $\mathds{R}^2$. We can conclude, however, that $(2, 1)$ is a strict local minimizer.\\
	
	It should be emphasized that if the Hessian is positive semidefinite or negative semidefinite at a critical point, then it cannot be concluded that the critical point is necessarily a minimizer, maximizer or saddle point of the function.
	\end{tcolorbox}
	
	
	\paragraph{Lagrange Remainder}\label{Lagrange Remainder}\mbox{}\\\\
	There may be an interest for some numerical applications (\SeeChapter{see section Numerical Methods page \pageref{numerical methods}}) to know the approximation error of the polynomial $P_n(x)$ in relation to the function $f(x), \forall x$.
	
	Let us define for this purpose a "remainder", such that:
	
	\begin{theorem}
	The function $R_n(x)$ is named "\NewTerm{Lagrange rest}\index{Lagrange rest}" or "\NewTerm{Lagrange remainder}\index{Lagrange remainder}" or "\NewTerm{Lagrange error}\index{Lagrange error}".
	
	\end{theorem}
	
	\begin{dem}
	Given a function $g(t)$ defined by the difference of a function $f(x)$ assumed to be known and a Taylor approximation of the same function:
	
	with, of course:
	
	We see that $g (t)$ vanishes as expected for value $t=x$.
	
	Now let us derive $g(t)$ with respect to $t$, we find:
	
	After simplification:
	
	According to Rolle's theorem (\SeeChapter{see section Differential and Integral Calculus page \pageref{rolle theorem}}), there exist a value $t=z$ for which the derivative $g'(t)$ is zero. So:
	
	We can simplify the equation by $(x-z)^n$:
	
	which can also be written as:
	
	so we find for the maximum of $R_n$:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	We see that as the polynomial $P_n(x)$ is of high degree, the more it approximates the function $f (x)$ with accuracy. What will happen when $n\rightarrow +\infty$?:
	
	Suppose that $f (x)$ has derivatives of all orders (what we denote for reminder $\mathcal{C}^n$) for all values of any interval containing $x_0$ and let the rest of Lagrange $R_n$ of f (x) of $f(x)$ on $x_0$. If, for any $x$ in the range:
	
	then $f (x)$ is exactly represented by P $(x)$ on the interval.
	\begin{dem}
	The proof simply stems from the expression of $P_n(x)$ when $n\rightarrow +\infty$.
	
	Indeed, if we take an infinity of terms for $P_n(x)$, the correspondence with the approximated function is perfect and so the rest is zero.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	The polynomial:
	
	is named "\NewTerm{Taylor polynomial}\index{Taylor polynomial}\label{Taylor polynomial}" or "\NewTerm{Taylor series}\index{Taylor series}". If $x_0=0$, it is named "\NewTerm{Maclaurin polynomial}\index{Maclaurin polynomial}" or "\NewTerm{Maclaurin series}\index{Maclaurin series}".
	
	\paragraph{Taylor Series with Integral Remainder}\mbox{}\\\\
	We'll see if a theorem that will be useful in the section of Statistics to link the Poisson and Chi-2 laws and that is used in statistical software for Poisson test of rare events (that is the only business practical application that is known to us at this day).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	If anyone has a more educational proof whose beginning is a little less "formula fell from the sky", we are takers!
	\end{tcolorbox}
	\begin{theorem}
	Let $f(x)$ be $n + 1$ times differentiable on the interval $[a, b]$. Then we have:
	
	where it is important (for the good understanding of what we will do in the section of Statistics) that the reader notices in the development that when the derivative stops at the $n$-th term in the series, the integral (the remainder) has a factor of $1 / n !$, a power $n$ and a derivative of order $n + 1$. So verbatim, as we shall prove it below, if we stop the development of the terms to $n-1$, the integral (the remainder) will have a factor of $1 / (n-1) !$, a power $n-1$ and a derivative of order $n$-th.
	\end{theorem}
	\begin{dem}
	The proof is mady by induction. We first consider the formula fallen from the sky:
	
	We show that it is correct for $k = 0$, then we do an induction on $k$ for $k\in \mathbb{N}$.
	
	For $k = 0$, we have the well-known relation (\SeeChapter{see section Differential and Integral Calculus page \pageref{fundamental theorem of calculus}}):
	
	Suppose the property true for $k<n$:
	
	We integrate by parts (\SeeChapter{see section Differential and Integral Calculus page \pageref{integration by parts}}) the term:
	
	Then we have:
	
	Therefore:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\subsubsection{Fourier Series (trigonometric series)}\label{fourier series}
	We name by definition "\NewTerm{trigonometric series}\index{trigonometric series}" a series of the form:
	
	or in a more compact form:
	
	The constants $a_n,b_n$ with $n\in \mathbb{N}^{*}$ are the coefficients of the trigonometric series usually named "\NewTerm{Fourier coefficients}\index{Fourier coefficients}".
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We have already mentioned this type of series in our study of the types of existing polynomials since Fourier series are in fact only trigonometric polynomials (\SeeChapter{see section Calculus page \pageref{trigonometric polynomials}}). Furthermore, we saw as example in the section of Functional Analysis during our study of scalar functional product that the sine and cosine functions were the bases of a vector space!
	\end{tcolorbox}
	If the series converges, its sum is a periodic function $f (x)$ of period $T=2\pi$, since $\sin (nx)$ and $\cos (nx)$ are periodic functions of period $2\pi$. So that:
	
	Let us now state the following problem: We give ourselves a known periodic function $f(x)$, piece-wise continuous of period $2\pi$. We ask ourselves if there is a trigonometric series converging to $f (x)$ under some conditions that must be satisfied on this series.
	
	Suppose now that the function $f (x)$, periodic and of period $2\pi$, can be effectively represented by a trigonometric series converging to $f (x)$ in the interval $[0, T]$, that is to say it the sum of this series:
	
	Suppose that the integral of the function of the left member of this equality is equal to the sum of the integral of all the terms of the above series. This will occur, for example, if we assume that the proposed trigonometric series converges absolutely, that is to say, the numerical series converges (by the property that the trigonometric functions are bounded):
	
	The serie:
	
	is then majorable and can be integrated term by term from $0$ to $T$ (where $T=2\pi$) which allows us to determine the different Fourier coefficients. But before we start let us present the following integrals that will be very useful later:
	
	\begin{center}
	\begin{tabular}{ccc}
	$\text{with }n,k\in \mathbb{N}\text{ and }n\ne k$
	&$\qquad$&
	$\text{with }n,k\in \mathbb{N}\text{ and }n = k$
	\end{tabular}
	\end{center}
	Before continuing, let us prove the value taken by these six integrals (following the request of readers). But first, remember that as $n,k \in \mathbb{N}$ then:
	
	
	\begin{enumerate}
		\item We proceed using the remarkable trigonometric relations (\SeeChapter{see section Trigonometry page \pageref{remarkable trigonometric identities}}) and the primitive of elementary trigonometric functions (\SeeChapter{see section Differential and Integral Calculus page \pageref{usual primitives}}):
		
		because as we have seen it in the section Trigonometry $\sin(k\pi)=0,k\in\mathbb{Z}$ and as $T=2\pi$ the two previous differences have all terms equal to zero such that at the end:
		
		
		\item For the second integral, we proceed using the same techniques and the same properties of trigonometric functions:
		
		
		\item And we continue like this also for the third one, according to the same properties:
		
		
		\item Once gain using the same methods (this becomes routine ...) first for $k\neq 0$:
			
			and for $k=0$ it comes immediately:
			
			
			\item Again ... (soon finish...) first for $k\neq 0$:
			
			and for $k=0$ it comes immediately:
			
			
			\item And finally the last (...):
				
		\end{enumerate}
		This small work done let us now come back on our topic... To determine the coefficients $a_n$ both members of equality:
	
	by $\cos(kx)$:
	
	The series of the second member of equality is majorable, since its terms do not exceed in absolute to the terms of the positive convergent series. So we can integrate term by term on every bounded segment $0$ to $T$:
	
	We have proved above that whatever the integer values that take $k$ or $n$ the second term in the parenthesis is always zero. It then remains only:
	
	But we have proved above that the integral on the right is always zero if $n$ and $k$ are different. This leaves only the case where $n$ and $k$ are equal. Meaning:
	
	In this situation, we first the special case where $k$ is zero. In that case:
	
	Therefore:
	
	It is obvious that the coefficient $a_0$ represents the average of the signal or of its DC component, if it exists.
	
	In the case where $k$ it is not zero, we have:
	
	Therefore:
	
	To determine the coefficients $b_n$ we proceed the same way but this time multiplying both members of equality by $\sin(kx)$:
	
	The series of the second member of equality is majorable because its terms are not higher in absolute values to the terms in the convergent positive series. So we can integrate term by term on every bounded segment from $0$ to $T$:
	
	We have shown proved before that whatever are the integer values that taken by $k$ or $n$ the first term of the parenthesis is always zero. It remains then only:
	
	
	But we have proved before that the integral on the right is always zero if $n$ and $k$ are different. This leaves only the case where $n$ and $k$ are equal. Meaning:
	
	In this situation, we first have the special case where $k$ is zero. But we see now that we have a zero indeterminacy. It is better to consider the general case from which have:
	
	Hence we easily derive that:
	
	Therefore, for the situation where $k$ is zero the coefficient is therefore equal to zero!
	
	So finally the Fourier coefficients are determined by the integrals:
	
	But as it's annoying to have three results for the coefficients we'll play a little with the definition of the Fourier series.
	
	Indeed by summing from $1$ to $+\infty$, rather than $0$ to $+\infty$, we have:
	
	This then allows us only to have to remember ($a_0$ therefore included!):
	
	Physicists have for habit to write the last two relations as follows:
	
	The possible decomposition of any periodic piecewise continuous function approximated by an infinite sum of trigonometric functions (sine or cosine) consisting of a basic function and its harmonics is named "\NewTerm{Fourier theorem}\index{Fourier theorem}" or "\NewTerm{Fourier-Dirichlet theorem}\index{Fourier-Dirichlet theorem}".
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/fourier_series_examples.jpg}
		\caption[Examples of some Fourier series]{Examples of some Fourier series (source: Mathworld)}
	\end{figure}
	It can also happen that sometimes we know the Fourier series and we are looking for the original function $f(x)$. As a companion example consider that we want to calculate:
	
	So this is like searching the original $f(x)$ of the above Fourier series.

	It follows therefore that $a_0=0$ and $b_n=0$ and:
	
	But as far as we know there is no easy way to extract $f(x)$ that seems accurate! So using hyperbolic trigonometry (\SeeChapter{see section Trigonometry \pageref{hyperbolic trigonometry}}), we write:
	
	Now these power series may be identifies as Maclaurin expansions of $-\ln(1-z)$ (see proof above) with $z=e^{\mathrm{i}x}$ for the first term and $z=e^{-\mathrm{i}x}$ for the second term.

	Therefore:
	
	
	The Fourier series allows implicitly to represent all the frequencies in a periodical signal whose function is known mathematically (closed form). We can wonder why talk about Fourier series when, in practice, we do not really know the mathematical representation of a signal? This will bring us to a better understanding of the concept of the Fourier transform in discrete-time that we will see a little further, which does not need a mathematical representation of a continuous and periodic signal.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/algebra/time_frequency_domain.jpg}
		\caption{Time\index{time domain} and frequency domain\index{frequency domain}}
	\end{figure}
	We notice also that if $f (x)$, that is to say the periodic function of which we seek expression in trigonometric Fourier series, is even then the series will also be even and thus contain only cosine terms (the cosine function being an even function) implying that $b_n=0$ and otherwise in the case of an odd function $a_n=0$ (the sine being for reminder an odd function)!
	
	It should be noted, and this is important for what will follow, that as we have seen in the section of Calculus during our study of trigonometric polynomials, Fourier series could be written in the following complex form (by changing some notations and passing the sum to infinity):
	
	and we have seen that (always in the section of Calculus) that:
	
	Therefore:
	
	This gives us:
	
	Therefore:
	
	Or more generally:
	
	So if we take the famous case where $t_0=-T/2$ we get:
	
	Obviously the two relation above are the Fourier coefficient in the "time space" (or "time domain") point of view. We will see in the section of Electrical Engineering that there is an equivalent if we work with Fourier series in the frequency space (or "frequency domain").
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Upon decomposition of a continuous signal, we say (improperly at our point of view) that the coefficients $a_n,b_n$ are each (implicitly) a separate frequency associated with an amplitude that we visualize on a graph by vertical lines. This graph shows the "\NewTerm{frequency spectrum}\index{frequency spectrum}" of the decomposed signal. We can also add another representation which is named "\NewTerm{phase spectrum}\index{phase spectrum}". This spectrum gives us the phase of the harmonic signal (in phase advance or delay).
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/fourier_spectrum_graph.jpg}
		\caption{Example of amplitudes and frequencies associated to the different coefficients}
	\end{figure}
	Let us see now how to decompose a known periodic signal into several distinct amplitudes and frequencies signals
	Let us take for example, a periodic square wave signal defined over a period $T = 2$ and of amplitude $A$ such that:
	
	At period $T = 2$ corresponds as we know a pulsation:
	
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	Let us calculate first the coefficients $c_k$ thanks to the integral that determine the coefficients (the choice of the bounds of the integral is therefore assumed that the signal is periodic by construction!):
	
	Taking $k = 2$, we have:
	
	Similarly for $k = 4,6,8$ and for any even number.\\
	
	About odd numbers, we will have:
	
	The coefficients will then be:
	
	There is only problem in this relations, the coefficient $c_0$ cannot be calculated according to this relation because you can see that if $k = 0$ in the result above, we have an infinite value and it is at least impossible. The coefficient is null or not null  but never infinite (at least in physics because this implies and infinite energy).\\
	
	To find the coefficient $c_0$, we must calculate the integral for $k = 0$. The coefficient $c_0$ is then determined by:
	
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	The "frequency" spectrum (caution to the abuse of language!) and amplitude will be of the following form for $k=-5...+5$ and $A=1$ null frequencies not being shown:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/fourier_coefficients_example.jpg}
		\caption{Frequency spectrum of the example Fourier series coefficients}
	\end{figure}
	E2. Let us consider another famous case! The signal decomposition of the  square plus of amplitude $A$ and width $T_p$ (this function is sometimes denoted $\Pi_P(t)$) and of period $T$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/square_pulse.jpg}
	\end{figure}
	
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	So we fall back on a discrete version of the famous cardinal sine function (\SeeChapter{see section Trigonometry page \pageref{sinc cardinal}})!
	\end{tcolorbox}
	The abuse to talk about "frequencies" for Fourier coefficients thus leads us to have negative frequencies on the  $x$-axis... but it's only a question of vocabulary (there is no direct relation with the real frequencies) with which you must be familiar.
	
	The amplitude spectrum and phase is calculated according to the following relations:
	
	It is then relatively easy to notice that if $T$ tends to a larger and larger number, the spectrum peaks approach increasingly. So when $T$ tends to infinity the spectrum becomes continuous!!!
	
	The phase spectrum of the above example will give the following for the odd values:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/fourier_phase_diagram.jpg}
		\caption[]{Phase spectrum for the Fourier Series}
	\end{figure}
	It is even possible for example to obtain relatively easily the frequency spectrum in a software like Microsoft Excel 11.8346 (the reader will find an example much more detailed and interesting on the companion exercise server in the section Sequences and Series) !!!
	
	Indeed, it is enough for this purpose to sample for example our signal $128$ times (Microsoft Excel 11.8346 needs $2^n$ samples and works only under this condition!). Then we divide the interval $-1<t<0$ in $64$ samples and ditto for the interval $0>t>1$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/signal_sample_01.jpg}\\
		\includegraphics{img/algebra/signal_sample_02.jpg}\\
		\includegraphics{img/algebra/signal_sample_03.jpg}
		\caption[]{Signal sample}
	\end{figure}
	Which gives in graphical form (be careful because for the discrete Fourier transform works well in Microsoft Excel 11.8346, it is necessary that the sampling frequency - corresponding to the number of measurements in a second - is at least 100 times higher than the frequency of the original signal otherwise the result can be absurd!):
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/signal_fourier_transform_excel.jpg}
		\caption[]{Graphical representation of the data series in Microsoft Excel 11.8346}
	\end{figure}
	Afterwards in Microsoft Excel you simply go to the menu \textbf{Tools/Utility Analysis} and choose the \textbf{Fourier Analysis} option:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/excel_data_analysis_tool.jpg}
		\caption[]{Screenshot of \textbf{Utility Analysis} dialog box of Microsoft Excel 11.8346}
	\end{figure}
	Then comes the following dialog box that must be fill-in as indicated below (we see that the $x$-axis does not matter!):
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/excel_fourier_analysis_dialog_box.jpg}
		\caption[]{Parameters of the \textbf{Fourier Analysis} tool in Microsoft Excel 11.8346}
	\end{figure}
	Then comes the following generated list for the coefficients:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/fourier_coefficients_excel_list_01.jpg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/fourier_coefficients_excel_list_02.jpg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/fourier_coefficients_excel_list_02.jpg}
		\caption[]{Corresponding Fourier coefficients to sampled signal with Microsoft Excel 11.8346}
	\end{figure}
	It remains to calculate the module of the complex numbers with the native function  \texttt{IMABS( )} function in Microsoft Excel 11.8346 and divide the result by $128$ for each of the coefficients $c_n$ but we already see that each pair even coefficient  is zero and this match well the theoretical result obtained previously.
	
	We then have putting the index $n$ in front of each module:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/fourier_coefficients_excel_list_completed_01.jpg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/fourier_coefficients_excel_list_completed_02.jpg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/fourier_coefficients_excel_list_completed_03.jpg}
		\caption[]{Module of complex coefficients of the example Fourier Transform with Microsoft Excel 11.8346}
	\end{figure}
	By plotting a customized scatter diagram (still with  Microsoft Excel 11.8346) of columns D and E, we finally get (we restricted the $x$-axis to $[-5, +5]$ for easier reading:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/excel_fourier_spectrum_frequencies.jpg}
		\caption[]{Frequency spectrum of the transformed with Microsoft Excel 11.8346}
	\end{figure}
	To compare with the theoretical calculations (chart already presented previously) ...:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/fourier_coefficients_example.jpg}
		\caption{Theoretical frequency spectrum of the example Fourier series coefficients}
	\end{figure}
	Or in a more general and pedagogical way:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/algebra/frequencies_fourier_series.jpg}
	\end{figure}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us now consider another example identical to the previous with a different approach. We define a periodic function of period $T=2\pi$ as follows:
	
	Let us calculate the Fourier coefficients (we translate the bounds of the integral since the function is periodic this change nothing but facilitate the calculations!):
	
	and:
	
	We notice that $b_n$ is equal to $0$ for $n$ even and equal to $4\pi/n$ when $n$ is odd.
	The Fourier series of the function under consideration is thus written:
	
	What in Maple 4.00b will be written:\\
	
	\texttt{>S:=(4/Pi)*Sum(sin((2*n+1)*x)/(2*n+1),n=0..N);}
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	and that we can plot using the command:\\
	
	\texttt{>plot({subs(N=4,S),subs(N=8,S),subs(N=16,S)},x=-Pi..Pi,\\
	color=[red,green,blue],numpoints=200);}\\
	
	What gives three plots for $4$, $8$ and $16$ of the series in red, green and blue:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/fourier_series_with_various_terms.jpg}
		\caption{Example of Fourier series in Maple 4.00b with $4$, $8$ and $16$ terms}
	\end{figure}
	For $50$ terms we get:\\
	
	\texttt{> plot(subs(N=50,S),x=-Pi..Pi,numpoints=800);}\\
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/fourier_series_with_fifty_terms.jpg}
		\caption{Example of Fourier series in Maple 4.00b with $50$ terms}
	\end{figure}
	\end{tcolorbox}
	We see on the example above the side effects named "\NewTerm{Gibbs phenomenon}\index{Gibbs phenomenon}". It is possible to prove they occur to the value of the abscissa corresponding to $x=\pi/2n$ and matching equation and the peak rises to $\pm 1.179$ for all values of $n$. Let's see this!
	
	We proved just before for our example that:
	
	Which can be written:
	
	using the proof made much more earlier at the beginning of this section as that:
	
	Then we have:
	
	Remember that during our study of complex numbers (\SeeChapter{see section Numbers page \pageref{complex numbers}}) we proved that:
	
	Which brings us to:
	
	We will now focus on small values of $x$. So we can then make a Maclaurin  development of first order at the denominator (but not the numerator because of the presence of the $n$):
	
	We make a change of variable:
	
	where we used the traditional notation of the "cardinal sinus" in the last relation as defined in the section trigonometry (remember that this fraction is common in physics this is why it has a specific notation).
	
	As what interest us is to determine the maximum of the Gibbs phenomenon (the disturbance), we see that it takes place in this particular case that we presented (see figure above) for each multiple of $\pi$ and as the denominator of the expression of the integral will decrease as the multiple is higher, it follows that the greatest maximum is at the point where $2nx=\pi$  (the point $0$ at the opposite will cancel the integral therefore we must this latter of our choice). Then we have:
	
	the evaluation of this integral can be done only numerically as far as we know, therefore we get:
	
	That is about $18\%$ above the expected threshold value.
	
	\paragraph{Fourier series generative art}\mbox{}\\\\
	Before continuing on hard mathematical topics about Fourier series, let us open a parenthesis where the purpose here take the science out of this quote\index{Fermi's elephant}:
	\begin{fquote}[J. Von Neumann]With four parameters i can fit an elephant.
 	\end{fquote}
 	With computer it is easy to drawing stuffs using points moving on a circle, that are themselves centered on another circle like illustrated below (an epicycloid):
 	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\textwidth]{img/algebra/generative_art_epycloid.jpg}
	\end{figure}
	or a more complicated example:
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\textwidth]{img/algebra/generative_art_epycloid_three_points.jpg}
	\end{figure}
	Obviously as the examples above, the results change vary depending on the rotating speed of each point on its circle:
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\textwidth]{img/algebra/generative_art_epycloid_three_points_various_speed.jpg}
	\end{figure}
	or a case with eight circle (each of them have a point moving at a specific rotation speed):
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\textwidth]{img/algebra/generative_art_fermi_elephant.jpg}
	\end{figure}
 	is based in fact on Fourier series!
 	
	To understand why consider the following example:
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\textwidth]{img/algebra/generative_art_two_points_with_equations.jpg}
	\end{figure} 
	The first point is described  obviously by the parametric equation:
	
	and the second point therefore by (if it turns two time faster than the angle in the main circle):
	
	This is the parametric equation of the following epicycloid:
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.6\textwidth]{img/algebra/generative_art_epycloid_two_points.jpg}
	\end{figure} 
	In the general case we have:
	
	where $a_i$ are the radius of the circles and $n$ are the various circles speed (pulsation).
	
	But the question is rather the opposite. From a given curve, how can found the values of the parameters?
	
	The idea is quite simple. It's to follow respectively each coordinate in function of another one as illustrated below, where the closed shape is transformed into two curves:
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{img/algebra/generative_art_epycloid_conversion_into_curves.jpg}
	\end{figure} 
	and as seen during our study of the Fourier series, we know how to found the Fourier series of such curves!
	
	We know that the Fourier coefficients are given by:
	
	If we do that numerically for example with $x(t)$:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{img/algebra/generative_art_epycloid_x_t_numerical_integration.jpg}
	\end{figure} 
	That gives:
	
	Therefore:
	
	and using the same procedure, we could find:
	
	Therefore:
	
	We see however that $x(t)$ doesn't have only cosine terms and that $y(t)$ doesn't have only  sine terms. Then playing a bit with elementary trigonometric identities we can found (it's not absolutely a necessary transformation but like this it will be in adequation with the general expression given earlier above):
	
	Now let us come back on the four parameter elephant challenge. A satisfying result is:
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\textwidth]{img/algebra/generative_art_fermi_elephant_4_parameters.jpg}
	\end{figure} 
	with:
	
	with the four parameters:
	
	Some people have fun doing other things:
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{img/algebra/generative_art_woman_with_a_perl.jpg}
	\end{figure} 
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\textwidth]{img/algebra/simpson_generative_art.jpg}
	\end{figure} 
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	If the reader want to see how we plot the Fermi's elephant, he can refer to our R companion book.
	\end{tcolorbox}
	
	
	\paragraph{Power of a signal}\mbox{}\\\\
	A periodic signal has an infinite energy and null average power (\SeeChapter{see secton Electrokinetics}). Its average power over a period is then defined by:
	
	If we develop this equation, we have:
	
	This means that the power of a continuous-time periodic signal is equal to the sum of the squared Fourier coefficients. This is what we name the "\NewTerm{Parseval's theorem}\index{Parseval's theorem}\label{Parseval theorem}". This means that if we have any signal that can be decomposed in Fourier series, we can know the power of that signal using only the spectral coefficients.
	
	In reality, we can't mathematically determine the expression of this signal, we use therefore discretization or sampling and then we use a discrete Fourier transform, we can calculate the power of this signal using only the spectral coefficients. This gives us a characteristic of the signal.
	
	Let us also indicate the following result which will be very useful to us in the section of Thermodynamics for the study of the black-body and is also very closely related to important properties of the Riemann zeta function:
	
	The following relation:
	
	is named "\NewTerm{Parseval's equality}\index{Parseval's equality}".
	
	According to the definition of the Fourier series and the definition of the coefficient $a_0$ which follows immediately, we also have frequently in the literature:
	
	
	
	\pagebreak
	\paragraph{Fourier Transform}\label{fourier transform}\mbox{}\\\\
	Fourier series are a very powerful tool for the analysis of periodic signals for example, but the set of periodic functions is small compared to all the functions that we encounter in physical and engineering problems. So, will we introduce a new extremely powerful analytical tool that extends to a class of more general functions that have very important applications in signal processing, image processing, sound processing, statistics, finance (fourier option pricing techniques) and markets advanced statistics!!!
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Many teachers and authors associate Fourier Series and Fourier Transform in the field of Functional Analysis. This is right in fact but it seemed to us more appropriate to put the study of the basics of this two subjects in this section because closely related to Sequences and Series. However, the subjects that follows normally the study of Fourier Transform, that is to say: Laplace Transfoms, Hilbert Transform and others will be given in the section of Functional Analysis of this book (see page \pageref{fourier transform analysis}). The Fast Fourier Transform study can be found in the section of Theoretical Computing.
	\end{tcolorbox}
	The Fourier transform (FT) is then used for both periodic signals and for aperiodic signals.
	
	For this, we start from study of Fourier series with the complex notation of a periodic function of period $T$ by considering that the period is becoming increasingly big to such that $T\rightarrow +\infty$. Therefore the spectral lines gradually approach to turn into a continuous spectrum.
	
	Therefore, let us resume the expressions proved just earlier:
	
	that we can write equivalently in the following traditional form (wherein it is customary to put the factor $1/T$ rather in $f(t)$):
	
	and let us write this for future needs in the following form:
	
	and let us put naturally that:
	
	Thus, when $T\rightarrow +\infty$, the pulsation tends to zero and we have $\omega_n\rightarrow \omega$ because we move from discrete values in continuous values that browse through the set of real number $\mathbb{R}$ (for all $k$). Therefore:
	
	we pass to the limit that is to say:
	
	This implies that:
	
	Therefore we obtain for the coefficients (we change the notation because the previous one is inadequate)
	
	and for the infinite series (the sum becomes an integral)
	
	Caution!!! To make the difference between the given function and its equivalent in which we seek expression in infinite sum, we will denote them differently now. Thus, we get:
	
	Thus the discrete Fourier series becomes a continuous function.
	
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] We name "\NewTerm{Fourier Transform (FT)}\index{Fourier transform }" of $f$, or more rigorously the "\NewTerm{Continuous Time Fourier Transform (CTFT)}\index{Continuous Time Fourier Transform }", the relation:
		
		sometimes also denoted as follows:
		
		sometimes also named "\NewTerm{spectral density amplitude}\index{spectral density amplitude}".
		
		\item[D2.] We name "\NewTerm{Inverse Fourier Transform (IFT)}\index{inverse Fourier Transform}", or more rigorously the \NewTerm{Continuous Time Inverse Fourier Transform (CTIFT)}\index{Continuous Time Inverse Fourier Transform }",  of $F$ the relation:
		
	\end{enumerate}
	Any such transformation technique (as there are many as we will see in the section of Functional Analysis!) is named an "\NewTerm{integral transformation}\index{integral transformation}".
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	There are many ways of writing the Fourier transform according to the choice of the initial value of $T$! The reader must also know that it is quite common in physics to invert the definitions...the Fourier transform may be designated as the Inverse Fourier transform and reciprocally...
	\end{tcolorbox}
	Some physicist and engineers prefer to make the two previous relations symmetrical by putting the same coefficient in both directions, which will be for example $1/\sqrt{2}$. This will give:	
	
	Let us also give the corresponding three-dimensional version that will serve us many times in wave mechanics, electrodynamics, wave optics or in the various sections of quantum physics of this book:
	
	To make things perhaps clearer (at least we hope so), let us prove generally that the previous Fourier transform $\mathcal{F}$ is isometric (retains the "norm" - or "modulus" if you prefer...)
	
	\begin{theorem}
	For any functions $f, g$ we have the functional inner product:
	
	But since the functions are in the complex space, as we saw in the section of Vector Calculus, then we must use the notation of the hermitian product:
	
	Remember that:
	
	\end{theorem}
	\begin{dem}
	Then we want to prove the equality:
	
	Explicitly:
	
	But the variable to integrate but must be the same and for $\mathcal{F}(g)$ to be implicitly dependent of $\vec{r}$ it is necessary to take the Fourier transform on $\vec{k}$. Such as:
	
	Therefore:
	
	Therefore using the Fubini's theorem (\SeeChapter{see section Differential and Integral page \pageref{fubini theorem}}):
	
	Thanks to this result, we have also proved (this is immediate)
	
	We have not specified the bounds: they are infinite in every definition (we include all possible $\vec{k}$ or $\vec{r}$).
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Let us now see and prove three interesting properties of the Fourier transform:
	\begin{enumerate}
		\item[P1.] If the function $f$ is an even function (\SeeChapter{Functional Analysis}), it comes a simplification of the Fourier Transform such that:
		
		
		\item[P2.] If $f$ is odd, we proceed in the same manner as above, and we get:
		
		
		\item[P3.] Very important property of the Fourier transforms which will be useful to us in finance (\SeeChapter{see section Economy page \pageref{economy}}), and also as part of the study of the heat equation (\SeeChapter{see section of Thermodynamics page \pageref{fourier transform heat equation}}).
		
		First remember that the Fourier transform is given by:
		
		We want to see what happens if:
		
		By doing an integration by parts (\SeeChapter{see section Integral and Differential page \pageref{integration by parts}}):
		
		we get:
		
		where we put ourselves in the situation where:
		
		Therefore:
		
		More generally:
		
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The branch of "\NewTerm{harmonic analysis}\index{harmonic analysis}", or "\NewTerm{2D Fourier analysis}\index{2D Fourier analysis}", is the branch of mathematics that studies the representation of functions or signals as a superposition of basic waves. It deepens and generalizes the notions of Fourier series and Fourier transform. The basic waves are named "harmonics", hence the name of discipline. During the last two centuries it has had numerous applications in physics and economics under the name "spectral analysis" and knows recent applications including signal processing, quantum mechanics, neuroscience, stratigraphy, statistics, etc.
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	\label{fourier transform pulse square}E1. Let us see now a first example (among the three fundamentals) of a Fourier transform that we use again in the sections about quantum physics as well as in wave optics. We will calculate the Fourier transform (spectrum) of the following function (rectangular pulse):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/algebra/fourier_transform_rectangular_pulse.jpg}
		\caption{Rectangular pulse example for Fourier Transform}
	\end{figure}
	We have therefore:
	
	where $\text{sinc}$ is the sine cardinal as we already know (\SeeChapter{see section Trigonometry page \pageref{sinc cardinal}}). So we fall back on the $\text{sinc}$ and if we take the squared modulus squared we therefore get the decomposition of a theoretical monochromatic wave diffracted by a rectangular slot!!! Thus, it seems possible to study the diffraction phenomena using the Fourier transform and this field is named "\NewTerm{Fourier optics}\index{Fourier optics}". We'll come back on this later in the section of Functional Analysis when we deepen the Fourier transforms.\\
	
	We know that the spectrum (described by the $\text{sinc}$ function) crosses zero every time that the sine function is zero, that is to say, every time the frequency is a multiple of $1 / a$.\\
	
	The spectrum of this pulse illustrates two important points regarding the limited time signals:
	\begin{enumerate}
		\item[P1.] A short signal has a broadband spectrum.
		\item[P2.] To a narrow spectrum correspond a long-term signal.
	\end{enumerate}
	\end{tcolorbox}

	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	E2. The Fourier transform of an integrable function $f$ is given as we know now by:
	
	Consider the integrable Gaussian function of the type \label{fourier transform gaussian function}
	
	with $a>0$ define on $\mathbb{R}$.\\
	
	We want to compute its Fourier transform because it is a very important case and particularly useful for solving the heat equation that we will treat in the section of Thermodynamics and also to solve the differential equation of Black \& Scholes in the section Economy.\\
	
	The brilliant trick, if we want to avoid making complex analysis on $3$ A4 pages, is to notice that $F(\omega)$ is the solution of the following linear differential equation:
	
	where $y$ is a function of $\omega$.\\
	
	Indeed deriving  $F(\omega)$  we get:
	
	Integration by parts gives us:
	
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	We recognize the expression of the Fourier transform of $f$. Therefore:
	
	This shows that the $F(\omega)$ is solution of the differential equation above.\\
	
	We have proved in the section of Differential and Integral Calculus  that the general solution of this differential equation is given by:
	
	where $A \in \mathbb{R}$. And as in the present case:
	
	The primitive $G (x)$ is therefore easy to calculate and we get:
	
	Therefore:
	
	To determine the constant $A$ it suffices to notice that:
	
	To determine the constant $A$ it suffices to note that:
	
	and therefore:
	
	It is then usual to say that the Fourier transform of a Gaussian is another Gaussian!!
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	E3.\label{inverse fourier transform square pulse} This last example is very important to understand the Nyquist sampling. We consider a square pulse in the frequency domain:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/algebra/inverse_fourier_transform_rectangular_pulse.jpg}
		\caption{Rectangular pulse example for Inverse Fourier Transform}
	\end{figure}
	Let us calculate its inverse Fourier transform:
	
	There is something important to notice here (or to remember whatever...)!!! First, we know that sinc is equal to $1$ at $t=0$. But we also know that every time $Wt=k\pi$ with $k\in\mathbb{N}^{*}$ we have $\mathrm{sinc}=0$. Therefore, every time $t=k\pi/W$ we have:
	
	To make things more nice, signal processing engineers like to write rather the previous result into the form:
	
	and as you have almost surely noticed... this introduce a new type of sine cardinal (sic...!).  You may ask yourself why....????? That's quite easy. Written this way the $\mathrm{sinc}_\pi$ cancels for:
	
	This is prettier.... and even has a name: the "\NewTerm{normalized sinc function}\index{normalized sinc function}" and result in the following well known plot in signal processing for the study of the Nyquist sampling theorem:
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/algebra/sinc_inverse_fourier_transform.jpg}
	\end{figure}
	\end{tcolorbox}
	We will see plenty of other Fourier transform in the section of Function Analysis page \pageref{usual Fourier transforms}.
	
	\pagebreak
	\subsubsection{Bessel Series}
	Bessel functions are very useful in many advanced fields of physics involving delicate differential equations to solve. The areas in which we find them most often are calorimetry (heat conduction), nuclear physics (physics of reactors), optics and fluid mechanics.
	
	This series are still not study too much in the graduate curriculum and it is often the role of the student to seek the additional information it needs on this subject in the library of his school. We wanted to present here the developments that avoid this approach while staying at home in front of our computer (furthermore books on the subject are quite rare...).
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We usually speak by abuse of language of "\NewTerm{Bessel functions}\index{Bessel functions}\label{bessel functions}" instead of "\NewTerm{Bessel series}\index{Bessel series}".
	\end{tcolorbox}
	There is a significant amount of Bessel functions but we will restrict ourselves to the study of those most used one in physics.
	
	\paragraph{Zero order Bessel's Functions}\mbox{}\\\\
	The function known as the "\NewTerm{Zero order Bessel's function}\index{zero order Bessel's function}", is defined by the power series:
	
	It is during the study of the properties of derivation and integration that Friedrich Bessel found that this series of power is a solution to a differential equation that is found frequently in physics. That is why it bears his name.
	
	If $u_r$ represents the $r$-th term of the series, we easily see that:
	
	which tends to zero as $r\rightarrow +\infty$, regardless of the value of $x$. This has for consequence that the series converges for all values of $x$. Since this is a series of positive power, the function $J_0(x)$ and all its derivatives are continuous function for all values of $x$, real or complex.
	
	\paragraph{$n$ order Bessel's Functions}\mbox{}\\\\
	The function $J_n(x)$, known as the "\NewTerm{$n$ order Bessel's function}\index{$n$ order Bessel's function}", is defined, when $n$ is a positive integer, by the power series:
	
	which converges for all values of $x$, real or complex.
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/bessel_functions.jpg}
		\caption[Plot of few Bessel functions]{Plot of few Bessel functions (source: Wikipedia)}
	\end{figure}
	and in Microsoft Excel or Maple 4.00b the previous function can be found under the name \texttt{BESSELJ( )}. For example for the previous graph in Maple, we just write:
	
	\texttt{>plot([BesselJ(0,x),BesselJ(1,x),BesselJ(2,x),BesselJ(3,x)],x=0..20);}
	
	Let us see in particular, that for $n=1$ we have:
	
	and when $n=2$:
	
	We can notice that $N_n(x)$ is an even function of $x$ when $n$ is even and odd if $n$ is odd (\SeeChapter{see section Functional Analysis page \pageref{even function}}).
	
	If we play to do engineering maths we notice by trial and errors that:
	
	Based on this trial and error approach we have using the above expression by factorizing the $(x/2)^{2k}$ term only:
	
	So finally we see after trial and errors again (instead than doing 5 pages of mathematical developments as do mathematicians) that\label{condensed expression of Bessel series}:
	
	More generally for $n$ that is non-integer we use the Euler Gamma function (\SeeChapter{see section Differential and Integral Calculus page \pageref{gamma euler function}}):
	
	That is also sometimes written by pure mathematicians (...):
	
	Now by differentiating the function $J_0(x)$ and comparing the result with the series $J_1(x)$, we see that without much pain that:
	
	We also find without too much difficulty, the following relation\label{bessel differential reccurence relation}:
	 
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In general by recursive reasoning we get:
	
	Therefore:
	
	This last relation will be useful to us in the section of Wave Optics for our study of the circular aperture diffraction (Airy disk).
	\end{tcolorbox}
	Using the fact that:
	
	and including it in the previous relation, we find:
	
	written in another way:
	
	$y=J_0(x)$ is therefore a solution of the differential equation of the second order:
	
	written otherwise:
	
	or even:
	
	A solution to a an equation of parameter $n$ which is not a multiple of $J_n(x)$ is named "\NewTerm{Bessel function of the second kind}\index{Bessel function of the second kind}". Let us suppose now that $u$ is such a function and let us put $v=J_0(x)$; then according to the relation:
	
	we have:
	
	Multiplying the first relation by $v$ and the second by $u$ and after subtracting, we get:
	
	we therefore also have:
	
	we can therefore write:
	
	Indeed, because if we develop, we find:
	
	For the equality:
	
	is satisfied, we have:
	
	Dividing by $xv^2$, we have:
	
	which is equivalent to:
	
	immediately, by integrating it comes:
	
	where $A$ is a constant. Consecutively we have since $v=J_0(x)$:
	
	where we recall, $A$ and $B$ are constants, and $B\neq 0$ if $u$ is not a multiple of $J_0(x)$ by definition!
	
	If in the last relation, $J_0(x)$ is replaced by its expression in terms of series we have:
	
		For those who want to check this last relation (I do not like this kind of algebraic calculations) with Maple 4.00b just write:
	
	\texttt{>1/x*taylor(1/(series(BesselJ(0,x),x))\string^2,x=0,5);}
	
	Therefore:
	
	consecutively if we put:
	
	where $Y_0(x)$ is a particular Bessel function of the second type named "\NewTerm{Bessel-Neumann function of the second kind of zero order}\index{Bessel-Neumann function of the second kind of zero order}".

	Identically to the fact that when $J_0(x)\rightarrow 0$ when $x \rightarrow 0$, the expression $Y_0(x)$ because of the term $\log(x)$ when $x$ is small approaches $Y_0(x) \rightarrow -\infty$ when $x\rightarrow +0$.
	
	Finally, it comes from what we have seen that $J_0(x)$ and $Y_0(x)$ are independent solutions of the differential equation:
	
	The general solution being therefore:
	
	where $A$, $B$ are arbitrary constants and $x>0$ so that $Y_0(x)$ is real.

	If we replace $x$ by $k$, where $k$ is a constant, the differential equation becomes:
	
	by multiplying the whole by $k^2$ we find the general form of the differential equation:
	
	whose general solution is:
	
	where $k>0$ such that $Y_0(kx)$ is real when $x>0$.
	
	In fact, the Bessel functions are solutions of the differential equation previously studied and solved by the "\NewTerm{Frobenius method}\index{Frobenius method}\label{Frobenius method}". Indeed, let us write:
	
	and let us make the substitution:
	
	substituting in $Ly$, we get:
	
	Now let us choose the $c_i$ to satisfy the differential equation such as:
	
	Therefore, unless $\rho$ is a negative integer, we have:
	
	By substituting these values in the relation:
	
	we get:
	
	Therefore:
	
	If we put $\rho=$ in the prior-previous relation, we get:
	
	
	\paragraph{Bessel's Differential Equations of order $n$}\mbox{}\\\\
	We have defined the Bessel series as:
	
	Let us put:
	
	and let us derivate as follows:
	
	But we also have:
	
	By subtraction:
	
	Which finally gives:
	
	This is also written:
	
	which is named the "\NewTerm{Bessel differential equation of order $n$}\index{Bessel differential equation of order $n$}" or more simply "\NewTerm{Bessel equation}\index{Bessel equation}". In fact, most schools or Internet sites give this differential equation as a definition but now it is clear that there is rigorous reasoning behind this equation.
	
	The solution is therefore of the type:
	
	which is still sometimes written using the gamma Euler function (\SeeChapter{see section Differential and Integral Calculus page \pageref{gamma euler function}}):
	
	It follows that:
	
	and therefore that $y=J_n(x)$ is the solution of this differential equation.
	
	We will fall back on such a differential equation during our study of the wave equation of a circular drum (\SeeChapter{see section Wave Mechanics page \pageref{circular drum}}), during our study of the physics of nuclear reactors (\SeeChapter{see section Nuclear Physics}) and finally during our study of self-buckling (\SeeChapter{see section Mechanical Engineering page \pageref{self-buckling}}).
	
	\subsection{Convergence Criteria}
	When we study a series, one of the fundamental questions is that of the convergence or divergence of this series.
	
	If a series converges, its general term approaches zero as $n$ approaches infinity:
	
	or obviously generally:
	
	This criterion is necessary but insufficient to establish the convergence of a series. By cons, if this criterion is not met, we are absolutely sure that the series does not converge (so it diverges!).
	
	Three methods are proposed to deepen the convergence criteria:
	\begin{enumerate}
		\item The integral test

		\item The d'Alembert rule

		\item The Cauchy rule
	\end{enumerate}
	In the following paragraphs, we will assume the series with positive terms. The case of alternating series will be seen later.
	
	\subsubsection{Integral Test}
	The integral test for convergence is a method used to test infinite series of non-negative terms for convergence. It was developed by Colin Maclaurin and Augustin-Louis Cauchy and is sometimes known as the "\NewTerm{Maclaurin–Cauchy test}\index{Maclaurin–Cauchy test}".
	
	Given the series with decreasing positive (monotone decreasing) terms:
	
	That is to say:
	
	and given a continuous decreasing function such that:
	
	Then the infinite series
	
	converges to a real number if and only if the improper integral:
	
	is finite. In other words, if the integral diverges, then the series diverges as well.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In no case does the integral gives the value of the sum of the series! The full test only gives an indication of the convergence of the series. Before making the test of the integral, it is important to check that the terms of the series are strictly decreasing to fill the condition $a_1\ge a_2\ge a_3\ge \cdots\ge a_n\ge \cdots$.
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The harmonic series:
	
	diverges because (\SeeChapter{see section Differential and Integral Calculus page \pageref{usual primitives}}):
	
	So this harmonic series does not converge.
	\end{tcolorbox}
	
	\subsubsection{D'Alembert Rule}
	The "\NewTerm{ratio test}\index{ratio test}" is also a test (or "criterion") for the convergence of a series of the type:
	
	where each term is a real or complex number and $a_n$ is nonzero when $n$ is large. The test was first published by Jean le Rond d'Alembert and is sometimes known as "\NewTerm{d'Alembert's ratio test}\index{d'Alembert's ratio test}" or as the "\NewTerm{Cauchy ratio test}\index{Cauchy ratio test}".
	
	The usual form of the test makes use of the limit:
	
 	The ratio test states that:
	\begin{enumerate}
		\item if $L < 1$ then the series converges absolutely;
		\item if $L > 1$ then the series does not converge;
		\item if $L = 1$ or the limit fails to exist, then the test is inconclusive, because there exist both convergent and divergent series that satisfy this case
	\end{enumerate}
	and we define the radius of convergence by:
	
	For the proof suppose that:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In reality this rule is normally without the absolute value. The case with the absolute value as above is named the "\NewTerm{absolute convergence test}\index{absolute convergence test}" and applied for the more general case of an alternate series such that:
	
	If an alternating series of terms is absolutely convergent, the absolute series that follows also converge.\\
	
	Therefore the absolute convergence test is a generalization of the d'Alembert rule but most of time we don't make any distinctions between the both.
	\end{tcolorbox}
	We can then show that the series converges absolutely by showing that its terms will eventually become less than those of a certain convergent geometric series. To do this, let:
	
	Then $r$ is strictly between $L$ and $1$, and:
	
	 for sufficiently large $n$ (say, $n$ greater than $N$).

	Hence:
	
	for each $n > N$ and $i > 0$, and so:
	
	That is, the series converges absolutely.

	On the other hand, if $L > 1$, then:
	
	for sufficiently large $n$, so that the limit of the sum is non-zero. Hence the series diverges.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Given the following geometric series:
	
	We get quotient:
	
	Therefore the series converge!
	\end{tcolorbox}
	Obviously some practical applications will (can) give for example:
	
	and some practitioners name this the "\NewTerm{Cauchy convergence rule}\index{Cauchy convergence rule}"... (do not confuse with the Cauchy convergence test that will be study in the section Fractals).
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us do now a last important example. We have study earlier above Bessel series. But it can be not obvious that these series converge. Let us prove that this is indeed the case for $J_0$, that is to say for:
	
	Therefore:
	
	\end{tcolorbox}
	
	\subsubsection{Alternating Series Test}
	The alternating series test is a method used to prove that an alternating series with terms that decrease in absolute value is a convergent series. The test was used by Gottfried Leibniz and is sometimes known as "\NewTerm{Leibniz's test}\index{Leibniz's test}", "\NewTerm{Leibniz's rule}\index{Leibniz's rule}, or the "\NewTerm{Leibniz criterion}\index{Leibniz criterion}".
	
	A series of the form
	
	where either all an are positive or all an are negative, is named an "\NewTerm{alternating series}\index{alternating series}".

	The alternating series test then says: if $a_n$ decreases monotonically and:
	
 	then the alternating series converges.
	
	There a lot of other tests as the Raabe's test, the Bertrand's test, the Gauss's test, the Kummer's test.
	
	\subsubsection{Fixed Point Theorem}\label{fixed point theorem}
	The fixed point theorem is not really useful in physics and for the engineers (but implicitly it is essential but physicists and engineers often use math tools whose properties have already been approved in advance by mathematicians), however we find it in chaos theory and in theoretical computing (see the sectiona on Fractals especially the topic on the Sierpinski triangle). We can therefore only recommend the reader to take the time to read and understand the explanations and developments that follow.
	
	Let $(X,d)$, be a complete metric space (\SeeChapter{see sections Topology page \pageref{metric space} or Fractals page \pageref{fractal metric space}}) and $T:X\mapsto X$ an application strictly contracting of constant $L$ (see the Lipschitz functions in the section Topology page \pageref{lipschitz functions}), then there exists a unique point $\omega\in X$ such that:
	
	$\omega$ is then named the "\NewTerm{fixed point}\index{fixed point}" of $T$ (think to the case $\cos(x)=x)$. Furthermore, if denote by:
	
	the image of $x$ by the $n$-th iterate of $T$, then we have:
	
	and the convergence speed can also be estimated by:
	
	By the fact that we restrict our study to iterating a function, we speak of "\NewTerm{Banach fixed-point theorem}\index{Banach fixed-point theorem}\label{banach fixed point theorem}" that gives a general criterion guaranteeing that, if it is satisfied, the procedure of iterating a function yields a fixed point.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	You can have fun with your pocket calculator or your operating system by choosing a random number and taking the cosine iteratively. You will find you will tend to $0.74$ and therefore it is verbatim the solution of $\cos (x) = x$.
	\end{tcolorbox}
	\begin{dem}
	Given $x\in X$. We consider the following sequence $(T^n(x))_{n\in \mathbb{N}}$ as defined above. First we will prove that this sequence is a Cauchy sequence (see above what is a Cauchy sequence).

	Applying the triangle inequality (\SeeChapter{see section Vector Calculus page \pageref{triangle inequality}}) several times we have:
	
	But:
	
	Therefore:
	
	and as:
	
	Therefore:
	
	To finish:
	
	that is to say, that in a first time $(T^n(x))_{n\in \mathbb{N}}$ converge, and we put:
	
	Now we check that $\omega$ is a fixed point of $T$. Indeed $T$ is uniformly continuous (as Lipschitz - see section Topology page \pageref{lipschitz functions}) therefore a fortiori continues:
	
	It remains to check that $\omega$ is the only fixed point (therefore we will have proved that $\omega$ does not depend on the choice of $x$). Suppose that we also have $T(y)=y$ then:
	
	An estimate of the speed of convergence is given by:
	
	$\mathrm{d}(,)$ is continuous with respect to each variable so:
	
	and the limits preserve the inequalities (not strict one) thus:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\pagebreak
	\subsection{Generating Functions (transformation of a sequence into a series)}
	For some mathematical financial risk management models (\SeeChapter{see section Economy page \pageref{economy}}) and also integration transformation of Bessel functions (\SeeChapter{see section Differential and Integral Calculus page \pageref{integral representation of first kind Bessel's function}}) we will need in this book a gentle introduction to generating functions.
	
	\subsubsection{Ordinary Generating Functions (transformation of a sequence into a series)}
	Remember first that we have proved earlier above that the general Maclaurin expansion of a function was given by:
	
	That is for $x=\cong 0$
	
	In the case of our study, let us write the latter relation as:
	
	We say then that the above relation is the "\NewTerm{ordinary generating function}\index{ordinary generating function}\label{ordinary generating function}" of the sequences of numbers $a_0,a_1,a_2,a_3,\ldots$ in the formal parameter $x$. And we don't care if it diverge or not!
	
	Because the generating function is an algebraic expression that encodes the sequence and allows us to manipulate it in ways that are not possible in other forms. Many times if the sequence you are looking at is "interesting" (and this word has lots of interpretations), the generating function has a short simple form.

	The generating function allows us to derive formulas for the sequence, identities involving the sequence, estimate the values and so much more as we will see further below.	
	
	One thing to never forget: The generating function is not the sequence and the sequence is not the generating function. They are not the same thing. One is a sequence, the other is an algebraic expression!

	If you have a sequence you can say "the generating function of the sequence" to refer to the algebraic object. If you have a generating function you might say "the sequence of coefficients of the generating function" in order to refer to the sequence.
	
	Let us try now a companion example, the sequence consisting of all $1$:
	
	The generating function is therefore the geometric series that we have proved earlier above implicitly (explicitly you can see the proof at page \pageref{sum of powers}):
	
	The next simple companion example would be the positive integers:
	
	This has generating function:
	
	Now we observe that the derivative of this generating function is the derivative of the generating function:
	
	Indeed:
	
	That is we have therefore:
	
	We conclude that:
	
	We can't use the exactly same trick to figure out the generating function for the sequence:
	
	because if we take the derivative of:
	
	then we do not quite have the square integers. But the attentive reader will notice that if we first multiply the latter relation by $x$ and then take the derivative then we have:
	
	Therefore:
	
	It is easy with a software like Maple to control that the Maclaurin series of $(1+x)/(1-x)^3$ is equal to the generating function and therefore give us the coefficients $a_i$ of the sequence.
	
	Let us see now a non-trival example... It's the Fibonacci sequence given for recall (see page \pageref{Fibonacci Sequence}) by (each term is equal to the sum of the previous two):
	
	We will give the generating function for this sequence a name $F(x)$ so then:
	
	where $F_0=F_1=1$ and $F_k=F_{k-1}+F_{k-2}$ for $k\geq 2$. Then we can see that:
	
	Since we have figured out that:
	
	then:
	
	and this can be rewrittten as:
	
	and hence:
	
	It is always surprising that the generating function for the Fibonacci numbers has such a compact formula. But once again, using for example a software like Maple and doing the Maclaurin expansion of the above function, you will get a series whose coefficients $a_k$ corresponds to the Fibonacci sequence!
	
	Bu let us continue to investigate...! We have therefore the denominator:
	
	that can be factorized assuming the equation:
	
	has solutions! We quickly found that the roots are given by:
	
	We recognize here the negative values of the Golden ratio $\varphi$ and its conjugate (see earlier above page \pageref{golden ratio}).
	
	Therefore the denominator can be written as (using the relation between the Golden ratio and its conjugate):
	
	Thus we can write:
	
	Let us decompose this fraction (the $-$ sign is now the in the constants $A$ and $B$):
	
	where $A$, $B$ need to be found out. Clearly we have:
	
	By identification, we have the following linear system:
	
	Hence:
	
	Or written explicitly:
	
	The solution to this linear system is straightforward and given by:
	
	Thus we have:
	
	Next we use another result the famous Taylor development:
	
	for $|x|<1$. Using this result we can see that:
	
	Hence:
	
	Hence:
	
	This result is sometimes named the "\NewTerm{Fibonacci Binet's Formula}\index{Fibonacci Binet's Formula}".
		
	\paragraph{Composition of Generating functions}\mbox{}\\\\
	Now if we have two generating functions:
	
	for two sequences of integers $a_0,a_1,a_2,a_3,\ldots$ and $b_0,b_1,b_2,b_3,\ldots$ then there are several ways that we can combine the sequences and get generating functions fr new sequences.

	\begin{itemize}
		\item Sum: If we add the generating functions we have that:
		
		is a generating function for the sequence:
		
		
		\item Product: However if we multiply the two generating function, we ha that:
		
		This can be summarized in the expression:
		
		Another special case of the product of generating function is the product:
		
		It is the product of two generating functions, the first one being the generating function:
		
		By:
		
		the product of these is a generating function for the sequence (as all $a_i=1$):
		
	
		\item Derivative: We have already seen a couple examples of the
use of the derivative in previous examples.
	\end{itemize}
	Remember now that we have proved that:
	
	Therefore by the fact that:
	
	has for generating sequence:
	
	we have then that:
	
	is then a generating function for the sequence of the sum of the first $n$ positive integers:
	
	In particular, the coefficient of $x^k$ is:
	
	We also know by taking the derivative of $1/(1-x)^2$ we have:
	
	Therefore if we divide this equation by two we have:
	
	It must be that the coefficient of $x^k$ in $1/(1-x)^3$ is equal to $(k + 1)(k + 2)/2$ and it is equal to the sum of the first $k+1$ integers, so:
	
	
	\subsubsection{Multivariate Generating Functions}
	Remember that in the section of Calculus we have proved that:
	
	can be expressed by a condensed form that involves the binomial coefficient:
	
	then this is what we name a "\NewTerm{multivariate generating function}\index{multivariate generating function}". It works just as the other generating functions we have previously worked with except that it has two parameters.
	
	\subsubsection{Functional Generating Functions}
	So far we have seen generating functions that gives scalar values. But a more general family are the "\NewTerm{functional generating functions}\index{functional generating functions}\label{functional generating function}" that gives functions instead of simple scalars.
	
	A generating function for a sequences of functions $\{f_n(x)\}$ is obviously a power series of the type:
	
	whose coefficient are now functions of $x$.
	
	Let us see the both examples that will be useful to us in the sections of Economy, Wave Optics and also Differential and Integral Calculus!.
	
	Let us start with the most important example involving probabilities and especially the Poisson distribution as used in the First Boston Credit Risk Metric model in the section Economy!
	
	Let us recall that the Poisson distribution mass, that is, the probability that $N$ is equal to $n$, is given by (\SeeChapter{see section Statistics page \pageref{poisson distribution}}):
	
	for $n=0,1,2,\ldots$.

	The probability generating function is equal to:
	
	We know also the following Maclaurin infinite series expansion:
	
	And if we rewrite:
	
	in the following way:
	
	And further assuming the above Maclaurin expansion we can write:
	
	Which implies:
	
	This is the probability generating function of a Poisson distribution that we will use for our study of the CreditRisk model.
	
	And now for our study of Wave Optics (especially the Fresnel diffraction for a circular aperture!) let us found the functional generating function of the first kind Bessel functions!
	\begin{theorem}
	The generating function for the sequence of Bessel function of first kind, on integer order, is:\label{generating function for bessel function of first kind}
	
	\end{theorem}
	\begin{dem}
	To obtain an expression for $J_n(x)$, we use the Maclaurin series for $e^x$ to get:
	
	Now let us  make the change of variable: $n=r-s$.

	Therefore the expression in the sum becomes:
	
	Then it is obvious that as the range of $r$ and $s$ is $]-\infty,+\infty[$ we have for the range of $n$ also: $]-\infty,+\infty[$. So the first sum is easy to determine:
	
	But as we can see in the expression in the sum above, we don't get rid of the summation variable $s$. So there is obviously a second missing sum on the variable $s$! The interval of summation for $s$ is not obvious at a first glance...

	What is sure is that $s$ is still in the range $[0,+\infty]$ (positivie values) if we look at the term $s!$ in the denominator. But what makes problem is the lower bound of the sum. So if we look closely to the relation in the sum above we have a term $(n+s)!$. Obviously $n+s$ must never be negative! It comes therefore that if $n<0$, we must have $n+s\geq 0$, that is when $n<0$, $s$ must start $-n$. Finally:
	
	That is:
	
	where the $J_n$ are the Bessel function of the first kind.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem} 
	Therefore, for $n\geq 0$:
	
	And for $n<0$:
	
	Using an index shift, we obtain:
	
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{95} & \pbox{20cm}{\score{3}{5} \\ {\tiny 44 votes,  64.09\%}} 
	\end{tabular} 
	\end{flushright}
	
	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Vector Calculus}\label{vector calculus}

	\lettrine[lines=4]{\color{BrickRed}V}ector calculus or "vector analysis" is a branch of mathematics that studies the scalar or vector fields that are sufficiently regular of Euclidean spaces (see definition further below).\\\\

The importance of vector calculus comes from its extensive use in physics and in the engineering sciences. It is from this perspective that we will present it, and this is why we limit ourselves mostly to the case of the usual three-dimensional spaces. In this context, a vector field associates to each point of the space a vector (with three real components), while a scalar field associates just a unique real number to such a point.

There is a phenomenal amount of series and theories about these, but we will mention especially the Taylor series (used almost everywhere in applied science), Fourier series (signal theory, statistics, wave mechanics or quantum physics) and Bessel series functions (very important in nuclear physics!) that we will make a brief study here and that will continue in the section of Functional Analysis.

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
For example, imagine the water from a lake. The temperature data at each point forms a scalar field, that of his speed at each point, a vector field (see definition further below).
	\end{tcolorbox}

Physical concepts such as strength or speed are characterized by a direction, an orientation and an intensity. This triple character is highlighted by arrows. These are the source of the concept of vector and are the most suggestive example. Although their nature is essentially geometric, it is their ability to bind to each other, so their algebraic behavior, which mostly retain our attention. Split into equivalence classes their set represents the classic model of a "\NewTerm{vector space}\index{vector space}" (\SeeChapter{see section Set Theory page \pageref{poisson distribution}}). One of our primary goals here is the detailed description of this model.

	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} Before reading what follows, the reader is advised to have at least read diagonally the section on Set Theory in the Arithmetic chapter. We define there what is a "vector space" using the tools of Set Theory. Even if this concept  is although not absolutely essential it is still interesting to see how two areas of mathematics fit together and also just for the reason... of introducing vector stuffs with a least a little bit rigour.\\
	
	\textbf{R2.} Vectorial analysis contains many terms and definitions that must be learned by heart. This work is hard but unfortunately necessary...
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Concept of Arrow}	

\textbf{Definition (\#\mydef):} We denote by $U$ the ordinary space of elementary geometry and $P, Q,...$ its points. We will call "\NewTerm{arrow}\index{arrow}" all directed line segment (in space). The arrow of origin $P$ (origin point) and extremity $Q$ (terminal point) will be denoted $\overrightarrow{PQ}$ or abbreviated by a single letter (Latin or Greek) arbitrarily chosen as example: $\overrightarrow{F}$.

	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
In the norm ISO 80000-2:2009 it is authorized to represent the vectors with a letter in bold.
	\end{tcolorbox}	

We will consider as obvious that any arrow is characterized by its direction, its orientation (because of a given direction it can point in both directions), its intensity or magnitude (length) and its origin.

In vector (or multivariable) calculus, we will deal with functions of two or three variables (usually $x, y$ or $x, y, z$, respectively). The graph of the arrow of coordinates $(x, y)$, lies in Euclidean space, which in the Cartesian coordinate system consists of all ordered doublets of real numbers $(a,b)$. Since Euclidean can be 3-dimensional (and more or less for sure!), we denote it by $\mathbb{R}^3$.

The graph of the arrow consists of the points $(a, b, c)$. The 3-dimensional coordinate system of Euclidean space can be represented on a flat surface, such as this page or a blackboard, only by giving the illusion of three dimensions, in the manner shown in the figure below:

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/algebra/euclidian_vector.eps}
\caption{Example of arrow in $\mathbb{R}^3$ euclidian space}
\end{figure}

Euclidean space has three mutually perpendicular coordinate axes ($x$, $y$ and $z$), and three mutually perpendicular coordinate planes: the $xy$-plane, $yz$-plane and $xz$-plane:

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/algebra/euclidian_planes.eps}
\caption{Mutually perpendicular planes in $\mathbb{R}^3$}
\end{figure}

The coordinate system shown above is known as a right-handed coordinate system, because it is possible, using the right hand, to point the index finger in the positive direction of the $x$-axis, the middle finger in the positive direction of the $y$-axis, and the thumb in the positive direction of the $z$-axis, as below:

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/algebra/right_hand.eps}
\caption{Right hand system}
\end{figure}


	\subsection{Set of Vectors}	

\textbf{Definitions (\#\mydef):}

	\begin{enumerate}
		\item[D1.] We say that two arrows are "\NewTerm{equivalent arrows}" \index{equivalent arrows}  if they have the same direction, the same orientation and the same intensity.
		\item[D2.] We say that two arrows are "\NewTerm{colinear arrows}"\index{colinear arrows} only if they have the same direction.
	\end{enumerate}
Let us now split the set of all arrows in equivalence classes: two arrows belong to the same class if and only if they are equivalent.

So:

\textbf{Definitions (\#\mydef):} 

\begin{enumerate}
	\item[D1.]Each equivalence class of arrows whose origin point and terminal point are distinct is a "\NewTerm{vector}"\index{vector}\label{vector}" or rather a "\NewTerm{free vector}"\index{free vector} because its origin is not taken into account (if its origin is well defined, then we have a "\NewTerm{bounded vector}")\index{bounded vector}.
	
	\item[D2.]Degenerated arrows (that is to say of the form $\overrightarrow{PP}$) are named "\NewTerm{zero vector}"\index{zero vector} and written $\vec{0}$ when they have an undefined direction and orientation and zero intensity (origin and terminal point are not distinct).
\end{enumerate}	

The set of vectors will be commonly referred  by $\mathbb{V}$. Note that the elements of $\mathbb{V}$ are (equivalence) classes arrows and not individual arrows. It is however clear that any arrow is sufficient to determine the class of equivalence to which it belongs and it is natural to name the corresponding class: "\NewTerm{representative class}"\index{representative class} of the vector.

Let us now draw a representative of a vector $\vec{y}$ from the end of a representative vector $\vec{x}$. The arrow whose origin is that of the representative $\vec{x}$ and the end of representative $\vec{y}$ determines a new vector which we write: $\vec{x}+ \vec{y}$. The operation that combines any two vectors by their sum is named "\NewTerm{vector addition}"\index{vector addition}.

\begin{figure}[H]
\centering
\includegraphics[scale=1]{img/algebra/vector_addition.jpg}
\caption{Example of a sum of two vectors}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=1]{img/algebra/vector_addition_robotics.jpg}
\caption{Example of a sum of two vectors with robot dynamics notation}
\end{figure}

Using a figure, it is easy to show that the operation of vector addition is associative and commutative, i.e. that:
	
and:
	
It is also evident that the zero vector $\vec{0}$ is the neutral element of the vector addition. Formally:
	
where $-\vec{x}$ means the opposite of vector $\vec{x}$, that is to say the vector whose representatives have the same direction and the same intensity as those of $\vec{x}$, but the opposite orientation. 

\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] Two vectors whose sum is zero are then named "\NewTerm{opposed vectors}"\index{opposed vectors} since the only thing that differentiates them is their orientation...
		\item[D2.] It follows that if two or more vectors have the same direction, the same intensity and the same orientation then  we say that they are "\NewTerm{equal vectors}"\index{equal vectors}.
	\end{enumerate}
As we can see the reverse operation of vector addition is the vector subtraction. Subtract a vector is equivalent to adding the opposite vector.

	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
\textbf{R1.} The addition extends, by induction, to the case of any finite family of vectors. Under associativity, these successive additions can be performed in any order, which justifies the writing without brackets.\\\\
\textbf{R2.} The multiplication between two vectors is a concept that does not exist. But, as we shall see it a little further, we can multiply the vectors by some other vector properties that which we call the "norm" or simply by scalars and still by some other things...
	\end{tcolorbox}	

\subsubsection{Pseudo-Vectors}

In physics, in the statement named the "\NewTerm{Curie's principle}"\index{Curie's principle} (\SeeChapter{see section Principia page \pageref{curie principle}}), physicists mention of what they name "\NewTerm{pseudo-vectors}"\index{pseudo-vector}\label{pseudo vector}. This is the simple vocabulary to talk about something equally trivial but basically only a few people actually do use. But it can still be useful to present what it is.

	In fact, vectors and pseudo-vectors are transformed in the same way for a rotation or translation (we will see in our study of Linear Algebra how mathematically perform this type transformations). It is not the same in symmetry with respect to a plane or at one point. In these transformations we have by definition the following properties:
	\begin{enumerate}
		\item[P1.] A vector is transformed into its symmetrical.
		\item[P2.] A pseudo-vector is converted into the opposite of its symmetrical.
	\end{enumerate}
	Here is a figure with typical examples (the choice of letters representing the vectors and pseudo-vectors is not due to chance; they are a wink to the properties of electric and magnetic fields as studied in the Electromagnetism chapter):
	\begin{figure}[H]
	\centering
	\includegraphics[scale=0.75]{img/algebra/pseudo_vector.eps}
	\caption{Differences of transformations between a vector and a pseudo-vector}
	\end{figure}

A well known practical example is a pseudo-vector that we will study in detail much further below and resulting of an operation named "\NewTerm{cross product}"\index{cross product}\label{cross product}:
	
		And to see why the result is a pseudo-vector, consider the special simple case:
	
	Now if we do a symmetric operation of the $X\text{O}Z$ plan we get:
	
	So as we can see the vector resulting of the cross product is a pseudo-vector because under transformation of the plan it's orientation change!

A vector resulting of a mathematical operation of symmetry that does not change its orientation is named a "\NewTerm{polar vector}"\index{polar vector} but in fact almost everybody say just "vector".

Now that we have an idea of what vectors are, we can start to perform some of the usual algebraic operations on them and this is what we name "\NewTerm{vector algebra}"\index{vector algebra}.

	\subsubsection{Normal vector}
	The "\NewTerm{normal vector}\index{normal vector}\label{normal vector}", denoted $\vec{N}$ or $\vec{n}$, to a surface is a vector which is perpendicular to the surface or curve at a given point. When normal vectors are considered on closed surfaces or closed curves, the inward-pointing normal (pointing towards the interior of the surface) and outward-pointing normal are usually distinguished.

	The unit vector obtained by normalizing the normal vector (i.e., dividing a nonzero normal vector by its vector norm) is the unit normal vector, often known simply as the "\NewTerm{unit normal}"\footnote{Care should be taken to not confuse the terms "vector norm" (length of vector), "normal vector" (perpendicular vector) and "normalized vector" (unit-length vector)}.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/algebra/normal_vector_field.jpg}
		\caption{A vector field of normals to a surface}
	\end{figure}
	In the three-dimensional case a "\NewTerm{surface normal}\index{surface normal}", or simply normal, to a surface at a point $P$ is a vector that is perpendicular to the tangent plane to that surface at $P$. The word "normal" is also used as an adjective: a line \textit{normal} to a plane, the \textit{normal} component of a force, the \textit{normal} vector, etc. 
	
	We will see and prove in details in this section and many others, how to calculate explicitly various normal vectors to curves and surfaces.
	
	For example that the normal vector to two vectors $\vec{a}$ and $\vec{b}$ is given by the cross product (see further below page \pageref{cross product}):
	
	Or the normal to a parametric curve in Differential Geometry page \pageref{first Frenet formula}:
	
	Or the normal vector of a plane of equation $ax+by+cz+d=0$ in the section of Analytical Geometry page \pageref{vector normal plane}:
	
	Or the gradient that is perpendicular to the isolines of a surface given by the cartesian function $f(x,y,z)=0$ of smoothness $\mathcal{C}^1$ as we will see further below page \pageref{gradient normal}:
	
	and so on...
	
	

\pagebreak
\subsubsection{Multiplication by a scalar}
The vector expression $\alpha\cdot \vec{x}$ named "\NewTerm{product of vector $\vec{x}$ by scalar $\alpha$}"\index{vector scalar product} is defined as follows:

Take a representative arrow $\vec{x}$ and construct a same direction arrow in the same or opposite orientation , depending on whether $\alpha$ (scalar) is positive or negative, and of intensity $\mid \alpha \mid$ times the intensity of the initial arrow. The arrow thus obtained is a representative of the vector of relation:
	
If $\alpha=0$ or $\vec{x}=0$ we write:
	
The operation consisting of performing the product of a scalar by a vector is named "\NewTerm{scalar multiplication}\index{scalar multiplication}".

We easily check that the scalar multiplication is associative and distributive with respect to the vector numerical addition, formally:
	

	The multiplication of a vector by non-null scalar doesn't change its direction if the scalar is positive but if the scalar is negative the vector will still have the same direction but it orientation will be opposite.
	
	From this definition he have that two vectors $\vec{v}$ and $\vec{w}$ are parallel (denoted by $\vec{v}\mid\mid\vec{w})$) if one is a scalar multiple of the other. You can think of scalar multiplication of a vector as stretching or shrinking the vector, and as flipping the vector in the opposite direction if the scalar is a negative number.
	
	
	Let's see a concrete example worldwide known of use of vectors with scalars (probably also the simplest example):
	
	\paragraph{Rule of three}\mbox{}\\\\
	Let us go back to the "\NewTerm{rule of three}"\index{rule of three} (sometimes also named "\NewTerm{rules of ratios and proportions}\index{rules of ratios and proportions}" or "\NewTerm{unit reduction method}\index{unit reduction method}") often define in small classes (middle-school) intuitively but without nice proof. This rule is probably the most widely used algorithm in the world that identifies a fourth number when are given three and the four numbers are linearly dependent.

The rule of three is derived most of time in two versions:
	\begin{enumerate}
		\item[V1.] Simple an direct if the magnitudes are directly proportional.
		\item[V2.] Simple and reverse if the quantities are inversely proportional.
	\end{enumerate}
and when two variables $X$ and $Y$ are proportional we note for recall:
	
\begin{theorem}
Suppose now that $X$ can take the values $x_1,x_2$. $Y$ will take  the values linearly dependent $y_1,y_2$ then the following proportional relation applies:
	
is named "\NewTerm{simple and direct ratio}\index{simple and direct ratio}".
\end{theorem}
\begin{dem}
	Given two collinear vectors $\vec{x}=(x_1,x_2),\vec{y}=(y_1,y_2)$ and therefore proportional to a given factor $\lambda$ such that:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
\end{dem}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
If this ratio is not equal (thus: not proportional), then we must switch to other tools such as simple and inverse ratio, or regression techniques and verbatim: extrapolation.
	\end{tcolorbox}	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
In Lausanne (Switzerland), in 2011, garbage bags ar taxed and following rates apply: a bag of $17 [L]$ is $1.-$ and the bag of $110 [L]$ is $3.80.-$. Reported to $17 [L]$ the price of the garbage bag of 110 [L] is thus:
	
That is to say approximately $60\%$ of the price of the bag of $17 [L]$ (then go search for an explanation... ???).
	\end{tcolorbox}
\begin{theorem}
The following proportional relation:
	
is named "\NewTerm{simple and inverse ratio}\index{simple and inverse ratio}".
\end{theorem}
\begin{dem}
	Given two collinear vectors $\vec{x}=(x_1,x_2),\vec{y}=(y_1,y_2)$ and therefore proportional to a given factor $\lambda$ such that:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
\end{dem}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
\textbf{R1. }If this ratio is not equal (thus: not inverse proportional), then we must switch to other tools such as simple and direct ratio, or regression techniques and verbatim: extrapolation.\\
\textbf{R2.} We also name "\NewTerm{simple or inverse joint rule}\index{simple or inverse joint rule}", a series of direct or inverse rule of three.
	\end{tcolorbox}	

	Basically, it is enough that we knew three of the four variables to solve this simple equation of the first degree.

	In such calculations, the agents of the exchange market have noticed that most of the time the ratio values were close to unity. They were thus naturally led to define the "percentage" as the proportion of a quantity or magnitude relative to another, measured with hundred (at least most of time...). Remember (\SeeChapter{see section Numbers page \pageref{percentage}}):

	\begin{itemize}
		\item Given a scalar $x \in \mathbb{R}$ then expressed in percentage it will denoted by:
			
		\item Given a scalar $x \in \mathbb{R}$ then expressed in per-thousand it will denoted by:
			
	\end{itemize}

	\subsection{Vector Spaces}
	\textbf{Definition (\#\mydef):} We name "\NewTerm{vector space}\index{vector space}" a set $E$ of elements designated by $\vec{x},\vec{y},...$ and named (as we know) "vectors", with a "vector algebraic structure" defined by the operation of vector addition (and thus vector subtraction) and scalar multiplication. These two operations satisfy the laws of associativity, commutativity, distributivity, neutral element and opposing element as we have already seen in the section of Set Theory.

	For more information about what a vector space set is exactly  the reader will have therefore to refer to the section of Set Theory where this concept is defined more strictly (it would be redundant to repeat it here an anyway it is not crucial because the properties are intuitive).

	For every positive integer $n$, $a_i$ means all the $n$-tuples of numbers arranged in a vector column:
	
	or as line vector (vector column that has been \textbf{T}ransposed):
	
	and $\mathbb{R}^n$ provides clearly a vector space structure. The vectors of this space will be named as we already know: "vectors". They are often denoted more briefly by:
	
	or even more briefly by:
	
	The number $a_i$ is sometimes name "\NewTerm{term}\index{vector term}" or "\NewTerm{component of index $i$}\index{vector component}" of $(a_i)$.

Now, unless stated otherwise, the vectors will always be the elements of a vector space $E$.

	\subsubsection{Linear Combinations}\label{linear combinations}
	\textbf{Definition (\#\mydef):} We name "\NewTerm{linear combination}\index{linear combination}\label{vector linear combination}" of vectors any vector relation of the form:
	
When a vector can be expressed in the above way we say that the vector is in "\NewTerm{component form}\index{vector component form}".

	The null vector $\vec{0}$ is a linear combination of the $\alpha_1\vec{x}_1+\alpha_2\vec{x}_2+...+\alpha_n\vec{x}_n$ with all coefficients equal to zero. We speak therefore of "\NewTerm{trivial linear combination}\index{vector trivial linear combination}".

	\textbf{Definition (\#\mydef):} We name "\NewTerm{convex combination}\index{convex combination}", any linear combination whose coefficients are non-negative and sum equal to 1. The set of convex combinations of two points $P$ and $Q$ of a punctual space $P_0$ (with an origin) is the line segment $P$ and $Q$. To realize this, we just write:
	
	and we make $\alpha$ vary from 0 to 1 and to find that all the points of the segment are thereby obtained.
	
	If the vector $\vec{v}$ is a linear combination of $\alpha_1\vec{x}_1+\alpha_2\vec{x}_2+...+\alpha_n\vec{x}_n$ and each of these vectors $\vec{x_i}$ is a linear combination of a set of independent vectors $\vec{y}_1,\vec{y}_2,...,\vec{y}_n$, then it could be obvious that $\vec{v}$ is also a linear combination of  $\vec{y}_1,\vec{y}_2,...,\vec{y}_n$.
	
	\textbf{Definition (\#\mydef):} A number $n$ of non-zero vector are "\NewTerm{coplanar}\index{coplanar}" if one of them is a linear combination of the others. For example, three vectors are coplanar if one of them is in the plane defined by the two others.
	
	\subsubsection{Sub-vector spaces}
	\textbf{Definition (\#\mydef):} We name "\NewTerm{vectorial subspace $V$ of $E$}\index{vectorial subspace}" any subset of $E$ that is itself a vector space for the operations of addition and scalar multiplication defined in $E$.
	
	A vectorial sub-space $V$, as a vectorial space, can not be empty as it includes at least one vector, i.e. its zero vector, this being also necessarily also the zero vector of $E$. In addition, together with the vectores $\vec{x}$ and $\vec{y}$ (if it contains other vectors than the zero vector), it also includes all their linear combinations $\alpha\vec{x}+\beta\vec{y}$.
	
	Conversely, as soon as we see any subset having these properties is a vectorial subspace. We have thus established the following proposition:
	
	A subset $V$ of $E$ is a subspace of $E$ if and only if $V$ is not empty and $\alpha\vec{x}+\beta\vec{y}$ belongs to $V$ for every pair $(\vec{x},\vec{y})$ of $V$ and all any pair $(\alpha,\beta) \in \mathbb{R}$.
	
	\subsubsection{Generating families}
	It follows that if we have a family of vectors $(\vec{x}_1,\vec{x}_2,...,\vec{x}_k)$ the set of linear combinations of $\vec{x}_1,\vec{x}_2,...,\vec{x}_k$ with $k<n$ can be a subspace $S$ of $E$, more specifically the smallest subspace of $E$ consisting of $\vec{x}_1,\vec{x}_2,...,\vec{x}_k$.
	
	The $\vec{x}_1,\vec{x}_2,...,\vec{x}_k$ vectors that satisfy the above condition are named "\NewTerm{generators}\index{generators of a family of vectors}" of $S$ and the family $(\vec{x}_1,\vec{x}_2,...,\vec{x}_k)$ the "\NewTerm{generating family}\index{generating family}" of $S$. We also say that these vectors or family "generate $S$".
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
The subspace generated by a nonzero vector consists of all multiples of this vector. We name such a subspace a "\NewTerm{vector line}\index{vector line}". A subspace generated by two vector non multiple of each other is named a "\NewTerm{vector map}\index{vector map}" or "\NewTerm{vector plane}\index{vector plane}".
	\end{tcolorbox}
	
	\subsubsection{Linear Dependence or Independence}
	What follows is very important in physics: we advise future physicists or engineer really take the time to read the developments below.
	
	If $(\vec{e}_1,\vec{e}_2,\vec{e}_3)$ are three vectors of $e^3$ whose representatives are not parallel to the same plane (by convention a zero-vector is parallel to any plane), so any vector $\vec{x}$ of $E^3$ can be written by the linear combination:
	
	where $\alpha_1,\alpha_2,\alpha_3$ are typically in $\mathbb{R}$.
	For example the above vector $\vec{x}$ (but can also be obtained for different values of $\alpha_i$!):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/algebra/vector_linear_combination.jpg}
		\caption{Example of a construction of a vector in a three-dimensional space}
	\end{figure}
	In particular, the only possibility to get the zero vector $\vec{0}$ as a linear combination of $(\vec{e}_1,\vec{e}_2,\vec{e}_3)$ is to assign the trivial value $0$ to the coefficients $\alpha_1,\alpha_2,\alpha_3$.

	Conversely, if for three vectors  $\vec{e}_1,\vec{e}_2,\vec{e}_3$ of $E^3$ the relation:
		
	implies $\alpha_1=\alpha_2=\alpha_3=0$, any vectors may be linear combination of the other two, in other words, their representatives are not parallel to the same plane.
	
	Based on these observations, we will extend the notion of absence of parallelism to a same plane in the case of any number of vectors of a given vector space $E$.
	
	We say that the vectors $\vec{x}_1,\vec{x}_2,...,\vec{x}_k$ are "\NewTerm{linearly independent}\index{linearly independent vectors}" if the relation:
	
	necessarily implies  $\alpha_1=\alpha_2=...=\alpha_k=0$, in other words, if the trivial linear combination is the only linear combination of $\vec{x}_1,\vec{x}_2,...,\vec{x}_k$ which is zero. Otherwise, we say that the vectors $\vec{x}_1,\vec{x}_2,...,\vec{x}_k$ are "\NewTerm{linearly dependent}\index{linearly dependent vectors}".
	
	If the intention is fixed on the family $(\vec{x}_1,\vec{x}_2,...,\vec{x}_k)$ rather than the terms of which it is made, we say that the latter is a "\NewTerm{free family}\index{free family (vector calculus)}" or "\NewTerm{linked family}\index{linked family (vector calculus)}" following that the vectors are linearly independent or dependent.
	
	\subsubsection{Base of a vectorial space}
	\textbf{Definition (\#\mydef):} We say that a family of finite vectors is a basis of $E$\index{vector basis}\label{vector basis} if and only if:
	\begin{enumerate}
		\item If it is free.
		
		\item It generates $E$.
	\end{enumerate}
	Following this definition, every free family $\vec{x}_1,\vec{x}_2,...,\vec{x}_k$ is a basis of the subspace it generates.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	If we consider $\mathbb{C}$ as a $\mathbb{R}$-vector space (\SeeChapter{see section Set Theory page \pageref{vector space}}), then since all the elements of $\mathbb{C}$ are written $a+\mathrm{i}b$, the elements that generate $\mathbb{C}$ are $1$ and $\mathrm{i}$ (both are free).\\
	
	A base of $\mathbb{C}$ (which is 2-dimensional) as a $\mathbb{R}$-vector space is therefore the free finite set $\left\lbrace 1, i \right\rbrace$.
	\end{tcolorbox}
	For a family of vectors $(\vec{e}_1,\vec{e}_2,...,\vec{e}_n)$ to be a basis of $E$, then it is necessary and sufficient that every vector $\vec{x}$ of $E$ is expressed uniquely as a linear combination of the vectors $(\vec{e}_1,\vec{e}_2,...,\vec{e}_n)$:
	
	The above relation is decomposition of $\vec{x}$ following the base $(\vec{e}_1,\vec{e}_2,...,\vec{e}_n)$ where the coefficients $x_1,x_2,...,x_n$ are the components of $\vec{x}$ in this base. In the presence of a base, each vector is determined entirely by its components.
	
	Proposition:
	
	If $x_1,x_2,...,x_n$ are the components of $\vec{x}$ and $y_1,y_2,...,y_n$ those of equation then: 
	
	are the components of $\vec{x}+\vec{y}$.
	
	In other words, add two vectors is equivalent to add their components and multiply a vector by a scalar obviously equivalent to multiplying its components by the same scalar. The basis is an important tool because it allows you to perform operations on vectors through operations on numbers.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The following column vectors of $\mathbb{R}^n$:
	
	generate a base that we name "\NewTerm{canonical basis}\index{canonical basis}\label{canonical basis}" of $\mathbb{R}^n$ (we will work in complex spaces in another section of this book).
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	As part of the three-dimensional space, bases are very often treated as a triad (actually if you connect the ends of the three vectors by features you will get an imaginary triad).
	\end{tcolorbox}	
	
	\pagebreak
	\subsubsection{Direction Angles}
	It is clear that only one standard angle cannot describe the direction of a vector in space. We then use the concept of "\NewTerm{direction angles}\index{direction angles}". This is to measure the angle of the vector $\vec{U}$ with respect to each of the positive axis of the base:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/direction_angles.jpg}
		\caption{Representation of direction angles}
	\end{figure}
	if:
	
	Then by definition:
	
	The values:
	
	are named the "\NewTerm{cosines directions}\index{cosines directions}\label{cosines directions}" of $\vec{x}$.
	
	The three angles mentioned are not completely independent. Indeed, two are enough to completely determine the direction of a vector in space, the third can be deduced from the following equality (obtained from the calculation of the sum of squares of previous relations):
	
	Therefore the direction cosines are the scalar components of a unit standard vector  $\vec{u}$ having the same direction as $\vec{U}$:
	
	
	\subsubsection{Dimensions of a vector space}
	We say that a basis $E$ is of "\NewTerm{finite size}\index{finite size basis}" if it is generated by a finite family of vectors. Otherwise, we say that $E$ is of "\NewTerm{infinite dimension}\index{infinite dimension basis}" (we'll discuss this type of spaces in another section). Any finite dimensional vector space and not reduced to the zero vector has a basis. In fact, from any generating family of such a vector space we can extract a basis.
	
	The dimension of a vector space is denoted by:
	
	Any vector space $E$ of nonzero finite dimension $n$ can be mapped in one to one correspondence (that is to say in bijection) with $\mathbb{R}^n$. We just need to choose a basis of $E$ and to match to any vector $\vec{x}$ of $E$ the column vector whose terms are the components of $\vec{x}$ in the chosen basis (this is  mathematician blah blah but it will be useful when we will discuss more complex spaces):
	
	This correspondence preserves the operations of addition and multiplication by a scalar as we have already seen; in other words, it can perform operations on vectors by operations on numbers.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
For "classic" resolution methods of such systems, we refer the readers to the section on Numerical Methods of the chapter on Computing Science.
	\end{tcolorbox}	
	Then we say that $E$ and $\mathbb{R}^n$ are "\NewTerm{isomorphic}\index{isomorphic basis}" or that the correspondence is an isomorphism (\SeeChapter{see section Set Theory page \pageref{isomorphism}}).
	
	\subsubsection{Extension of a free family}
	\begin{theorem}
	Given $(\vec{x}_1,\vec{x}_2,...,\vec{x}_k)$ a free family and $(\vec{v}_1,\vec{v}_2,...,\vec{v}_m)$ a generating family of $E$. If $(\vec{x}_1,\vec{x}_2,...,\vec{x}_k)$ is not a basis of $E$, we can extract a subfamily $$(\vec{v}_{i1},\vec{v}_{i2},...,\vec{v}_{il})$$ of $(\vec{v}_1,\vec{v}_2,...,\vec{v}_m)$ so that the family $(\vec{x}_1,\vec{x}_2,...,\vec{x}_k,\vec{v}_{i1},\vec{v}_{i2},...,\vec{v}_{il})$ is a basis of $E$.
	\end{theorem}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Such a theorem is useful when going from a mathematical space passage having given properties to another space with different mathematical properties.
	\end{tcolorbox}	
	\begin{dem}
	We assume that at least one of the vectors $\vec{v}_i$ is not a linear combination of vectors $(\vec{x}_1,\vec{x}_2,...,\vec{x}_k)$, otherwise $(\vec{x}_1,\vec{x}_2,...,\vec{x}_k)$ would generate $E$ and would therefore be a possible basis of $E$. Let us note that vector $\vec{v}_{i1}$. The family $(\vec{x}_1,\vec{x}_2,...,\vec{x}_k,\vec{v}_{i1})$ is then a free family. Indeed, the relation:
	
	then implies first that $\beta_1=0$, otherwise $\vec{v}_{i1}$ would be a linear combination of the vectors $\vec{x}_1,\vec{x}_2,...,\vec{x}_k$, and then all $\alpha_i=0$ since the vectors $\vec{x}_1,\vec{x}_2,...,\vec{x}_k$ are linearly independent.
	
	If the family $(\vec{x}_1,\vec{x}_2,...,\vec{x}_k,\vec{v}_{i1})$ generates $E$, it is then a possible base for $E$ and the theorem is proved. Otherwise, the same reasoning ensures the existence of another vector $\vec{v}_{i2}$ .... If the new resulting family is not a basis of $E$, then the extraction process vectors $\vec{v}_i$ of $(\vec{v}_1,\vec{v}_2,...,\vec{v}_m)$ continues. When it stops, we will get an "extension" of $(\vec{x}_1,\vec{x}_2,...,\vec{x}_k)$ in a free family generating $E$, that is to say a base of $E$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	It returns a corollary: 

	Every finite dimensional vector space and not reduced to zero vector has a basis! In fact, from any generating family of such a space, we can extract a base.
	
	\subsubsection{Rank of a finite family}
	\textbf{Definition (\#\mydef):} We name "\NewTerm{rank of a family of vectors}\index{rank of a family of vectors}" and denote by $\text{rk}(S)$ the dimension of the subspace $S$ of $E$ it creates.
	
	\begin{theorem}
	The rank of a family of vector $(\vec{x}_1,\vec{x}_2,...,\vec{x}_k)$ is less than or equal to $k$ and is equal to $k$ if and only if the family is free.
	\end{theorem}
	\begin{dem}
	Let us set aside the trivial first case where the rank of the family $(\vec{x}_1,\vec{x}_2,...,\vec{x}_k)$ is zero. By the previous corollary, then we can extract from this family a base of the subspace it generates. The rank is than less or equal to $k$ following that $(\vec{x}_1,\vec{x}_2,...,\vec{x}_k)$ is a linked family or not.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\pagebreak
	\subsubsection{Direct Sums}
	\textbf{Definition (\#\mydef):} We say that the sum $S + T$ of two subspaces $S$ and $T$ of $E$ is a "\NewTerm{direct sum}\index{direct sum of subspaces}\label{direct sum}" if (special case applied to a 2 dimensional space!):
	
	In this case, we note it:
	
	In other words, the sum of two vector subspaces $S$ and $T$ of $E$ is direct if the decomposition of all element $S + T$ into a sum of an element of $S$ and of $T$  is unique.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	For example, the $XY$-plane, a two-dimensional vector space, can be thought of as the direct sum of two one-dimensional vector spaces, namely the $X$ and $Y$ axes. In this direct sum, the $x$ and $y$ axes intersect only at the origin (the zero vector). Addition is defined coordinate-wise, that is:
	 
	which is the same as vector addition.
	\end{tcolorbox}	
	This concept of trivial decomposition will be very useful in some theorems, the most important in this book is definitely the spectral theorem (\SeeChapter{see section of Linear Algebra page \pageref{spectral theorem}}) that has important implications in statistics!!!
	
	From the direct sum we can introduce the concept of "\NewTerm{complementary subspace}\index{complementary subspace}" also named "\NewTerm{subspace}\index{subspace}" (depending on countries ...):
	\begin{theorem}
	Suppose that $E$ is of finite dimensions. For any subspace $S$ of $E$, there exists a subspace $T$ (not unique) of $E$ such that $E$ is the direct sum of $S$ and $T$. We say then that $T$ is a "\NewTerm{supplementary subspace}\index{supplementary subspace}" of $S$ into $E$.
	\end{theorem}
	\begin{dem}
	First let us set aside the trivial case where $S=\left\lbrace \vec{0} \right\rbrace$  and $S = E$. The subspace $S$ admits a basis $(\vec{e}_1,\vec{e}_2,...,\vec{e}_k)$, where $k$ is less than the dimension $n$ of $E$. By the theorem of extension of a free family, this basis can be extended in a basis $(\vec{e}_1,\vec{e}_2,...,\vec{e}_k,\vec{e}_{k+1},...,\vec{e}_n)$ of $E$. Let $T$ be the subspace vector generated by the family $(\vec{e}_{k+1},...,\vec{e}_n)$ . If $\vec{x}$ is any vector of $E$, then $\vec{x}=\vec{s}+\vec{t}$, where $\vec{s}$ is a vector of $S$ and $\vec{t}$ a vector of $T$. In addition $S\cap T=\left\lbrace \vec{0} \right\rbrace$, because no vector, excepted the zero vector may be a linear combination of the vectors $\vec{e}_1,...,\vec{e}_k$ and of the vectors $\vec{e}_{k+1},...,\vec{e}_n$. We therefore conclude that:
	 
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\subsubsection{Affine spaces}\label{affine space}
	In mathematics, an affine space $G=\text{A}\mathbb{R}^n$ is a geometric structure that are independent of the concepts of distance and measure of angles, keeping only the properties related to parallelism and ratio of lengths for parallel line segments as their is not origin point $\text{=}(0,0)$.
	
	The space $G$ of elementary geometry is both common and the source of the concept of "affine space" that we will introduce because when high-school student begins learn geometry they learn it without any reference point $\text{=}(0,0)$.

	In an affine space, there is therefore no distinguished point that serves as an origin. Hence, no vector has a fixed origin and no vector can be uniquely associated to a point. In an affine space, there are instead "\NewTerm{displacement vectors}\index{displacement vectors}", also named "\NewTerm{translation vectors}\index{translation vectors}\label{translation vector}" or simply translations, between two points of the space.
	
	This space $G$ is associated with the "\NewTerm{geometric vector space}\index{geometric vector space}" $V$ by the correspondence between vectors and arrows studied so far! The following definition is only to highlight the main common points of this correspondence:
	
	\textbf{Definition (\#\mydef):} Let $G$ be a non-empty set of elements that we name "\NewTerm{points}\index{points in a vector space}" and let us  denote them by the letters $P, Q, ...$; given also $E$ a vector space. Suppose that to any two points $(P, Q)$ corresponds a vector denoted $\overrightarrow{PQ}$ (typically the point $P$ is chosen as fictive origin). We say then that $U$ is an "\NewTerm{affine space}\index{affine space}" of directed space $E$ if the following conditions are met:
	\begin{enumerate}
		\item[C1.] For any fixed point $P$, the correspondence between pairs $(P, Q)$ and vectors $\vec{x}$ defined by only one point plus the origina point is bijective, ie, for every vector $\vec{x}$ it exists a point $Q$ such that we can define a vector $\overrightarrow{PQ}$.
		
		\item[C2.] For each triple of points $(P, Q, R)$:
		
		This is the famous "\NewTerm{Chasles relation}\index{Chasles relation}" (which we will see later a pseudo-equivalent in the section of Differential and Integral Calculus).
		
		\item[C3.] If $P$ is a point and a $\vec{x}$ vector, to express that $Q$ is the unique point such as $\vec{x}=\overrightarrow{•}{PQ}$, we write:
		
		Although being a bit excessive, this writing is consistent with the usage and suggests well the idea of the operation it designates.
	\end{enumerate}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Below an "artistic" example of an affine space $G=\text{A}\mathbb{R}^2$ where there is no origin and any extremity of a line can be considered as the origin of a vector:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/affine_space.jpg}
		\caption{Artistic but real example of an $G=\text{A}\mathbb{R}^2$ affine space}
	\end{figure}
	\end{tcolorbox}
	
	The following properties follow directly from the definition of affine space:
	\begin{enumerate}
		\item[P1.] For any point $P$, $P+(\vec{x}+\vec{y})=(P+\vec{x})+\vec{y}$
		
		\item[P2.] For any point $P$, $\overrightarrow{PP}=\vec{0}$. This results from the $\overrightarrow{PQ}+\overrightarrow{QR}=\overrightarrow{PR}$ provided in the case where we have $P=Q=R$.
		
		\item[P3.] $\overrightarrow{PQ}=-\overrightarrow{QP}$. Just put $R = P$ in the De Chasles relation $\overrightarrow{PQ}+\overrightarrow{QR}=\overrightarrow{PR}$.
		
		\item[P4.] Parallelogram rule:
		Given the polygon with the vertices (clockwise) $P,P',Q,Q'$ and edges $\overrightarrow{PP'},\overrightarrow{P'Q'},\overrightarrow{QQ'},\overrightarrow{PQ}$:
		\begin{figure}[H]
			\centering
			\includegraphics{img/algebra/affine_parallelogram.jpg}
			\caption{Vector polygon in $\text{A}\mathbb{R}^2$}
		\end{figure}
		We have:
		
		if and only if:
		
		which would then give a parallelogram!
	
		Indeed, replacing $R$ with $Q'$ in the Chasles relation we have:
		
		and by doing the same but replacing $R$ with $Q$ and $Q$ by $P'$ we get:
		
		We then have by equalizing the last two relations:
		
	\end{enumerate}
	Earlier we saw what made that a space $G$ could be provided with a vector space structure (we saw then that it was therefore "vectorialized"). In the general case of an affine space $G$, the process is the same:
	
	We choose any point $\text{O}$ of $G$. The correspondence between pairs $(\text{O},P)$ and vectors of director space $E$ being therefore biunivocal we define then the addition of points and multiplication of a point by a scalar by the corresponding operations on the vectors of $E$. Armed with these two operations, $G$ becomes a vector space, named "\NewTerm{vectorialized space $G$ regarding to $Q$}\index{vectorialized space}". We denote this space by $V$ and named the point $\text{O}$ "\NewTerm{origin}\index{origin of a vector space}".
	
	Given how operations have been defined, it follows that $V$ is isomorphic to the space vector $E$:
	
	However, this isomorphism depends on the choice of the origin $\text{O}$ and in practice this origin is selected on the basis of the data inherent to the studied problem. For example, if an affine transformation allows an invariant point (which does not move), it is advantageous to select that point as the origin.
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} When we talk about dimension of an affine space, we talk about the size of its director space.\\
	
	\textbf{R2.} The space $G$ of elementary geometry is an affine space of type $\text{A}\mathbb{R}^2$. Indeed, its direction is the geometrical space $G$ and the conditions of definition of affine space are met.\\
	
	\textbf{R3.} An affine space is a set of elements with a difference function. This difference is a binary function, which takes two points $p$ and $q$ (both in $G$) and yields an element (a vector) $\vec{v}$ of a vector space $E$. We write $\vec{v}=p-q.$ Additionally, this difference function must ensure that, for any point $p$ in $E$, it holds$p-p=0$, where $\vec{0}$ is the null vector of $E$.
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Euclidean Vector Spaces}
	Before defining what is an Euclidean vector space, let us first define some mathematical tools and some concepts.
	
	We can, by choosing a unit length, measure the intensity of each arrow, in other words, determine its length. We can also measure the angular distance of two arrows (or vectors) of any common origin (not necessarily distinct) taking as the unit of angle measurement for example the radian (\SeeChapter{see section Trigonometry page \pageref{radian}}). The measurement of this difference is then a number between $0$ and $\pi$ named "\NewTerm{angle}\index{angle between two vector}" of the two arrows (see the section of Euclidean Geometry for more details). If both arrows have same direction and orientation, their angle is zero and if they have same direction and the opposite orientation, this same angle is $\pi$.
	
	The representing arrows of a same vector $\vec{x}$ all have the same length. We denote this length (distance), named also "\NewTerm{norm}\index{norm}\label{vector norm}" or "\NewTerm{module}\index{module}", by the notation:
	
	given in the three-dimension case explicitly by the following Euclidean distance\label{euclidean distance vector} if the origin of the vector coincides with the origin O of the vector basis:
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/details_norm_calculation.jpg}
		\caption{Details of the calculation of the norm in an orthogonal coordinate system $\mathbb{R}^3$}
	\end{figure}
	If $\vec{x}$ is a nonzero vector we can build an unit norm vector $\vec{u}$ of same direction and orientation (colinear) by the following operation that is used a lot in physics:
	
	We will name "\NewTerm{non-zero angle of vectors $\vec{x}$ and $\vec{y}$}" the angle of two arrows of common origin representing one being $\vec{x}$ and the other $\vec{y}$.
	
	However, more strictly speaking a "norm" is defined on a real vector space (or complex) $E$, so that we speak then of "\NewTerm{normalized vector space}\index{normalized vector space}" is an application:
	
	satisfying the following properties:
	\begin{enumerate}
		\item[P1.] Positivity:
		
		\item[P2.] Linearity:
		
		where we take the modulus of the constant if this is not in the set of real numbers $\mathbb{R}$ but in the set of complex numbers $\mathbb{C}$.
		\item[P3.] Nullity (often associated with the property P1):
		
		\item[P4.] Minkowski inequality (triangle inequality):
		
		That we will prove further below.
	\end{enumerate}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} These properties are mainly imposed by our intuitive approach of Euclidean space (vector space of finite dimension over the field of real number $\mathbb{R}$ and with a scalar product that we will see later) and its geometric interpretation (through the fact that it is also an affine space $\text{A}\mathbb{R}^n$).\\
	
	\textbf{R2.} We will prove a little further below the property P4 under the name of "triangle inequality" and we will do a little more general study of this inequality under the name "Minkowski inequality" in the section Topology.
	\end{tcolorbox}	
	
	\pagebreak	
	\subsubsection{Scalar Product (Dot Product)}\label{dot product}
	\textbf{Definition (\#\mydef):} An "\NewTerm{Euclidean vector space}\index{Euclidean vector space}" of dimension $n$ is a vector space (real and of finite dimensional for the purists) with a specific operation, the "\NewTerm{scalar product}\index{scalar product}" also named "\NewTerm{dot product}\index{dot product}" which we denote (notation specific to this website) regarding to the special case of vectors:
	
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} We find in some books (for information) the notation $\left( \vec{x}|\vec{y}\right)$ or $\langle \vec{x} | \vec{y} \rangle$ in the generalization of this definition as we shall see a little further below. According to the standard ISO 80000-2:2009 we should write the dot product as $a\cdot b$. Using the Linear Algebra notation that we will see later, the dot product will be written: $\vec{x}^T\vec{y}$.\\
	
	\textbf{R2.} The scalar product has a huge importance in the whole field of mathematics and physics; and we will see it again in all following chapters of this book. It is therefore necessary to carefully understand what follows.\\
	
	\textbf{R3.} The scalar product may be viewed as a projection of the length of a vector along the length of another one as we will see later.
	\end{tcolorbox}	
	This scalar product has the following properties (most of which stem from the definition itself) in a Euclidean space:
	\begin{enumerate}
		\item[P1.] Commutativity: $\vec{x}\circ\vec{y}=\vec{y}\circ\vec{x}$
		\item[P2.] Associativity: $\alpha(\vec{x}\circ\vec{y})=(\alpha\vec{x})\circ\vec{y}=\vec{x}\circ(\alpha\vec{y}) $
		\item[P3.] Distributivity: $\vec{x}\circ(\vec{y}+\vec{z})=\vec{x}\circ\vec{y}+\vec{x}\circ\vec{z}$
		\item[P4.] Non-degerated: $\vec{x}\circ\vec{y}=0$ then $\forall\vec{x}\neq\vec{0}\Rightarrow \vec{y}=\vec{0}$
		\item[P5.] Squared scalar: $\|\vec{x}\|^2=\vec{x}\circ\vec{x}$ and $\vec{x}\circ\vec{x}>0$ if $\vec{x}\neq \vec{0}$
		\item[P6.] Bi-linearity: $(\alpha\vec{x}+\beta\vec{y})\circ\vec{z}=\alpha(\vec{x}\circ	\vec{z})+\beta(\vec{y}\circ\vec{z})$
	\end{enumerate}
	Only the latter property requires perhaps a proof (and one of the results of the proof  will be useful to us later to prove another very important property of the scalar product):
	\begin{dem}
	Given:
	
	which is the "\NewTerm{orthogonal projection vector}\index{orthogonal projection vector}\label{orthogonal projection vector}" (the $x$ on index of $\text{project}$ meaning "the vector $\vec{x}$") of the vector $\vec{y}$ of standardization at the unit of vector $\vec{x}$.
	
	Using the scalar product, the vector $\text{proj}_x\vec{y}$ can be expressed otherwise, we just need to take the relation that we have seen above:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/dot_product.jpg}
		\caption{Geometrical representation of the dot product (projection)}
	\end{figure}
	
	and introduce it into $\text{proj}_x\vec{y}$ with to obtain:
	
	The norm of $\text{proj}_x\vec{y}$ is written:
	
	If $\vec{x}$ has a unit norm, the relations of previous projections are simplified and become obviously:
	
	By elementary geometric considerations (distributivity of the scalar product), it is easy to realize that:
	
	If we now return to the proof of the bi-linearity property:
	
	We have in a first time:
	
	and, from the definition of the property of the orthogonal projection, it comes immediately by a one-to-one correspondence:
	
	hence the property $P6$ that follows by multiplying the two members of equality by $\vec{z}\circ\vec{z}$ and after by simplification by $\vec{z}$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{enumerate}
		\item[D1.] A vector space $E$ is said to be an "\NewTerm{proper Euclidean vector space}\index{proper Euclidean vector space}" if $\forall \vec{x} \in E \qquad \|\vec{x}\|>0$.
		
		\item[D2.] We say that the vectors $\vec{x}$ and $\vec{y}$ are "\NewTerm{orthogonal vectors}\index{orthogonal vectors}" if they are non-null and that their scalar product is equal to zero (their angle is equal to $\pi/2$).
		
		\item[D3.] A basis of vectors $(\vec{e}_1,\vec{e}_2,...,\vec{e_n})$ is said to be an "\NewTerm{orthonormal basis}\index{orthonormal basis}" if all the vectors $\vec{e}_1,\vec{e}_2,...,\vec{e_n}$ are pairwise orthogonal and their norm is equal to the unit (thus constituting a: free family).
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We will see in the section Tensor Calculus (we could have done here too but we don't use it for a practical case in this section therefore...) how from a set of independent vectors build an orthogonal basis. This is what the reader will find under the name  "(Gram-)Schmidt orthogonalization method".
	\end{tcolorbox}	
	By a simple geometric argument, we see that every vector is the sum of its orthogonal projections on the vectors of an orthonormal basis, that is, if $(\vec{e}_1,\vec{e}_2,\vec{e}_2)=(\vec{u},\vec{v},\vec{w})$ is an orthonormal basis in $\mathbb{R}^3$ for example:
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/orthonormal_basis_projection.jpg}
		\caption{Orthonormal basis projection example}
	\end{figure}
	This decomposition is also obtained by the P6 property of the scalar product. Indeed, consider the components $(\vec{x}_1,\vec{x_2},\vec{x}_3)$ of a vector $\vec{x}$ in our orthonormal basis:
	
	since $\vec{e}_1\circ\vec{e}_1=1$ and $\vec{e}_1\circ\vec{e}_2=0$. Therefore we get immediately:
	
	hence the decomposition.
	
	Given the respective components $(\vec{x}_1,\vec{x_2},\vec{x}_3)$ and $(\vec{y}_1,\vec{y_2},\vec{y}_3)$ of the vectors  $\vec{x}$ and $\vec{y}$ vectors in a canonical orthonormal basis $(\vec{e}_1,\vec{e_2},\vec{e}_3)$ we know now that we can write the scalar product in the form:
	
	by the property P6 of the scalar product:
	
	using the properties P1 and P6 again:
	
	Which finally gives us the very famous and important decomposition:
	
	This is one of the most important relation in the field of vector calculus, which we name "\NewTerm{canonical scalar product}\index{canonical scalar product}" or "\NewTerm{canonical dot product}\index{canonical dot product}".
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Then angle $\theta$ of the dot product is sometimes denoted by:
	
	\end{tcolorbox}
	Now let us prove with a simple two dimensional case a property that physicist like a lot to characterize an orthogonal linear application (\SeeChapter{see section Linear Algebra page \pageref{orthogonal matrix}}): that the dot product is invariant under any orthogonal transformation (then abusively said "invariant under basis change...").

	For this let us consider a vector $\vec{x}=(x_1,x_2)$ and the 2D rotation matrix (\SeeChapter{see section Numbers page \pageref{rotation matrix in the plane}}):
	
	Now let us calculate:
	
	So now let us consider two vectors $\vec{a}$ and $\vec{b}$ we have proved just above that their dot product is given by:
	
	And after the chosen orthogonal transformation we get:
	
	So the dot product is indeed invariant under this rotation that is a special 2D case of orthogonal transformation  and in fact under any other orthogonal transformation.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The dot product, must not  be confused with the "\NewTerm{element-wise multiplication}\index{element-wise multiplication}\label{element-wise multiplication}" of two vectors defined by:
	
	\end{tcolorbox}		
	
	\pagebreak
	\paragraph{Cauchy–Schwarz inequality}\mbox{}\\\\
	In mathematics, the Cauchy–Schwarz inequality is a useful inequality encountered in many different settings, such as linear algebra, analysis, probabilities, statistics and other areas (just read this book entirely to have an idea...). It is considered to be one of the most important inequalities in all of mathematics!!!
	
	The relation
	
	 can also be trivially written as follows if we use the concept of the norm and the definition of the scalar product:
	 
	 It is interesting to notice that if both $\vec{x}$ and $\vec{y}$ are orthogonal vectors, we fall back on the result of a famous theorem: the Pythagorean theorem!

	Indeed, therefore we have if the two vectors are orthogonal:
	 
		This gives us:
	 
	This relations is very important in physics and mathematics. It must be remembered!
	 
	\begin{theorem}
	We name "\NewTerm{Cauchy-Schwarz inequality}\index{Cauchy-Schwarz inequality}\label{cauchy-schwarz inequality}", the inequality, valid for any choice of the vectors $\vec{x}$ and $\vec{y}$, the relation:
	
	Which can also be written as:
	
	\end{theorem}
	First we will consider as obvious that equality only occurs when the two vectors are collinear.	
	\begin{dem}
	We put ourself in the case where $\vec{x},\vec{y}\neq\vec{0}$. So then $\lambda\in\mathrm{R}$ we have obviously according to the properties of the scalar product:
	
	So this is a simple equation of the second degree where variable is $\lambda$. Remembering what we saw in our study of polynomials of second degree (\SeeChapter{see section Calculus page \pageref{polynomial}}), the previous relation (that it is always greater than or equal to zero) is satisfied if the discriminant $b^2-4ac$ is negative or zero. In other words, if:
	
	Thus after simplification:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	When $E$ is in $\mathbb{R}^n$, the Cauchy-Schwarz inequality is written with the vector components:
	
	In the particular case where $\forall i,b_i=1$ it becomes:
	
	or even:
	
	which shows that the square of the arithmetic mean is less than or equal to the arithmetic mean of the squares. This result is important for the study of Statistics!
	
	Furthermore, using the property of the cosine and the Cauchy-Schwarz inequality we can write immediately:
	
	relation that we will see again in the context of the study of Statistics (\SeeChapter{see section Statistics page \pageref{coefficient of correlation}}).
	
	\paragraph{Triangle Inequalities}\mbox{}\\\\
	By majoring $2\vec{x}\circ\vec{y}$ by $2\|\vec{x}\|\cdot\|\vec{y}\|$ (using the Cauchy-Schwarz inequality!) in the relation already establish previously:
	
	we get:
	
	which take us immediately by taking the root square the "\NewTerm{triangle inequality}\index{triangle inequality}\label{triangle inequality}" that is very useful for the study of Sequences and Series and also in Topology:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The generalization of this inequality relatively to the choice of the norm (that is to say: the way we define a distance) as we will see in the section of Topology, gives what we name the "\NewTerm{Minkowski inequality}\index{Minkowski inequality}".
	\end{tcolorbox}	
	By applying one time the triangle inequality to the vectors $\vec{x}$ and $(\vec{y}-\vec{x})$ and another time to vectors $(\vec{y})$ and $(\vec{x}-\vec{y})$ we get the variant:
	
	
	\paragraph{General Scalar/Dot Product}\mbox{}\\\\
	Let us see now another and little more general, formal and abstract way to define the dot product while trying to stay as simple as possible (caution! in the general case the notation of the scalar product changes! ).
	
	First the reader must know that, as we will see it in the section of Functional Analysis, all the concepts studied until now in this section can also be applied to a special category of functions! Yeeesss!!! Add, subtract functions like vector is obvious but you must know that some more or less complicated functions are colinear or orthogonal (think to affine functions!) and furthermore there are not limits to $\mathbb{R}$ but can be extended to $\mathbb{C}$ easily and hence the scalar product departure set. 
	
	To make thinks to too much complicate we will focus here only on a gentle generalization of the dot product to vectors (we will come back on functions in the section of Functional Analysis later).
	
	\textbf{Definition (\#\mydef):} Let $E$ be a real vector space (once again we focus here only on simple vectors for the moment). A "\NewTerm{positive symmetric bilinear form}\index{positive symmetric bilinear form}" on $E$ also named "\NewTerm{inner product}\index{inner product}\label{inner product}", is an application:
	
	\begin{enumerate}
		\item[P1.] Positivity: 
		
	  	
	  	\item[P2.] Nullity (defined): 
	  	 
	  	
	  	\item[P3.] Symmetry (defined): 
	  	
	  	
	  	\item[P4.] The bilinearity (bilinear form) with, in order, the "\NewTerm{linearity on the left}\index{linearity on the left}" and "\NewTerm{linearity on the right}\index{linearity on the right}":
	  	
		
		\item[P...] And so on... we have the same six properties as the scalare product as the both are the same if we focus only on vector in $\mathbb{R}$.
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Again, these properties are mainly imposed by our intuitive approach of the Euclidean space and its geometric interpretation.
	\end{tcolorbox}	
	
	\textbf{Definition (\#\mydef):} A space $E$ provided with a scalar product is named in general (with the departure set in $\mathbb{C}$) a "\NewTerm{pre-Hilbert space}\index{pre-Hilbert space}" or "\NewTerm{inner product space}\index{inner product space}". If $E$ is of finite dimension, then we speak of "Euclidean space".
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	If $E$ is a Euclidean space, then if the determinant $\det(\vec{e}_1,\vec{e}_2,\ldots,\vec{e}_n)$ is equal $\pm 1$ (\SeeChapter{see section Linear Algebra page \pageref{determinant}}) we speak of "\NewTerm{oriented Euclidean space}\index{oriented Euclidean space}\label{oriented Euclidean space}".
	\end{tcolorbox}	
	
	We will see in our study of Topology (see section of the same name page \pageref{topology}) that the properties of the scalar product are the foundation bricks to set a norm and therefore a distance in a metric space. This distance will be given according to what we will see in the section of Topology:
	
	
	\textbf{Definition (\#\mydef):} We say that a space $E$ having a dot product (inner product) $ \langle \cdot | \cdot \rangle$ is a "\NewTerm{Hilbert space}\index{Hilbert space}" if this space is complete for the metric defined above.
	
	In other words, having a metric space provided with a distance generated by a scalar product is one thing. Then having a measurable distance is another one!!! A Hilbert space has thus distances measurable in the topological sense because the set we are working on is continuous and any point can be approached indefinitely (imagine having a rule and you can not approach ont this rule the points that define the dimensions of your object... it would be embarrassing...). So without complete space a lot of theorems of functional analysis (that is strongly linked to vector calculus) could not be used in the study of vector spaces and this would be very embarrassing in quantum wave physics for example...
	
	Formally, remember that a metric space is complete if all Cauchy sequences (\SeeChapter{see section Sequences and Series page \pageref{cauchy sequence}}) of this space are converging (\SeeChapter{see section Fractals page \pageref{complete space cauchy sequence}}) in a metric space (\SeeChapter{see section Topology page \pageref{metric space}}).
	
	\subsubsection{Cross Product}
	The cross product of two vectors is a proper operation to the dimension $3$. To introduce it, it is first necessary to orient the space intended to receive it. The orientation is defined by the concept of "determinant", therefore we will begin with a brief introduction to the study of this concept. This study will be repeated later in more details in the analysis of linear systems in the section of Linear Algebra.
	
	\textbf{Definition (\#\mydef):} We name basically "\NewTerm{determinant}\index{determinant}" of two column vectors of $\mathbb{R}^2$ (for the general form of the determinant see the section of Linear Algebra page \pageref{determinant}):
	
	and we denote it:
	
	the number:
	
	We name determinant of three column vectors of $\mathbb{R}^2$ (once again see the section Linear Algebra for a generalization):
	
	and we denote it:
	
	the number:
	
	Thus, the function that associates to each pair of column vectors of $\mathbb{R}^2$ (or respectively to each triplet of column vectors of $\mathbb{R}^3$) has a determinant named "determinant of order $2$" (respectively "determinant of order $3$")
	
	As we will prove it in the section of Linear Algebra the determining has the property of being multiplied by $-1$ if one of the column vectors is replaced by its opposite or two of its column vectors are exchanged. In addition, the determinant is nonzero if and only if its column vectors are linearly independent (the proof - that has a great importantce in Applied Mathematics - is a few lines further below and a generalization  can be found in the section of Linear Algebra).
	
	\textbf{Definition (\#\mydef):} Given $x_1,x_2,x_3$ and $y_1,y_2,y_3$the respective components of the vectors $\vec{x}$ and  $\vec{y}$ in the orthonormal basis $(\vec{e}_1,\vec{e}_2,\vec{e}_3)$. We name "\NewTerm{cross-product}\index{vector cross-product}" of $\vec{x}$ and $\vec{y}$, and we denote it in most books by:
	
	and in a minority of books:
	
	the vector:
	
	or as components\label{cross product matrix form}:
	
	The matrix form above will be very useful to us in the section Mechanics for the construction of the Inertial Matrix.
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} The first notation is the international notation due to Gibbs (which we will use throughout this book), the second is the French notation due to Burali-Forti (quite annoying because confusing with the notation of the operator AND in Proof Theory or Logical Systems).\\
	
	\textbf{R2.} It is usually quite annoying to remember by heart the relations that form the cross product. But fortunately there are at least three good mnemonics techniques:
	\begin{enumerate}

		\item The first and probably the fastest method is to remember by heart only one of the expressions of the components of the cross product and after by decrement of the indices (by starting again from $3$ when we reaches $0$) get all the other components. But we must still find a simple way to remember by heart one of the components... A good way is the following mathematical property of two collinear vectors giving an easy with to find back the third component (the one along the $z$-axis):
		
		Given two colinear vectors in the same plane, then:
		
		We fall back on the expression of the third component of the cross product of two vectors.\\
		
		Or if you want to remember only the first component given in letters by $z_x = x_y y_z - x_z y_y$ the indices gives "xyzzy" (like a name of a person...). The second and third equations can be obtained from the first by simply vertically rotating the subscripts, $x\rightarrow y \rightarrow  z \rightarrow x$. 
		
		\item The second method that we will see in details during our study of the section of Tensor Calculus is to use the Levi-Civita antisymmetric symbol. This method is certainly the most aesthetic of all but not necessarily the fastest to develop and the easiest to remember. We give here just the expression without explanations at the moment as we will study this later (but t is also useful to get the general expression of the determinant):
		
		
		\item The latter method is quite simple and trivial but it implicitly uses the first method as you must remember how the calculate at least a $2\times 2$ determinant. The idea is the following: the $i$-th component of $\vec{x}\times\vec{y}$ is the determinant of the two column vectors from which we have removed the $i$-th term, the second determinant is, however, with a "$-$" sign such that:
			  
	\end{enumerate}
	\end{tcolorbox}
	
	\pagebreak
	It is important, even if it is relatively simple to remember, that the different cross products for orthogonal basis vectors are (and especially for the canonical basis) first:
	
	and also:
	
	These relations can be condensed using the Dirac function symbol\label{orthogonal basis} (\SeeChapter{see section Tensor Calculus page \pageref{kronecker symbol}}):
	
	The vector product also has the following properties that we will prove just now:
	  \begin{enumerate}
	  	\item[P1.] Antisymmetry:
	  		
	  		
	  	\item[P2.] Linearity\label{cross product linearity}:
	  		
	  		
	  	\item[P3.] If and only if $\vec{x}$ and $\vec{y}$ are linearly independent (very important!):
	  	
	  	
	  	\item[P5.] Non associativity:
	  	
	  	
	  	\item[P4.] Distributivity over the sum:
	  	
	  \end{enumerate}
	  The first two properties directly derived from the definition and the property P4 is easily verified by developing the components and comparing the results.
	  
	 Then let us prove the third property which is very important in linear algebra (next section) and the fifth one (because requested by a reader).
	\begin{theorem}
	If and only if $\vec{x}$ and $\vec{y}$ are linearly independent (very important!):
	
	\end{theorem}
	\begin{dem}
	Given two vectors $\vec{x}(x_1,x_2,x_3)$ and $\vec{y}(y_1,y_2,y_3)$. If the two vectors are linearly dependent then there exists an $\alpha \in \mathbb{R}$ such that we can write:
	
	If we develop the cross product of two vectors that a dependent to a given factor, we get:
	
	It goes without saying that the above result is equal to the zero vector $\vec{0}$ if indeed the two vectors are linearly dependent.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\begin{theorem}
	The cross product is distributive over the sum.
	\end{theorem}
	\begin{dem}
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	If we now assume that both vector $\vec{x}$ and $\vec{y}$ are linearly independent and non-zero vector, we must prove that the cross product two properties:
	\begin{theorem}
	The resulting of a cross product results in a vector orthogonal (perpendicular) to $\vec{x}$ and $\vec{y}$ if they are not null.
	\end{theorem}
	\begin{dem}
	To prove this we simply write the development using the dot product:
	
	This equation shows that the vector $\vec{x}$ is perpendicular to the resulting vector of cross product between $\vec{x}$ and $\vec{y}$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{theorem}
	The cross product has for norm (module):
	
	where $\theta$ is the angle between $\vec{x}$ and $\vec{y}$.
	\end{theorem}
	\begin{dem}
		To prove this we simply write the development of the norm of the cross product:
		
		Finally:
		
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\label{cross product as surface parallelogram}We then notice that in the case where $E$ is the Euclidean vector space, the norm of the vector product is the area (surface) of the parallelogram constructed on representatives of vectors $\vec{x}$ and $\vec{y}$ of common origin:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/cross_product_parallelogram.jpg}
		\caption{Geometrical representation of the cross product}
	\end{figure}
	If $\vec{x}$ and $\vec{y}$ are linearly independent, the triplet $(\vec{x},\vec{y},\vec{x}\times \vec{y})$ and also the triplet $(\vec{x},\vec{y},\vec{y}\times \vec{x})$ are direct.
	
	Indeed, $(\vec{z}_1,\vec{z}_2,\vec{z}_3)$ being the components of $\vec{x}\times \vec{y}$ (in the basis $(\vec{e}_1,\vec{e}_2,\vec{e}_3)$), the determinant of passage of $(\vec{e}_1,\vec{e}_2,\vec{e}_3)$ to $(\vec{x}\times \vec{y},\vec{x},\vec{y})$ (form example) will be written:
	 
	 This determinant is positive, as at least one of the $z_i$ is not zero, according to the third property of linear independence of the cross product.
	 
	 Here are a few very important properties of practical utility of the cross product (particularly in the different section of physics in this book!) that are trivial to check whether the developments with explicit components are done (we can make them on request if needed!):
	\begin{enumerate}
		\item[P1.] $\vec{x}\times(\vec{y}\times\vec{z})=(\vec{x}\circ\vec{z})\vec{y}-(\vec{x}\circ\vec{y})\vec{z}$
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		The latter relation is sometimes named the "\NewTerm{Grassman rule}\index{Grassman rule}\label{grassman rule}", or more commonly "\NewTerm{dual vector product}\index{dual vector product}", or even by physicists "\NewTerm{triple cross product}\index{triple cross product}",  and it is important to note that without the parentheses the result is not unique!
		\end{tcolorbox}	
		Let us prove this identity! First the left part is equal to:
		
		And for the right part:
		
		and then we see that there is indeed equality!
		\item[P2.] $(\vec{x}\times\vec{y})\circ (\vec{z}\times\vec{v})=(\vec{x}\circ\vec{z})(\vec{y}\circ\vec{v})-(\vec{x}\circ\vec{v})(\vec{y}\circ\vec{z})$
		
		\item[P3.] $(\vec{x}\times\vec{y})\circ\vec{z}=-(\vec{x}\times\vec{z})\circ \vec{y}$
		
		\item[P4.] $(\vec{x}\times\vec{y})\circ\vec{z}=\vec{x}\circ(\vec{y}\times\vec{z})$
		
		\item[P5.] $\|\vec{x}\times\vec{y}\|^2=(\vec{x}\circ\vec{x})^2(\vec{y}\circ\vec{y})^2-(\vec{x}\circ\vec{y})^2$
	\end{enumerate}
	The last identity is related to the Pythagorean theorem (\SeeChapter{see section Euclidean Geometry page \pageref{pythagorean theorem}}). Indeed, we see it better by rewriting:
	
	This can be seen from the definitions of the cross product and dot product, as
	
	
	\pagebreak
	\subsubsection{Mixed Product (triple product)}
	We can extend the definition of the vector product to another type of mathematical tool we name the "\NewTerm{mixed product}".
	
	\textbf{Definition (\#\mydef):} We name "\NewTerm{mixed product}\index{mixed product}\label{mixed product}", also sometimes named  "\NewTerm{triple scalar product}\index{triple scalar product}", of vectors $\vec{x},\vec{y},\vec{z}$ the double product:
	 
	 often condensed under the following notation:
	  
	 From what we saw in the definition of the dot and cross product, mixed product can also be written:
	 
	 We note that in the case where $E$ is the Euclidean vector space $\mathbb{R}^3$, the absolute value of the mixed product symbolize the oriented volume of the parallelepiped, built on the representatives $\vec{x},\vec{y},\vec{z}$ of common origin.
	 
	It is quite trivial that the mixed product is an extension to the three-dimensional case of the cross product. Indeed, in the expression of the mixed product, the vector product is the base surface of the parallelepiped and the scalar product project the vectors on the resulting vector from the cross product which gives the height $h$ of the parallelepiped.
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/mixed_product.jpg}
		\caption{Mixed product illustration}		
	\end{figure}
			
	If we develop:
	
	so triple product can also be understood as the determinant of a $3\times 3$ matrix (thus also its inverse) having the three vectors either as its rows or its columns (\SeeChapter{see section Linear Algebra page \pageref{3x3 matrix determinant}}):
	
	 By the commutative properties of the scalar product, we have:
	 
	and the reader will check without any trouble (we can write the details on request) that by developing components we have:
	
	The triple product has also the following properties that the reader could be able to check easily by developing just the components of each expression excepted perhaps the third one (we can detailed as always on request if needed):
	\begin{enumerate}
		\item[P1.]  $[\vec{x},\vec{y},\vec{z}] =
	   [\vec{z},\vec{x},\vec{y}] =
	   [\vec{y},\vec{z},\vec{x}] =
	  -[\vec{y},\vec{x},\vec{z}] =
	  -[\vec{z},\vec{y},\vec{x}] =
	  -[\vec{x},\vec{z},\vec{y}]$
	  
	  \item[P2.] $[\alpha\vec{x}+\beta\vec{y},\vec{z},\vec{v}] =
	  \alpha[\vec{x},\vec{z},\vec{v}] +
	    \beta[\vec{y},\vec{z},\vec{v}]$
	    
	  \item[P3.] $[\vec{x},\vec{y},\vec{z}] \ne 0$ if and only if $\vec{x},\vec{y},\vec{z}$ are independent.
	  
	  \item[P4.] $(\vec{x}\times\vec{y})\times(\vec{z}\times\vec{v}) =
	  [\vec{x},\vec{y},\vec{v}] \cdot \vec{z} - [\vec{x},\vec{y},\vec{z}] \cdot \vec{v}$ 
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We will come back on the triple product during our study on tensor calculus as it gives the opportunity to get a very interesting result concerning a future application in General Relativity.	
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Vectorial Functional Space}
	Given $\mathcal{C}_{[a,b]}^k$ the set of real functions that can be $k$-times derivates (\SeeChapter{see section Differential and Integral Calculus page \pageref{smoothness}}) in the closed bounded interval $[a,b]$. We will designate the elements of this set by the letters $\vec{f},\vec{g},...$.
	
	The value of $\vec{f}$ at the point $t$ will be obviously denoted by $\vec{f}(t)$. Say that $\vec{f}=\vec{g}$ is therefore equivalent than to say that:
	 
	 In a condensed way some practitioners denote this $\vec{f}(t) \equiv \vec{g}(t)$, the symbol $\equiv$ indicating obviously that the two members are equals for any $t$ in the bounded interval $[a,b]$.
	 
	 Consider the two following operations:
	\begin{itemize}
		\item $\vec{f}(t)+\vec{g}(t)$ defined by the relation $(\vec{f}+\vec{g})(t)\equiv \vec{f}(t)+\vec{g}(t)$
		\item $\alpha\vec{f}$ defined by the relation $(\alpha \vec{f})(t)=\alpha\vec{f}(t)$
	\end{itemize}
	These two operations satisfy to all conditions of the vectors of a vector space as we have already defined at the beginning of this section (associativity, commutativity, null vector, opposed vector, distributivity, neutral element) and therefore gives us the possibility to assign to $\mathcal{C}_{[a,b]}^k$ of a vector space structure! Le null vector of this space being obviously the null function (equal to zero everywhere) and the opposite of $\vec{f}$ being $-\vec{f}$.

	It is interesting to notice that $\mathcal{C}_{[a,b]}^k$  as a vector space is a generalization of $\mathbb{R}^n$ to the continuous case. Indeed, we can consider any vector $\vec{v}=(a_i)$ of $\mathbb{R}^n$ in the form of a real function defined on the set $\left\lbrace 1,2,...,n \right\rbrace
$: the value of this function at the point $i$ is simply $a_i$.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The polynomials of order $n$ with one unknown form as an example of functional vector space of dimension $n + 1$ such that for each coefficient of the polynomial corresponds a vector component such that:
		
	\end{tcolorbox}
	The preferred application field of the abstract theory of the dot product (inner product) is formed by the functional vector spaces. We name therefore "\NewTerm{canonical scalar product}\index{canonical dot product}" in $\mathcal{C}_{[a,b]}(\mathbb{R}^2)$ the operation defined by the relation:
	
	This operation defines indeed a scalar product, the properties of the latter being verified (on reader request we can add the proof if necessary), and furthermore, the integral:
	
	is positive if the continuous function $\vec{f}$ is not identically zero.
	
	Technically the latter relation is written when in $\mathbb{R}^2$:
	
	We will give more precision about this norm and its associated scalar product and with example in the section of Functional Analysis.
	
	\pagebreak
	\subsection{Hermitian Vector Space}
	The purpose of what will follow is, as always in this book, not to give a detailed study about vector spaces in $\mathbb{C}$ but just to give the minimal knowledge and vocabulary necessary to the lecture of some theoretical models in physics and especially those presented in this book in the section of Wave Quantum Physics.
	
	When the scalars that appears in the definition of vector spaces are complex numbers (in $\mathbb{C}$ as seen in the section Numbers), and not only real numbers, then we speak obviously about "\NewTerm{complex vectorial spaces}\index{complex vectorial spaces}".
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Rigourlsy in the common communication, people should always precise if we speak of real vectorial space or complex vectorial space...
	\end{tcolorbox}
	Let us give some expamples of famous complex vectorial spaces (as many people think the are useless):

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. The space vector $\mathbb{C}^n$ of column-vectors with $n$ components ($\mathbb{C}^1$ being obviously identified to $\mathbb{C}$). We will meet, among others, such vector space in the section of Relativistic Quantum Physics.\\
	
	E2. The vectorial space of univariate polynomial with coefficient in $\mathbb{C}$. We will meet such spaces in the section of Wave Quantum Physics or even Quantum Chemistry.\\
	
	E3. The vectorial space of complex functions of one real or comple variables continuous or not. We will meet such vector spaces frequently in the section of Wave Mechanics and especially in the section of Electrodynamics.
	\end{tcolorbox}
	The purpose here is to adapt what we have seen so far to complex vectorial space. The following example, show us that we can transpose as it the previous definitions. Indeed, let us consider the vector space $\mathbb{C}^n$. As for $\mathbb{R}^n$, we could have the tentation to define a dot product on $\mathbb{C}^n$ by:
	
	with $x_i,y_i\in \mathbb{C}$.
	Sadly, we see that this definition is not satisfactory because we could have therefore:
	
	and this quantity is in general not a real number in a complex vector space and violates the property of positivity of the dot product and therefore prevent us to introduce and use the concept of distance. What is obviously a big problem in our actual perception of the world.
	
	We could therefore not define anymore a norm in $\mathbb{C}^n$ by writing:
	
	For $\langle \vec{x}|\vec{x}\rangle$ to be a positive real number we see that it would be better to define the scalar product like this:
	
	In this case we have therefore:
	
	which is well a positive real number. From there, we can once again define a norm for complex vector space $\mathbb{C}^n$ by putting:
	
	We will now show how to define an inner product on a complex vector space in the general case.
	
	\subsubsection{Hermitian Inner Product}\label{hermitian inner product}
	\textbf{Definition (\#\mydef):} Let $\mathcal{H}$ be a complex vector space (!). We name "\NewTerm{scalar product}\index{scalar product}" or more accurately "\NewTerm{Hermitian inner product}\index{Hermitian inner product}" on $\mathcal{H}$ (that is to say: a dot product in complex space...), an application:
	
	That satisfies (they are more properties but we will focus only about what we need for practical application in this book and especially quantum physics):
	\begin{enumerate}
		\item[P1.] Positivity:
		
	  	
		\item[P2.] Nullity:
		 
	  	
		\item[P3.] Hermitian symmetry:
		
	  	
	  	\item[P4.] Bilinearity (bilinear form) changes a little bit too ... so that we speak now of "\NewTerm{sesquilinearity}\index{sesquilinearity}". We speak then, in order, of left anti-linearity and of right linearity such as:
	  	
	\end{enumerate}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} It seems that some mathematicians put the anti-linearity on the right. It's just a matter of agreement that does not matter and exists because of a lack of international norms in mathematics.\\
	
	\textbf{R2.} The reader may notice easily that if the elements of the above definitions are all in the set $\mathbb{R}$ then the sesquilinearity is reduced to the bilinearity and the hermitian character to a simple symmetry. So the hermitian inner product reduces to the scalar product.\\
	
	\textbf{R3.} We want to give for now only the minimum on the vast subject that is complex vector spaces so that the reader can read without too much trouble the beginning of the section of Wave Quantum Physics.
	\end{tcolorbox}	
	When we join to a complex vector space a scalar product then just as a real vector space becomes an Euclidean vector space or Prehilbertien vector space, the complex vector space becomes what we name a "\NewTerm{Hermitian vector space}\index{Hermitian vector space}" (term often used in the section of Wave Quantum Physics).
	
	\textbf{Definition (\#\mydef):} Again, we say that a space $\mathcal{H}$ provided with an Hermitian product $\langle \vec{x}|\lambda \vec{y} +\mu \vec{z}\rangle$ is a "\NewTerm{Hilbert space}\index{Hilbert space}" if this space is complete for the metric defined above.
	
	Thus, Hilbert spaces is a generalization of spaces including dot products and Hermitian dot product of Euclidean and Prehilbertien spaces.
	
	\subsubsection{Types of Vectors Spaces}
	To sum it all up:
	\begin{itemize}
		\item We name "pre-Hilbert space" (real or complex) any vector space of finite dimension or not, provided with a dot (scalar) product.
		
		\item We name "Hilbert space" (real or complex) any complete prehilbertian space (as space provide with a norm).
		
		\item We name "Euclidean space" any real vector space of finite dimension with a dot (scalar) product and denoted by $\mathcal{E}^n$.
		
		\item We name Hermitian space any complex vector space of finite dimension with a dot (scalar) product and denoted by $\mathcal{H}^n$.
	\end{itemize}
	
	\pagebreak
	\subsection{System of Coordinates}\label{system of coordinates}
	We will address here the aspect of coordinates changes of vector components not from a basis to another one (for that you need to go see the section of Linear Algebra) but from one coordinate system to another. That means that in any case we will stay in an Euclidean space. This type of transformation has strong implication in physical (and a little bit less in pure mathematics) when we want to simplify the study of physical systems whose equations become easier to handle in other coordinate systems.
	
	\textbf{Definition (\#\mydef):} In mathematics, a "\NewTerm{coordinate system}\index{coordinate system}" is used to match to each point of an $n$-dimensional space, a $m$-tuple of scalars.
	
	Although we are in a chapter and a section of this book that is suppose to be pure maths oriented..., we will allow ourselves in what follows to make a direct connection with physics relatively the terms of the speed and acceleration in different coordinate systems (sorry for the "math skills only" people...). Our teaching experience has show that this helps the readers (most of time students) to better understand the various abstract concepts.
	
	
	\subsubsection{Cartesian (rectangular) Coordinate System}
	We do not want to take too much time on this system as it is well known to everyone usually. However, let us recall that most of the time, in physics, the Cartesian systems in which we are working are in $\mathbb{R}^2 $(two real spatial dimensions), or $\mathbb{R}^3$ (three real spatial dimensions) or even $\mathbb{R}^4$ or $\mathbb{C}^4$ (three spatial dimensions and one of time) when we work in relativity. The number of dimensions can be higher as for example with the Kaluza-Klein theory (five dimensions) merging General Relativity and Electromagnetism or much more with String Theory (above 20 dimensions!!!).
	
	In $\mathbb{R}^3$ (the most common case), there are three basic vectors traditionally denoted by:
	
	Or more explicitly:
	
	
	In this system, the position of a point $P$ (identifiable by a vector $\vec{x}$ for example) is defined by the three numbers named  "\NewTerm{coordinates}\index{coordinates}" (more generally "\NewTerm{components}") denoted (typically in Tensor Calculus):
	
	and in physics denoted more conventionally by:
	
	where usually the component $(z)$ represents the height (vertical), the component $(x)$ is the width and the component  $(y)$ is the length (obviously these are completely arbitrary choice).
	
	This point $P$ can be spotted by a vector arbitrarily designated $\vec{r}$ in the basis  $\vec{e}_i$ by the relation (using Tensor notation):
	
	and if the basis is canonical (orthonormal) such that:
	
	we write:
	
	In physics, if we work with coordinates, it is always to be able to determine the location of an item. Or, as we shall see it more rigorously in the section of Analytical Mechanics, the physicist works with the following concepts (each element being often time-dependent):
	\begin{itemize}
		\item Positions: $\vec{r}=\biggl(x(t),y(t),z(t),t\biggr)$
		
		\item Velocity: $\displaystyle\frac{\mathrm{d}\vec{r}}{\mathrm{d}t}=\dot{\vec{r}}=\vec{v}
	=\biggl(\dot{x}(t),\dot{y}(t),\dot{z}(t),t\biggr)$
	
		\item Acceleration: $\displaystyle\frac{\mathrm{d}\vec{v}}{\mathrm{d}t}=\dot{\vec{v}}=\vec{a}=
	\biggl(\ddot{x}(t),\ddot{y}(t),\ddot{z}(t),t\biggr)$
	\end{itemize}
	Now let us see how the different concepts are expressed in systems such as spherical, cylindrical and polar coordinates (remember that we remains for all of them in a flat Euclidean space!!!).
	
	\pagebreak
	\subsubsection{Spherical Coordinate System}\label{spherical coordinates}
	The choice to start with this coordinate system is not a coincidence. It has the advantage of being a generalization of cylindrical and polar systems that we will meet thereafter and will help us easier to determine the expressions of position, velocity and acceleration\footnote{Two commonly-used sets of "\NewTerm{orthogonal curvilinear coordinates}\index{orthogonal curvilinear coordinates}" are cylindrical polar coordinates and spherical polar coordinates.}\label{orthogonal curvilinear coordinates}.
	
	We traditionally represent (in Switzerland ... and in accordance with the standard ISO 31-11) a spherical coordinate system as follows:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/coordinate_system_spherical.jpg}
		\caption{Representation of the spherical coordinate system}
	\end{figure}
	We see very clearly if we know the basic trigonometric relations and identities (see the section of the same name in the Geometry chapter) we have the transformations:
	
	
	where the two angles $\theta, \phi$ are respectively the latitude and colatitude (longitude):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/algebra/latitude_longitude.jpg}
		\caption[Latitude and Longitude concepts illustrated]{Latitude and Longitude concepts illustrated (source: OpenStax)}
	\end{figure}
	We have inversely:
	
	Now we must find the expressions that connect the vectors of the spherical basis that we choose to denote by $\vec{e}_r,\vec{e}_\theta,\vec{e}_\phi$ with the vectors of the Cartesian basis $\vec{e}_x,\vec{e}_y,\vec{e}_z$:
	
	Let us indicate that by dividing by $\sin(\theta)$ the second basis vector $\vec{e}_\phi$, we make sure that by the properties of the norm of the vector product that:
	
	will be well normalized to unity as expected (as we know from start that as we toke a direct orthogonal coordinate system the product of norms of the basis vectors must be equal to $1$)!
	
	We will also use later (for the study of vector operators further below and the geodesic of the sphere in the section of Analytical Mechanics) the variation $\mathrm{d}\vec{r}$ expressed in spherical coordinates:
	
	Or more explicitly:
	
	
	All this can be put in a famous matrix form as (this also applies automatically to the component of the unitary vector):
	
	To express the velocity and acceleration in spherical coordinates, we will also need the derivatives with respect to time:
	
	So if we do now a little bit of physics, we have:
	
	This brings us to (we will need this relation mainly in the chapter of Astrophysics):
	
	It is interesting that we get the same result through the following method that may be less intuitive:
	
	and substituting the derivative obtained above:
	
	Concerning the acceleration we get:
	
	But we have:
	
	Therefore it comes:
	

	Thus finally:
	
	
	\paragraph{Orthodromic distance}\mbox{}\\\\
	We will now see a powerful and simple application of vector calculus (especially of the dot procuct with spherical coordinates!). The reader will see later in the section of Analytical Mechanics that the geodesic of the sphere is the equation of the great circles (circles on the sphere whose centers coincide with the center of the sphere).

	But without knowing this result, and even without needing it, we want to calculate the "\NewTerm{great-circle distance}\index{great-circle distance}" or "\NewTerm{orthodromic distance}\index{orthodromic distance}" that is the shortest distance between two points on the surface of a sphere, measured along the surface of the sphere:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/orthodromic_distance.jpg}
		\caption{Orthodromic distance (great-circle distance)}
	\end{figure}
	Through any two points on a sphere that are not directly opposite each other, there is a unique great circle. The two points separate the great circle into two arcs. The length of the shorter arc is the great-circle distance between the points. A great circle endowed with such a distance is named a "\NewTerm{Riemannian circle}\index{Riemannian circle}" in Riemannian geometry.
	
	Let us consider two points on the surface of a sphere $P_1$ and $P_2$ and their respective latitude and longtitude such that they are defined by:
	
	Therefore in spherical coordinates:
	
	Hence taking the vectors having for origin the center of the sphere:
	
	The Earth is nearly spherical, so great-circle distance formulas give the distance between points on the surface of the Earth (as the crow flies) correct to within $0.5\%$ or so.
	
	We do now the dot product:
	
	In function of the angle theta we know that the dot product has also for expression:
	
	In function of the an angle that we will denote $\alpha$ we know that the dot product has also for expression:
	
	By identification of that latter relation with prior-previous one we get:
	
	Hence after simplification:
	
	As we know that the circonference of circle is given by $P=2\pi R$ then the arc lengths by unit of angle is given by:
	
	So finally:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider $P_1$ as being the Kourou in French Guiana $(\theta_1,\phi_1)=(+5^\circ.23,52^\circ.75)$ and the second point $P_2$ as being Toulouse in France $(\theta_2,\phi_2)=(43^\circ.63,-1^\circ.37)$. Then we get $\cos(\alpha)=0.485348$ and therefore $l=6,786$ [km].
	\end{tcolorbox}

	\pagebreak
	\subsubsection{Cylindrical Coordinate System}\label{cylindrical coordinates}
	The cylindrical coordinate system (very useful in the study of helical motion systems) is quite similar to spherical coordinates as it can be seen as a slice of the sphere. 

	Given the figure:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/coordinate_system_cylindrical.jpg}
		\caption{Representation of the cylindrical coordinate system}
	\end{figure}
	Warning!!! The vector $\vec{r}$ is unlike the previous spherical case defined only in the $XY$ plane or a plane which is parallel to it!
	
	It comes easily in cylindrical coordinates for $r>0$:
	
	and vice versa:
	 
	Now we must find the expressions that connect the vectors of the cylindrical base that we choose to denote by $\vec{e}_r,\vec{e}_\phi,\vec{e}_z$ (instead of $\vec{r}, \vec{\phi},\vec{z}$ as it is done traditionally) with the vectors of the Cartesian base $\vec{e}_x,\vec{e}_y,\vec{e}_z$. We have identically to what we did for the spherical coordinates:
	
	Or more explicitly:
	
	All this can be put in a famous matrix form as (this also applies automatically to the component of the unitary vector):
	
	Let us indicate that by dividing by $\sin(\phi)$ the second vector base $\vec{e}_\phi$, we ensure us that by the properties of the norm of the cross product we have:
	
	will be well normalized to unity as expected (as we know from start that as we toke a direct orthogonal coordinate system the product of norms of the basis vectors must be equal to $1$)!! In the case of cylindrical coordinates the angle being anyway right, we would not be obliged to indicate this division, but we have made this choice for consistency with previous developments...
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	It is important to notice that the cross product of two basis vectors always gives the third perpendicular  basis vector (like the Cartesian and spherical coordinates so!).
	\end{tcolorbox}
	For future needs, let us determine the partial differential of each of these coordinates:
	
	We will also use later (for the study of vector operators) the variation $\mathrm{d}\vec{r}$ expressed in cylindrical coordinates:
	
	To express the speed and acceleration in cylindrical coordinates, we will also need the derivatives with respect to time:
	
	So if we now do a little bit physics, we get (let us recall that the $z$ component is independent of other cylindrical components):
	
	which brings us to:
	
	For acceleration we get (exactly the same approach as for the expression of the speed):
	
	
	\subsubsection{Polar Coordinate System}\label{polar coordinates}
	The polar coordinate system is very similar to the cylindrical coordinates as it can be seen as an entrenchment of one dimension (the height) of the cylindrical system (we will often encounter this system in the section of Classical Mechanics, Corpuscular Quantum Physics and Astronomy).

	Given the figure:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/coordinate_system_polar.jpg}
		\caption{Representation of the polar coordinate system}
	\end{figure}
	Thus, it comes easily in polar coordinates for $r>0$:
	
	and vice versa:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In physics we frequently use the previous parametric equation of the circle in the following way involving time $t$ and pulsation (as a constant) $\omega$\label{vector parametric circle equation}
	
	Hence:
	
	\end{tcolorbox}
	Now we must find the expressions that connect the vectors of the polar base that we choose to denote by $\vec{e}_r,\vec{e}_\phi$ (instead of $\vec{r}, \vec{\phi}$ as it is done traditionally) with the vectors of the Cartesian base $\vec{e}_x,\vec{e}_y$. We have identically to what we did for the spherical coordinates:
	
	Or more explicitly:
	
	Once again, dividing by $\sin(\theta)$ the second basis vector $\vec{e}_\phi$, we ensure the properties of the norm of the vector product that:
	
	will be well normalized to unity (as we know from start that as we toke a direct orthogonal coordinate system the product of norms of the basis vectors must be equal to $1$)!. In the case of polar coordinate the angle being a anyway right, we would not be obliged to indicate this division, but we have made this choice for consistency with the previous developments.
	
	For future needs, Let us determine the partial differential of each of these coordinates:
	
	We will also use later (for the study of vector operators) the variation $\mathrm{r}$ expressed in polar coordinates:
	
	To express the speed and acceleration in polar coordinates, we will also need the derivatives with respect to time:
	
	So if we now do a little bit physics, we have:
	
	and therefore:
	
	where the first term is the radial velocity component and the second term the tangential component of the (angular) velocity. The velocity expression in polar coordinates is very important in astronomy as it allows quite easily calculate the calculation of the kinetic energy:
	
	For the acceleration we get:	
	
	where the first term is the radial acceleration, the second term the centripetal acceleration, the third the Coriolis acceleration and finally the fourth one the tangential acceleration.
	
	\pagebreak
	\subsection{Differential Operators}\label{differential operators}
	\textbf{Definition (\#\mydef):} Define a scalar field, vector field or tensor field in a volume $V$, it is define an application that for any point $\vec{x}$ of this volume $V$ associates respectively a scalar, a vector or a tensor.
	
	Thus, the application $f$ that at any point $\vec{x}$ of $V$ of spatial coordinates $(x, y, z)$ associates the scalar value $f(\vec{x})=f(x,y,z)$ is a scalar field in $V$.
	
	At each point of a volume traversed by a moving fluid, the vector that coincides at every moment with the speed of the particle which pass through this point at this same time defines a 3D vector field, optionally variable in time. The fields thus defined are a basic mathematical tool in physics.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	When we plot a scalar field, the set of continuous dots of equal value is so-named "\NewTerm{isolines}\index{isolines}" or more commonly "\NewTerm{contours}\index{contours}".
	\end{tcolorbox}
	Vector fields are especially an important tool for describing many physical concepts, such as gravitation and electromagnetism, which affect the behavior of objects over a large region of a plane or of space. They are also useful for dealing with large-scale behavior such as atmospheric storms or deep-sea ocean currents.

	For example the figure below shows a gravitational field exerted by two astronomical objects, such as a star and a planet or a planet and a moon. At any point in the figure, the vector associated with a point gives the net gravitational force exerted by the two objects on an object of unit mass. The vectors of largest magnitude in the figure are the vectors closest to the larger object. The larger object has greater mass, so it exerts a gravitational force of greater magnitude than the smaller object.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/algebra/vector_field_gravitation.jpg}
		\caption[Gravitation vector field example]{Gravitation vector field example (source: OpenStax)}
	\end{figure}
	Another example is the figure below that shows the velocity of a river at points on its surface. The vector associated with a given point on the river's surface gives the velocity of the water at that point. Since the vectors to the left of the figure are small in magnitude, the water is flowing slowly on that part of the surface. As the water moves from left to right, it encounters some rapids around a rock. The speed of the water increases, and a whirlpool occurs in part of the rapids.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/algebra/vector_field_speed.jpg}
		\caption[Velocity vector field example]{Velocity vector field example (source: OpenStax)}
	\end{figure}
	The gradient, divergence and the rotational are the three main linear differential operators of the first order that will be introduced here. This means they only involve the partial first derivatives (or simply "differentials") of the fields, unlike, for example, the Laplace operator which involves partial derivatives of order $2$.	
	
	\pagebreak
	\subsubsection{Gradients of Scalar Field}\label{gradient scalar field}
	The gradient is an operator that applies to a scalar field and transforms it into a vector field. Intuitively, the gradient indicates the direction of the greater variation of the scalar field, and the intensity of this variation. For example, the gradient of the altitude is directed along the maximum slope line and its norm increases with the slope.
	
	Given a three-dimensional scalar field $f(x,y,z)$, wherein x and y and z are the Cartesian coordinates of a point $M$ in space. When $M$ moves in space according to the $\mathrm{d}\vec{r}$ of components $\mathrm{d}x, \mathrm{d}y$ and $\mathrm{d}z$, the scalar field $f$ varies according to the total differential $\mathrm{d}f$:
	
	From this relation, we can define the "\NewTerm{gradient operator}\index{gradient operator}" of a scalar field such as:
	
	where:
	
	is a vector operator named "\NewTerm{gradient of the scalar field $f$}\index{gradient of a scalar field}". To condense the writing, we sometimes use the symbol $\vec{\nabla}$ named the "\NewTerm{nabla of the scalar field $f$}\index{nabla of a scalar field }".
	
	The vector obtained by the gradient calculation has the following three fundamental properties:
	\begin{enumerate}
		\item[P1.] The components of the gradient represent by construction the variation (slope) of the function $f$ in the different directions of space.
		
		\item[P2.] The gradient is perpendicular to the isolines of the function $f$.

		\item[P3.] The direction of the gradient (and therefore its norm) is the maximum variation of $f$.

		\item[P4.] The direction of the gradient shows the values where $f$ increases.
	\end{enumerate}
	\label{gradient normal}Following the request of some readers let us prove some of these properties.
	
	Given $t\mapsto C(t)$ an isoline. Then $t\mapsto f(c(t))=c^{te}$ and therefore:
	  
	which proves that the gradient is orthogonal to the tangent of the isoline (property P2).
	
	Let us come back on:
	
	That it is tradition to write as:
	
	named the "\NewTerm{directional derivative}\index{directional derivative}". Its value is maximum obviously if $\theta=0$, that is to say that the gradient is colinear to the variation $\mathrm{d}\vec{r}$. Hence, the direction (and therefore the norm) of greatest increase of $f$, often named direction of "\NewTerm{deepest ascent}\index{deepest ascent}\label{eepest ascent}", is the same direction as the gradient vector!! Thus we proved the property P3.
	
	Obviously the directional derivative takes on its greatest negative value if $\theta=\pi$ (or $180$ degrees). Hence, the direction of greatest decrease of $f$ is the direction opposite to the gradient vector and often named direction of "\NewTerm{deepest descent}\index{deepest descent}\label{eepest descent}".
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/algebra/gradient_deepest_descent_deepest_ascent.jpg}
		\caption{Gradient deepest ascent and deepest descent}
	\end{figure}
	If $\theta=\pi/2$ then there is no change in altitude in this direction and we are on an "\NewTerm{isocline}\index{isocline}".
	
	The property P4 can be explained without formalism as following:
	\begin{itemize}
		\item If the function is decreasing in one variable, then the partial derivative is negative, so the component vector of the gradient for that variable points in the negative direction - which means increasing function value.

		\item If the function is increasing in one variable, then the partial derivative is positive, so the component vector of the gradient for that variable points in the positive direction - which means increasing function value.
	\end{itemize}
	Then is doesn't matter how the function profile is, the gradient, by definition, points in the increasing direction. Indeed, when 	
$f(x,y)$ is decreasing in $x$, the function decreases as you move forward in $x$. But because the partial derivative with respect to $x$ is negative, the $x$-component of the gradient points towards origin (move backward in $x$), that is to say in the direction which makes f to increase.
	
	From the definition and from the total differential, we get
	
	This leads us to put that:
	
	and so that finally the operator of the "\NewTerm{gradient in Cartesian coordinates}\index{gradient in Cartesian coordinates}" is given by:
	
	Finally we see that the gradient of a scalar field $f(x,y,z)$ is the vector field whose components at each point are the three derivatives of the scalar field $f$ with respect to the three-dimensional coordinates, denoted here by $x, y, z$ and that by its direction, and its norm, the gradient vector of a scalar field at a point therefore includes indications on how the field varies around this point.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	One of the necessary and sufficient conditions for a vector field to be the gradient of a scalar field $f$ is that this vector field is irrotational (see below the rotational operator of a vector field).
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us find the direction for which the directional of:
	
	at $(-2,3)$ is a maximum and what is its maximum value?
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/directional_derivative_search_plot_maple.jpg}
		\caption[]{Maple plot of $f(x,y,)=3x^2-4xy+2y^2$}
	\end{figure}
	The maximum value of the directiona derivative occurs as we have just proved it when $\vec{\nabla}f$ and the unit vector $\mathrm{d}\vec{r}$ point in the same direction.\\

	Therefore we start by calculating $\vec{\nabla}f(x,y)$:
	
	Next we evaluate the gradient at $(-2,3)$:
	
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	We need to find a unit vector that points in the same direction as $\vec{\nabla}f(-2,3)$, so the next step is to divide $\vec{\nabla}f(-2,3)$ by its norm (the value of that norm being also the maximum value of the directional derivative at point $(-2,3)$), which gives:
	
	As the unit vector above is a vector in the plane, nothing avoid us to calculate the angle it does with the axis. By applying elementary trigonometry, we get if we denote this angle by $\theta$:
	
	Since cosine is negative and sine is positive, the angle must be in the second quadrant. Therefore:
	
	that is to say approximately $114.5916$ degrees (or seen from the point of view of the first quadrant: $39.792$ degrees).\\
	
	With Maple 4.00b we can have a more detailed and general investigation of the calculation we just did:\\

	\texttt{>with(plots):\\
	>with(linalg):\\
	>Pa:=contourplot(3*x\string^3-4*x*y+2*y\string^2,x=-3..-1,y=2..4,filled=true):\\
	>Pb:=fieldplot(grad(3*x\string^3-4*x*y+2*y\string^2,vector([x,y])),x=-3..-1,y=2..4):\\
	>display(Pa,Pb);
	}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/algebra/directional_derivative_gradient_plot_maple.jpg}
	\end{figure}
	\end{tcolorbox}
	After having defined the gradient in Cartesian coordinates $x, y, z$ we have to address the expression of this operator in other coordinate systems. It is common in physics to have to use cylindrical, polar and spherical coordinates to simplify the formal study of physical systems. So if we refer to the previous study of coordinate systems, we have (recall) first in polar coordinates:
	
	But, with the definition of gradient in Cartesian coordinates, in polar coordinates we have the following definition:
	
	If we express the total exact differential (\SeeChapter{see section of Differential and Integral Calculus page \pageref{total exact differential}}) of $\mathrm{d}f$ we obtain the following relation:
	
	This allows us to get the relation:
	
	therefore:
	
	which bring us to:
	
	Thus the "\NewTerm{gradient in polar coordinates}\index{gradient in polar coordinates}\label{gradient in polar coordinates}" is expressed as
	
	Let us now tackle the expression of the gradient in cylindrical coordinates. Let us recall that  during our study of different coordinate systems we obtained for cylindrical coordinates:
	
	So we already know that the expression of the gradient in cylindrical coordinates will be the same in polar coordinates with the exception of the addition of the vertical $z$ component that is independent of other coordinates. Thus we get the "\NewTerm{gradient in cylindrical coordinates}\index{gradient in cylindrical coordinates}":
	 
	Let us now tackle on the expression of the gradient in spherical coordinates. Let us recall that during our study of the different coordinate systems we obtained for the spherical coordinates:
	
	But, with the definition of gradient in Cartesian coordinates, we have in spherical coordinates the following definition:
	
	If we express the total differential of $\mathrm{d}f$ we get the following relations:
	
	This allows us to obtain the relation (we now use the notation that uses the operator "nabla"):
	
	The relation:
	
	requires that:
	
	Thus the "\NewTerm{gradient in spherical coordinates}\index{gradient in spherical coordinates}" is expressed as:
	 
	So we finally saw all the expressions of the gradient operator in the Cartesian, polar, cylindrical and spherical systems.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us see now a visual example of the previous developements with Maple 4.00 and a special case function $f(x,y)=\sin(x)\sin(y)$.\\
	
	\texttt{> with(linalg):\\
	> with(plots):\\
	> plot3d(sin(x)*sin(y),x=-3..3,y=-3..3,axes=framed);}\\
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/gradient_function_of_example.jpg}
		\caption[]{Plot of the function taken as example}
	\end{figure}
	And now we show the isolines:\\
	
	\texttt{>contourplot3d(sin(x)*sin(y),x=-3..3,y=-3..3,filled=true,\\
	axes=framed,coloring=[red,blue],style=patch);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/gradient_function_with_isolines.jpg}
		\caption[]{Function with is isolines}
	\end{figure}
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]

	And now we plot the gradient vector with a plane projection and we see they are indeed perpendicular to the isolines:\\\\
		\texttt{>Pa:=contourplot(sin(x)*sin(y),x=-3..3,y=-3..3,contours=10,\\
		coloring=[red,blue],filled=true):
\\
	>Pb:=fieldplot(grad(sin(x)*sin(y),vector([x,y])),x=-3..3,y=-3..3,\\
	arrows=THICK):\\
	>display(Pa,Pb);}
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/gradient_function_with_isolines_and_projected_gradient.jpg}
		\caption[]{Function with its isolines and projected gradient}
	\end{figure}
	And with a 3D perpsective:\\
	
	\texttt{>campo:=fieldplot3d([diff(sin(x)*sin(y),x),diff(sin(x)*sin(y),\\
	y),0],x=-3..3,y=-3..3,
z=-3..3,axes=framed,arrows=THICK);\\
>superf:=plot3d(sin(x)*sin(y),x=-3..3,y=-3..3):
\\
	> display({campo,superf});}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/algebra/gradient_function_with_isolines_and_gradient.jpg}
		\caption[]{Function with its isolines and gradient in 3D}
	\end{figure}
	\end{tcolorbox}

	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	Seen from above with a small rotation:\\
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/gradient_function_with_isolines_and_projected_gradient_rotation.jpg}
		\caption[]{Function rotation with its isolines and gradient}
	\end{figure}
	\end{tcolorbox}
	
	\subsubsection{Gradients of Vector Field}\label{gradient of vector field}
	The gradient of a vector field $\vec{f}(x,y,z): \mathbb{R}^3\mapsto\mathbb{R}^3$ is the field named "\NewTerm{tensor field}\index{tensor field}" defined by the following nine relations in Cartesian coordinates:
	 
	 We will use such a gradient in our study in the section of Marine \& Weather Engineering of the Papillon effect whose origin comes from the determination of the Navier-Stokes equations of the section of Continuum Mechanics and we will also use this type of gradient in the section of Theoretical Computing in our study of the Gauss-Newton optimization method.
	
	We have the following 4 components in polar coordinates:
	 
	We have the following 9 components in cylindrical coordinates:
	 
	We have the following 9 components in spherical coordinates:
	 
	 So we finally saw all the expressions of a gradient vector field in Cartesian , polar, cylindrical and spherical system.
	 
	\subsubsection{Divergences of a Vector Field}\label{divergence vector field}
	The diverence is applied to a vector field and turns it into a scalar field. Therefore it is an application from $\mathbb{R}^3\mapsto \mathbb{R}$. Intuitively, and in the most common case, the divergence of a vector field expressed its tendency to come from or converge to some points.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Non-initated people often confuse the gradient and divergence operator. To make the difference we must remember that the divergence of a vector is a number and that the gradient is a vector! The gradient indicates the direction in which the change is the most important and its amplitude. The divergence simply say what comes in or out from a given point.
	\end{tcolorbox}	
	However, we must distinguish two contributions to the divergence that we will rigorously define a little further below: one due to the variations named the "\NewTerm{directional divergence}\index{directional divergence}" and the other due to variations in modules (norm) named the "\NewTerm{modular divergence}\index{modular divergence}". Thus, for simple fields, we can imagine cases where the divergence would only be modular and others, where it would only be directional. We could also build a field where the two types of divergence coexist, but having adverse effects.
	
	Let us consider for example a vector $\vec{f}$ of space and we make it pass through any surface $S$. Physicists then assimilate the quantity $\vec{f}$ which moves along the normal vector to the surface as a flow of $\vec{f}$ through $S$.
	
	To be convinced of this analogy we can imagine a fluid flowing on a flat surface, the flow through the surface is obviously zero in this cas, by cons if the fluid flows vertically through a horizontal surface the flow will be maximal. It is then immediate that we want to represent the flow by the scalar product of $\vec{f}$ with the normal $\vec{n}$ to  the surface $S$.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We must always pay attention to the direction of $¨\vec{n}$ because at any point of a surface $S$ we have in general two normal vectors $\vec{n}$ that are colinear but of opposite directions.
	\end{tcolorbox}	
	If the surface is planar  then the normal $\vec{n}$ is the same everywhere, but if it changes from place to place, then we will look at a small surface element $\mathrm{d}S$.
	
	If a small flow element is defined by:
	
	then the total flow will be given by:
	
	which is sometimes written (it's a little bit abusive but why not ...)
	
	Let us now suppose that our vector $\vec{f}$ moves a point $M(x,y,z)$ in space to  $M'(x+\mathrm{d}x,y+\mathrm{d}y,z+\mathrm{d}z)$ through a rectangular parallelepiped of sides of $\mathrm{d}x, \mathrm{d}y$ and $\mathrm{d}z$:
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/ostrogradsky_box_vector_displacement.jpg}
		\caption[]{Move of a vector through a parallelepiped}
	\end{figure}
	We can decompose the movement (flow) through each face of the parallelepiped (decompositions in the orthonormal basis). For example, if we are interested at the decomposed part of the flow through the face $(\mathrm{d}y, \mathrm{d}z)$ described by the peaks vertices $BCFG$ we have obviously $\vec{n}=(1,0,0)$.
	
	We still need to determined how to represent the flow $\vec{f}$ for this direction. As the flow is a function, that is to say that each of its components may be dependent of the three components of the space (if we take the case of a function in $\mathbb{R}^3$) we have:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Those who are not convinced can go read the beginning of section Electrodynamics where we take the electric field as an (excellent) example.
	\end{tcolorbox}	
	While the variation of the flow according to $x$ is given by:
	
	which give us:
	
	therefore by summing:
	
	Compared to the first expression of $\Phi$, the term $\mathrm{d}x\mathrm{d}y\mathrm{d}z$ is then a volume element and not more of surface. We also have an interesting result:
	
	whose more explicit and rigorous writing  should be (to highlight well that the considered closed surface is the boundary of the closed studied volume):
	
	or more commonly written:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	See the practical examples in the section Electrodynamics where for example the electric field divergence is zero for a free spherical charge as the vectors point in different directions (directional divergence) and where the norms decrease as the inverse of the square of the radius (modular convergence). Both contributions are in opposition and so the total divergence is zero.
	\end{tcolorbox}
	The development above is named "\NewTerm{Ostrogradsky theorem}\index{Ostrogradsky theorem}" or "\NewTerm{Gauss-Ostrogradsky theorem}\index{Gauss-Ostrogradsky theorem}\label{gauss ostrogradsky theorem}" or more simply "\NewTerm{divergence theorem}\index{divergence theorem}" and actually defines the total divergence of $\vec{f}$ in a volume as the flow $\vec{f}$ through the "walls" of the closed volume (Gauss closed surface), which expresses well the name "divergence".
	
	Now reconsider the previous relation but extracting and unitary vector from the vector field $\vec{f}$ such that:
	
	where now $f$ is a scalar field. This can be rewritten obviously using chain rule derivatives an dot product commutativity:
	
	In the special case of a uniform field we have:
	
	Then it remains:
	
	The dot product being distributive on the sum of vectors we can rewrite this as:
	
	and therefore we get the "\NewTerm{gradient theorem\footnote{As it is applicable only for uniform field it is not used a lot in practice}}\index{gradient theorem}":
	
	
	We define the operator "\NewTerm{divergence}\index{divergence operator}" by the following relation (the tensor notation has been used to shorten the writing) in an $n$-dimensional space:
	
	Thus we have for the operator "\NewTerm{divergence in Cartesian coordinates}\index{divergence in Cartesian coordinates}":
	
	If the divergence of a vector field is identically zero in all the points of an Eulerian frame \footnote{The Eulerian specification of the flow field is a way of looking at fluid motion that focuses on specific locations in the space through which the fluid flows as time passes.[1][2] This can be visualized by sitting on the bank of a river and watching the water pass the fixed location.}, the triple integral flux of this field through a volume $V$ will be:
	
	It follows that the flow of this vector field through the edges of the volume is zero, that is to say that the incoming flow compensates the output flow. We say that such a field vectors having a null divergence has a "\NewTerm{conservation flow}\index{conservation flow}".
	
	To determine the expression of the divergence operator in polar coordinates let us recall the relations proved earlier above:
	
	Given now a vectorial function $\vec{f}:\mathbb{R}^2\rightarrow \mathbb{R}^2$. We have:
	
	Knowing the expression of $\vec{e}_r,\vec{e}_\phi$ depending on $\vec{e}_x,\vec{e}_y$, from the expression above we deduce:
	
	The divergence of $\vec{f}$ being defined in the two dimensional case by:
	
	we then have:
	
	The first term is (application of the gradient in polar coordinates!):
	
	in the same way we get for the second term (we can as always give more details on request):
	
	By adding the two terms and expressing the partial derivatives of the functions $f_x,f_y$ in function of the partial derivatives of the functions $f_r,f_\phi$ using the relations:
	
	We get:
	
	After simplification:
	
	The expression of the operator "\NewTerm{divergence in polar coordinates}\index{divergence in polar coordinates}" is then:
	
	To determine the expression of the divergence operator in cylindrical coordinates let us recall the relations:
	
	Given now a vector function $\vec{f}:\mathbb{R}^3\rightarrow \mathbb{R}^3$. We have:
	
	As we know the expressions of $\vec{e}_r,\vec{e}_\phi,\vec{e}_z$ in function of the $\vec{e}_x,\vec{e}_y,\vec{e}_z$, from the above expressions we deduce:
	
	The divergence of $\vec{f}$ being defined in the three dimensional case by:
	
	we then have:
	
	The first term is equal to (application of the gradient in cylindrical coordinates):
	
	in the same way we get for the second component (we can give the details on request):
	
	and for the last one:
	
	By adding the three terms and expressing the partial derivatives of the functions $f_x,f_y,f_z$ in function of the partial derivatives of the functions $f_r,f_\phi,f_z$ using the relations:
	
	we get:
	
	After simplification:
	
	The expression of the operator "\NewTerm{divergence in cylindrical coordinates}\index{divergence in cylindrical coordinates}" is then:
	
	To find the expression of the divergence in spherical coordinates, let us recall the relations:
	
	Given now a vector function $\vec{f}:\mathbb{R}^3\rightarrow \mathbb{R}^3$. We have:
	
	Knowing the expression of $\vec{e}_r,\vec{e}_\theta,\vec{e}_\phi$ depending on $\vec{e}_x,\vec{e}_y,\vec{e}_z$, from the expression above we deduce:
	
	The divergence of $\vec{f}$ being defined in the three dimensional case by:
	
	we then have:
	
	The first component is equal to (application of the gradient in spherical coordinates):
	
	in the same way we get for the second component (we can give the details on request):
	
	and finally for the third and last one:
	and:
	
	By adding the three terms and expressing the partial derivatives of the functions $f_x,f_y,f_z$ in function of the partial derivatives of the functions $f_r,f_\theta,f_\phi$ using the relations:
	
	we get (we can develop the intermediate details on request):
	
	We notice that we can regroup terms depending on the same variable using the property of the derivative, so we get for the expression of the divergence in spherical coordinates:
	
	and therefore the operator of "\NewTerm{divergence in spherical coordinates}\index{divergence in spherical coordinates}" is:
	
	So we finally we saw all the expressions of the divergence operator of a vector field in Cartesian, polar, cylindrical and spherical systems.
	
	\pagebreak
	\subsubsection{Rotationals of a Vector Field (Curl)}\label{rotational}
	The "\NewTerm{curl}\index{curl}" or "\NewTerm{rotationnal}\index{rotational}" of a vector field can be seen (this is a simplification!) as the vector field whose field lines are perpendicular to those we have calculated the rotational as shown in the special example below (we will see more academic detailed further below):
	\begin{figure}[H]
	\centering
		\includegraphics{img/algebra/curl.jpg}
		\caption{Example of rotational of a vector field}
	\end{figure}
	In a little bit more technical way the rotational is a vector operator that describes the infinitesimal rotation of a $3$-dimensional vector field. At every point in the field, the rotational of that point is represented by a vector. The attributes of this vector (length and direction) characterize the rotation at that point. he direction of the rotational is the axis of rotation, as determined by the right-hand rule, and the magnitude of the rotational is the magnitude of rotation. 
	
	The rotational transforms a vector field in another vector field. For most people it is more difficult to accurately represent than the gradient and divergence, it intuitively reflects the tendency of a field to rotate around a point (the way it is twisted).
	
	Let us give before tackling with the mathematical stuff and also mathematical examples two every-day life examples:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. In a tornado, the wind turns around the eye of the storm and the wind velocity vector field has a non-zero rotational around the eye.\\

	E2. The rotational of the velocity field of a disc that rotates at a constant speed is constant, directed along the axis of rotation and oriented such that the rotation takes place, in relation to it, in the direct sense.\\
	\end{tcolorbox}
	A vector field is said to by "\NewTerm{irrotational}\index{irrotational}\label{irrotational}" when the rotational of this field is identically zero at all points of space. Otherwise, we say it is a "\NewTerm{vortex}\index{vortex}".
	
	In the usual case where $\mathrm{d}x$ is an element of length, the measurement unit of the rotational is then the unit of the considerated field divided by a unit length. For example, in fluid mechanics: the unity of the rotational of a velocity field is radians per unit time, as an angular velocity as we divide a velocity ([ms$^{-1}$] by the length [m]!
	
	The divergence gives some indication of the behavior of a vector or a vector field: how it moves in relation to the normal and how it crosses the surface, but it is insufficient. Take a field which would have the shape of a cylinder and another field which have a helicoidal form of the same diameter as the cylinder. If the move  in the same direction the divergence will be the same even if the movements are quite different This requires that we determine how the field is bent as it passes through a surface: this will be determined by the circulation (as the work of a force, for example) of the vector along a closed curve, obtained with the sum of dot products $\vec{f}\circ \mathrm{d}\vec{r}$ on the closed contour (\SeeChapter{see section Differential and Integral Calculus page \pageref{curvilinear integral}}):
	
	in fact it's the same to look at how twisted is the vector with respect to the normal vector of the surface which leads us to define the "rotational" or "swirl vector" by writing:
	
	that thus establishes a relation between the line integral and the surface integral (we then transform a line integral on a closed path in a surface integral delimited by the given path).
	
	\begin{theorem}
	In other words, the rotational is calculated by using the fact that the flow around a closed basic path of a vector field is equal to the flux of its rotational through the immediate elementary surface generated by this path.

	This is the "\NewTerm{Stokes' theorem}\index{Stokes' theorem}\label{stokes theorem}" (which is more rigorously demonstrable with a heavy mathematical formalism) which is in fact a definition of the rotational operator which we will seek the explicit mathematical expression right now!
	\end{theorem}
	\begin{dem}
	Given $\vec{f}$ a vector field defined in a given space. We want to calculate the circulation of $\vec{f}$ around a closed path (contour) $C$:
	
	We choose for contour $C$ the edged of an infinitesimal rectangle $(\mathrm{d}x,\mathrm{d}y)$ that is into $\mathbb{R}^3$and parallel to the $xy$-plane (note that we travel the contour so as to always have the surface to our left!):
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/rotational_contour_path.jpg}
		\caption[]{Contour (path) of integraton}
	\end{figure}
	For the two horizontal sides (edges), the contribution to the circulation is:
	
	Which authorize us to write:
	
	Same for the vertical sides  (edges) we have also:
	
	Therefore we have the circulation following $z$:
	
	Which can also be written in the following more general and important form:
	
	and that is nothing less than the famous "\NewTerm{Green's theorem}\index{Green's theorem}\label{green theorem}" or also known as the "\NewTerm{Green-Riemann theorem}\index{Green-Riemann theorem}" that we will see again in the section of Complex Analysis.
	
	Green's theorem is a special case of Stokes (general manifold) theorem, but a generalisation to 2-dimension of the fundamental theorem of calculus. But stated in another way: The total spin outside of a function (closed curve) is equal to the sum of little spins on the inside of the function (closed curve).
	
	And that we will write in the situation that interest us:
	
	By circulation permutation we then get:
	
	Either in vector condensed form:
	
	This allows us to better understand the notation, or the non intuitive definition of the rotational in many books and that is:
	
	that is to say the cross product of the gradient operator by the vector field!
	
	So finally we have proved the Stokes theorem or also named in this form "\NewTerm{curl theorem}\index{curl theorem}" that gives well:
	
	and at the same time the rotational in Cartesian coordinates.
	
	The above classical Stokes theorem can be stated in one sentence: The line integral of a vector field over a loop is equal to the flux of its curl through the enclosed surface.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The above relation is in fact not the "Stokes theorem" but a special case of it named the "\NewTerm{Kelvin–Stokes theorem}\index{Kelvin–Stokes theorem}". The stokes theorem is given by (without proof):
	
	and says that the integral of a differential form $\omega$ over the boundary of some orientable manifold $\Omega$ is equal to the integral of its exterior derivative $\mathrm{d}\omega $ over the whole of $\Omega$. It is the generalization of the fundamental theorem of calculus to $N$ dimensions.\\
	
	This  is  the  most  general  and  conceptually  pure  form  of  Stokes’  theorem,  of  which  the  fundamental  theorem  of calculus (1D case), the fundamental theorem of line integrals, Green’s theorem (2D case), Stokes’ (original) theorem (3D case), and the divergence theorem are all special cases!
	\end{tcolorbox}	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Let us take the vector field, which depends on $x$ and $y$ linearly:
	
	Its plot look like this in Maple 4.00b:\\
	
	\texttt{
	>with(DEtools): with(plots):\\
	>fieldplot([y, -x], x=-5..5, y=-5..5,arrows = medium, \\
	color = sqrt(x \string^2 + y\string^2),thickness=2,labels=[`x`,`y`],\\
	title=`Simple vector field`);	
	}
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/vector_field_01.jpg}
		\caption[]{Vector field example with Maple 4.00b}
	\end{figure}
	Simply by visual inspection, we can see that the field is rotating. If we place a paddle wheel anywhere, we see immediately its tendency to rotate clockwise. Using the right-hand rule, we expect the rotational to be into the page. If we are to keep a right-handed coordinate system, into the page will be in the negative $z$ direction. The lack of $x$ and $y$ directions is analogous to the cross product operation.\\

	If we calculate the rotational:
	
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	With Maple 4.00b we get this algebraic result with the following commands:\\
	
	\texttt{>with(linalg):\\
	>f:=[y,-x,0];v:=[x,y,z];\\
	>curl(f,v);\\
	}
	
	As we did yet not have the time to find an easy way to plot the resulting rotational vector field in the release 4.00b of Maple we will take the picture provided by Wikipedia:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/algebra/rotational_of_vector_field_01.jpg}
		\caption[]{Rotational of previous vector field (source: Wikipedia)}
	\end{figure}
		
	E2. Suppose we now consider a slightly more complicated vector field:
	
	Its plot:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/algebra/vector_field_02.jpg}
		\caption[]{second vector field example (source: Wikipedia)}
	\end{figure}
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	We might not see any rotation initially, but if we closely look at the right, we see a larger field at, say, $x=4$ than at $x=3$. Intuitively, if we placed a small paddle wheel there, the larger "current" on its right side would cause the paddlewheel to rotate clockwise, which corresponds to a rotational in the negative $z$ direction. By contrast, if we look at a point on the left and placed a small paddle wheel there, the larger "current" on its left side would cause the paddlewheel to rotate counterclockwise, which corresponds to a rotational in the positive $z$ direction. Let's check out our guess by doing the math:\\
	
	Indeed the rotational is in the positive $z$ direction for negative $x$ and in the negative $z$ direction for positive $x$, as expected. Since this rotational is not the same at every point, its plot is a bit more interesting:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/algebra/rotational_of_vector_field_02.jpg}
		\caption[]{Rotational of previous vector field (source: Wikipedia)}
	\end{figure}
	\end{tcolorbox}
	\pagebreak	
	Let us now determine the expression of the rotational in cylindrical coordinates (the rotational in polar coordinates is not defined in $\mathbb{R}^2$).
	
	Using the same technique as for the rotational in Cartesian coordinates, we write the circulation of $\vec{f}$ along a contour corresponding to a small piece $P_1P_2P_3P_4$ of an orthogonal cylinder (oriented in the direction of the $z$-axis):
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/rotational_cylindrical.jpg}
		\caption[]{Representation of the cylinder piece $P_1P_2P_3P_4$}
	\end{figure}
	We have then by fixing $z$ (caution! the $\mathrm{d}\vec{r}$ has nothing to do with the cylinder radius $r$... the notation can be confusing I'm sorry!):
	
	the total circulation thus gives after regrouping terms:
	
	We can not at this stage compare with the rotational because it is difficult to us to make appearing the differential of the surface if we look at the differentials that currently appear in circulation. The best is then to divide everything by $r\mathrm{d}\phi\mathrm{d}r$:
	
	Therefore:
	
	Now we determine the rotational by fixing $\varphi$. The problem is like having a rectangle in the space that we travel to determine the circulation. But we already know what is the result of the rotational for a rectangle in Cartesian coordinates following the $z$-axis:
	
	except that in cylindrical coordinates we have to replace $z$ by $\varphi$, $x$ by $y$, $y$ by $r$ and $f_y$ by $f_r$ and finally $f_x$ by $f_z$ (this choice is always appropriate simply because the circulation is such that the surface is always on our left). This gives us:
	
	It therefore only remains to us to find the component of the rotational on $r$ (therefore when $r$ is fixed). The calculation is more difficult as we have to follow (positively always!) a curved surface by the variation of the angle $\varphi$.
	
	We then have by fixing $r$:
	
	the total circulation thus gives after regrouping terms:
	
	We can not at this stage compare with the rotational because it is difficult to us to make appearing the differential of the surface if we look at the differentials that currently appear in circulation. The best is then to divide everything by $r\mathrm{d}\phi\mathrm{d}z$:
	
	Then finally:
	
		And finally we have the "\NewTerm{rotational in cylindrical coordinates}\index{rotational in cylindrical coordinates}" given in its globality by:
	
	The reader can check verify that this result is simply the gradient in cylindrical coordinates applied to the vector field $\vec{f}$.
	
	To be convinced, let us now show directly the expression of the rotational in spherical coordinates by showing this through the cross product of the gradient in spherical coordinates with the vector field $\vec{f}$.
	
	First let us recall that we have obtained for the gradient in spherical coordinates:
	
	Therefore we have:
	
	what we can write using the decomposition in basis vectors:
	
	Thanks to the partial derivatives that we proved earlier during our introduction to the spherical coordinates, it comes:
	
	The cross products with the colinears vectors canceled. Therefore it remains:
	
	As the cross product of two basis vectors give the corresponding orthogonal vector (positively or negatively) then we have:
	
	By regrouping the terms it comes:
	
	Thus by simplifying:
	
	Thus finally we get the "\NewTerm{rotational in spherical coordinates}\label{rotational in spherical coordinates}":
	
	
	\pagebreak
	\subsubsection{Laplacians of Scalar Fields (Laplace Operator)}\label{scalar laplacian}
	The Laplacian of a scalar field $\phi(x_1,x_2,x_3)$ give also a scalar field that gives the difference between the value of the function $\phi$ on one point and it average around this point. In other words: the second partial derivative measure the variations of the slope on the study point in its immediate neighborhood and following one direction at a time. If the second partial derivative is null following $x$, then the slope is constant in its immediate neighborhood and following this dimension (direction), this implies that the value of the function at the study point is the average of its neighborhood (following one dimension).
	
	The reader will be able to see again major practical applications of this differential operator in the sections of Complex Analysis, Quantum Chemistry, Astronomy, Electrodynamics, Weather \& Marine Engineering, Wave Mechanics, Wave Quantum Physic and Quantum Field Theory.
	
	This operator is defined from the divergence and the gradient and we denote it by (tensor notations):
	
	The Laplacian is null, or quite small, when the function varies. The functions satisfying the "\NewTerm{Laplace equation}\index{Laplace equation}":
	
	are named "\NewTerm{harmonic function}\index{harmonic function}".
	
	Thus the "\NewTerm{scalar Laplacian operator in cartesian coordinates}\index{scalar Laplacian operator in cartesian coordinates}" is by this definition, given by:
	
	The Laplacian of a scalar field in other coordinate systems is a little bit more hard to get that for the other differential operators. There are more than one possible proof but among the existing one we have try to choose (as always) this that seem to us the most interesting in the point of view of the tools used (and not of simplicity!).

	Given the Laplacian in cartesian coordinates in $\mathbb{R}^2$ of a scalar field $f$:
	
	To determine this expression in polar coordinates, we will use the total exact differential and the multivariate chain rule in polar coordinates (\SeeChapter{see section Differential and Integral Calculus page \pageref{multivariate chain rule}}):
	
	therefore for a second derivative:
	
	but we know that we have in polar coordinates:
	
	hence for the first derivative:
	
	and for the second derivative:
	
	therefore:
	
	and given that the second partial derivatives are continuous, then the cross derivatives are equal according to the Schwarz's theorem (\SeeChapter{see section Differential and Integral Calculus page \pageref{Schwarz theorem}}):
	
	Therefore:
	
	Similarly, we will have:
	
	hence the expression of the Laplace operator in polar coordinates by adding the last two expressions:
	
	Therefore the "\NewTerm{scalar Laplacian in polar coordinates}\index{scalar Laplacian in polar coordinates}" is finally given by:
	
	To find the expression of the Laplacian operator in spherical coordinates, we will use the intuition of the physicist and the concepts of similarity.
	
	We will first of all help us with the below figure to find out what we mean:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/coordinate_system_spherical_for_Laplacian_study.jpg}
		\caption[]{Recall of the spherical coordinate system representation}
	\end{figure}
	Recall that the relation between cartesian and spherical coordinates are given by the relations:
	
	We will now consider the following similarities:
	\begin{enumerate}
		\item Cylindrical coordinates:
			
		
		\item Spherical coordinates:
			
	\end{enumerate}
	Let us build a correspondence table:
	
	The goal is to play with this correspondence with in a first time the Laplacian in cylindrical coordinates where we have subtracted from both sides of the equality the term $\dfrac{\partial^2 f}{\partial z^2}$. Therefore:
	
	let us now use our small correspondence table and we get:
	
	The second term of the equality of the latter relation is the spherical equivalent of the term \#1 of the Laplacian in cylindrical coordinates:
	
	Now let us examine and focus on the term: $\dfrac{1}{\rho}\dfrac{\partial f}{\partial \rho}$
	
	Identically as when we determined the relation:
	
	we get:
	
	with:
	
	Which give us the possibility to write:
	
	If we play again with our small correspondence table we get:
	
	We divide the latter relation by $\rho$ and we get:
	
	We have therefore above the spherical equivalent of the second term \#2 of the Laplacian in cylindrical coordinates:
	
	The last and third term is quite simple to determine. We just have to replace $\rho$ by $r\sin(\theta)$ to get:
	
	By bringing together all terms obtained previously, we finally get the extended form of the Laplacian in spherical coordinates used so much in physics (see corresponding sections of this book):
	
	We can shorten this expression by factoring the terms:
	
	If we condense even a little bit more, we get the final expression of the "\NewTerm{scalar Laplacian in spherical coordinates}\index{scalar Laplacian in spherical coordinates}\label{scalar laplacian in spherical coordinates}" named also "\NewTerm{spherical Laplacian}":
	
	
	\subsubsection{Laplacians of Vector Fields}\label{laplacian of vector fields}
	As to the Laplacian of a scalar field, the Laplacian of a vector field is only a very convenient notation system for condensing the writing of the components of a vector field.
	
	The reader will also find practical applications of this operator in the sections of Electrokinetics, Electrodynamics and Continuum Mechanics.
	
	Thus, the vector Laplacian is often defined by:
	
	We also prove that in the specific case of the Cartesian coordinates, the Laplacian of a vector field has the components the scalar Laplacian of each of the components.
	
	We also prove that in the specific case of the Cartesian coordinates, the Laplacian of a vector field has the components the scalar Laplacian of each of the components.
	
	So therefore have in Cartesian coordinates:
	
	So we therefore in Cartesian coordinates:
	
	and thus the "\NewTerm{vector Laplacian of a vector field in Cartesian coordinates}\index{vector Laplacian of a vector field in Cartesian coordinates}" is indeed the scalar Laplacian of each component:
	
	Or more explicitly:
	
	The Laplacian of a vector field, frequently named "\NewTerm{vectorial laplacian}\index{vectorial laplacian}", in other coordinates systems is quite simple to get once we know the Laplacian of a scalar field in the same coordinates!
	
	We have first in cylindrical coordinates:
	
	To simplify (because of a lack of space) let us focus first on the first line:
	
	and after for the second line:
	
	and finally the third one:
	
	So what gives for us for the "\NewTerm{vector Laplacian in cylindrical coordinates}\index{vector Laplacian in cylindrical coordinates}" as we can find it in tables or forms:
	
	To finish and in the joy (...) let us make the merry and detailed calculations of the vector Laplacian operator in spherical coordinates (it's quite long but it's just to make sure that we fall back on what is in tables and forms):
	
	Let us focus on the first line (caution! this will be quite long...):
	
	\pagebreak
	
	That's it for the first line ... Let us get on to the second line always with joy...:
	
	
	\pagebreak
	and finally a last effort for the third and last line:
	

	\pagebreak
	
	So what gives for the "\NewTerm{vector Laplacian in spherical coordinates}\index{vector Laplacian in spherical coordinates}" as we can find it in tables or forms:
	
	that's it... for the skeptics...
	
	\subsubsection{Remarkable Identities}\label{differential operators identities}
	The scalar and vector differential operators have some very simple remarkable identities that we will find very often in physics in this book.
	
	Let us first see the relation that make no sense (in case you would fall on them without purpose...) :
	
	For the relation above, the rotational (curl) of a divergence does not exist since the rotational operator applies to a vector field while the divergence is a scalar!
	
	For the above relation the rotational (curl) of a scalar Laplacian does not exist since the rotational operator applies to a vector field while by construction, the Laplacian is a scalar.
	
	Let us now see some remarkable identities without proof for the majority (if there is a proof this is because one reader did the request to have all the details...):
	\begin{enumerate}
		\item By construction the scalar Laplacian is the divergence of the gradient of the scalar field:
		
		
		\item The rotational (curl) of the gradient is equal to zero:
		
		Therefore if the rotational (curl) of a vector variable (vector field) is zero, this same variable can be expressed as the gradient of a scalar potential!!!!!!!!!!!! This is a veeeeerrrrry important property (or trick depending of the point of view...) in Electromagnetics and Fluid Mechanics and Quantum Physics!
		\begin{dem}
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item The dot product of two rotational is equal to something boring to say just with words...:
		
		
		\item The divergence of the rotational (curl) of a vector field is always equal to zero:
		
		\begin{dem}
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item The rotational (curl) of the rotational  of a vector field is equal to the gradient of the divergence of this vector field less its vector Laplacian:
		
		\begin{dem}
		
		It is then easy to check that this last equality is equal to:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item The multiplication of the nabla operator by the dot product of two vectors is equal to ... (see below), which provides a very useful relation in Fluid Mechanics:
		
		
		\item The scalar product of the rotational (curl) of a vector is the difference of the commutated operators such that (we can provide the detail proof on request):
		
		We will use this last relation in our study of electromagnetic radiation pressure in the section of Electrodynamics (among others...).
		
		\item The gradient of a cross product is the difference of the commutated operators such that (we can provide the detail proof on request)
		
		We will use this last relation in our study of superconductors in the section of Electrokinetics page \pageref{superconductivity}.
		
		\item Let us consider a vector $\vec{v}$ which each component depends on $x,y,z$. We have (the left term is named the "material derivative", see section Continuum Mechanics page \pageref{material derivative}):
		
		Thus explicitly:
		
		Or even more explicit:
		
		So now to prove that the equality holds, we will develop the left part and right part. So let us begin with the left part:
		
		Now let us develop the right part:
		
		and then we see that the equality holds!
	\end{enumerate}
	
	\pagebreak
	\subsubsection{Summary}
	As part of this book, we will use the different notations presented and summarized in the table below. Their usage gives us the possibility in the context of the different theories to avoid confusion with other mathematics being (tools). It's annoying but we have to do with it.
	
	\begin{table}[H]
		\begin{center}
			\definecolor{gris}{gray}{0.85}
				\begin{tabular}{|p{8cm}|p{6cm}|}
					\hline
					\multicolumn{1}{c}{\cellcolor{black!30}\textbf{Type}} & 
	  \multicolumn{1}{c}{\cellcolor{black!30}\textbf{Expression}} \\ \hline
					Gradient of a scalar field $A:\mathbb{R}\mapsto \mathbb{R}^3$ & \centering\arraybackslash\ $\vec{\nabla}(f) \quad \vec{\nabla}\cdot f \quad \vec{\nabla}f \quad \overrightarrow{\text{grad}}(f)$ \\ \hline
					Gradient of a vector field $A:\mathbb{R}^n\mapsto \mathbb{R}^n\times \mathbb{R}^n$ & \centering\arraybackslash\ $\vec{\nabla}(\vec{f})$  \\ \hline
					Divergence of a vector field $A:\mathbb{R}^n\mapsto \mathbb{R}$ & \centering\arraybackslash\ $\vec{\nabla}\circ \vec{f}\quad \text{div}(\vec{f})$  \\ \hline
					Rotational of a vector field $A:\mathbb{R}^3\mapsto \mathbb{R}^3$ & \centering\arraybackslash\ $\vec{\nabla}\times\vec{f}\quad \text{rot}(\vec{f})$  \\ \hline
					Lapalacian of a scalar field $A:\mathbb{R}^n\mapsto \mathbb{R}$ & 		\centering\arraybackslash\ $\Delta \cdot f\quad \vec{\nabla}^2 f\quad \vec{\nabla}\circ(\vec{\nabla}\cdot f)$  \\ \hline
					Laplacian of a vector field $A:\mathbb{R}^3\mapsto \mathbb{R}^3$& \centering\arraybackslash\  $\Delta\vec{f}$  \\ \hline
			\end{tabular}
		\end{center}
		\caption{Summary of vector differential operators}
	\end{table}
	Now let us do a quick summary of main differential operators:
	\begin{itemize}
		\item The gradient can be assimilated to the "slope" (example: the electric field is the "slope" of the electrostatic potential).
		
		The various expressions of the gradient operator (placed under the form of the nabla operator) in Cartesian, polar, cylindrical, spherical coordinates are the following:
		
		
		\item The divergence characterizes a flow of something that comes from somewhere, a source, or who goes to it. If the divergence is different from zero, it means that there is concentration around a point, so the density increases (or decreases, it depends on the sign). It could be the density of electric charges or the mass density. Hence the famous theorem that says that the flow (that which passes through a surface) is equal to the integral of the divergence (what remains).
		
		The various expressions of the divergence operator (placed under the form of the nabla operator) in Cartesian, polar, cylindrical, spherical coordinates are the following:
		
		
		\item The rotational characterizes the existence of a vortex (Widely used in fluid mechanics). If there is a whirlwind, we can follow a flow line on a closed curve (closed: in the differential point of view, not in the geometrical one!) without it change of direction: the circulation will the not be equal to zero (it is equal to integral of the rotational (curl)).
		
		The various expressions of the rotational (curl) operator (placed under the form of the nabla operator) in Cartesian, cylindrical, spherical coordinates are the following:
		
		
		\item The Laplacian of a scalar field gives a scalar field that measures the difference between the value of the function at a point and its average around that point. In other words, the partial second derivative measure the variations of the slope at the point examined in the immediate surroundings and in one dimension at a time. If the partial second derivative is zero in one direction, then the slope is constant in the immediate surroundings and according to this dimension, this means that the value of the function on the study is equal to the average of his neighborhood (in one dimension).
		
		The different expressions of the scalar Laplacian operator (placed under the form of the nabla operator) in Cartesian, polar and spherical coordinates are:
		
		
		\item As for the Laplacian of a scalar field, the Laplacian of a vector field is only a very convenient notation system for condensing the writing of the components of a vector field.
		
		The different expressions of the scalar Laplacian operator (placed under the form of the nabla operator) in Cartesian, polar and spherical coordinates are:
		
	\end{itemize}
	And also we have the following list of remarkable identities:
	
	
	\pagebreak
	Finally let us finis this summary with all the theorem that we have obtained so far in this section that are named "\NewTerm{$1$st order integral theorems}\index{first order integral theorems}":
	\begin{itemize}
		\item Gradient theorem (only for uniform field):
		
	
		\item Ostrogradsky theorem (divergence theorem):
		

		\item Green theorem (Green-Riemann theorem):
		
	
		\item Stokes theorem (curl theorem):
		
	\end{itemize}
	Keep in mind that there are all special case of the Stokes (general manifold theorem\footnote{If one day we have the time to write the \LaTeX to provide a detailed example we will! But actually that doesn't bring too much interest for an engineer.}) including the fundamental theorem of calculus!
	
	Then the fundamental theorem of calculus (1D), Green's theorem (2D) and the divergence theorem (3D) (ie Stokes original theorem) can all be stated respectively by the following parametrised sentence (still special case of Stokes general manifold theorem): The total [change, spin, flow] outside is equal to the sum of little [changes, spins, flows] on the inside.
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{95} & \pbox{20cm}{\score{4}{5} \\ {\tiny 99 votes,  84.44\%}} 
	\end{tabular} 
	\end{flushright}
	
	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Linear Algebra}\label{linear algebra}

	\lettrine[lines=4]{\color{BrickRed}T}here are several approaches to learn Linear Algebra. First a pragmatic way (we'll start with this one because our experience has shown us that it is the one that seemed to work best for students) and a more formal way that we will present in after. We should first warn the reader that linear algebra is a powerful calculation tool that we use enormous in economic and industrial practice in the following areas (see the respective chapters of the book for specific examples): Statistics , Electrotechniques, Finance market, Numerical optimization methods, Optics, Quantum Physics, Electrodynamics, Relativity, Fluid mechanics, etc. It is then necessary to attach a special attention to this subject.
	
	First let us answer at  two questions from a reader: Why Linear Algebra is named like this? And is there a Non-Linear Algebra?
	
	Here are my answers:

	\begin{enumerate}
		\item This is named "Linear Algebra" because it was first necessary to choose a name... and also because it is a generalization of the scalar algebra but with vectors where applications are no longer scalar functions but matrix applications whose effect is to act as a linear sum of a base vectors (at least this can be interpreted as such).
		\item Officially and to my knowledge there is no non-linear algebra in the same philosophy as what we will see in this section. It seem that they are mathematicians who have created "non linear algebra" but these have nothing to do with matrices.
	\end{enumerate}

Now, remember that we studied in section Calculus how to determine the intersection (if any exist) of the equation of two lines in $\mathbb{R}^2$  (we can extend the problem obviously to more than two lines), as it is equivalent to solve a polynomial of order $1$, given by:
	
where $a_i,b_i \in \mathbb{R}$.

Thus seeking the value for which:
	
leads us to write:
	
However, there is another way of presenting the problem as we have seen in the Numerical Methods section (\SeeChapter{Theoretical Computer chapter}). Indeed, we can write the problem in the form of a block of equations:
		
and as we seek $y_1=y_2=y$, we have:
		
This writing is named as we have seen in the chapter of Theoretical Computing (\SeeChapter{see section of Numerical Methods page \pageref{linear systems of equations}}) a "\NewTerm{linear system}\index{linear system}" that we can solve by subtracting or adding the lines between them (all the solutions are always equal), which gives us:
	
and we see that we fall back on the solution:
	
So there are two ways to present a problem of intersection of lines:
	\begin{enumerate}
		\item In the form of an equation
		\item In the form of a system
	\end{enumerate}
We will focus in a part of this section to the second method that will allow us with tools seen in the section of Vector Calculus to resolve not only the intersections of one or more straight lines but one or more straight, plans, hyperplanes, etc. in respectively $\mathbb{R},\mathbb{R^2},...,\mathbb{R^n}$. 

	Obviously we will see that Linear Algebra is not only use for this purpose but can also be used the generalized some physics mathematical models or to express geometrical transformations of vectors or figures in 2D or 3D or also express Markov Chains, special properties of Multivariate Statistics, calculation of some differential equation (for reliability engineering for example) and much more! Many examples are given in this book about the application of Linear Algebra in real life.

Before attacking the theoretical part, let us presented a very interesting example but that requires a concept - the determinant - that we will prove rigorously much further in detail in this section (it seemed to us more pedagogical to approach this subject now rather than the reader must wait to browse dozens of mathematical developments of pages before reaching the rigorous definition of the determinant).

Consider the system of two linear equations with two unknowns (system of intersection of planes):
	
If we solve this, we quickly get (technique named "\NewTerm{substitution method}\index{substitution method}"):
	
It comes then:
	
and so at the end:
	
and if we define a little bit fast something that is named the "\NewTerm{determinant}\index{determinant}" that we will see further below rigorously as follows:
	
or with another very more common notation:
	
we thus have:
	
And by proceeding in the same way we get:
	
It comes then:
	
and so finally we get:
	
It then appears clear that if:
		
the system has infinitely many solutions. In contrast, the system has no solution if:
	
And if the reader repeats (happily...) the procedure for a system of three equations with three unknowns of the type (intersection of hyperplanes):
	
We then get (after some basic boring algebraic operations):
	
with:
	
	It then appears clear that if:
	
	the system has infinitely many solutions. In contrast, the system has no solution if:
	
	and so on for $n$ equations with $n$ unknowns.

	However, there was a condition to satisfy: as we have seen in the previous example, we could not solve a system of equations with two unknowns if we have only one equation. That is why it is necessary and sufficient for a system of equations with $n$ unknowns to have at least $n$ equations. Thus, we speak of "\NewTerm{systems with $n$ equations in $n$ unknowns}\index{systems with $n$ equations in $n$ unknowns}". We also prove that it is necessary and sufficient that the determinant is non-zero for a linear system, whose matrix equivalent is square, to have a unique solution (the concept of "determinant" and "matrix" will be defined further below robustly) and therefore that the corresponding matrix to the system is invertible (non-singular).

\pagebreak
\subsection{Linear Systems}\label{linear systems}

\textbf{Definitions (\#\mydef):}
\begin{enumerate}
	\item[D1.] We name "\NewTerm{linear system}\index{linear system}", or simply "\NewTerm{system}" any family of equations of the form:
	
where each line represents the equation of a line, plane or hyperplane (\SeeChapter{see section Analytical Geometry page \pageref{equation of the plane}}), and $a_{mn}$ are the "\NewTerm{system coefficients}\index{linear system coefficients}", the $b_m$ the "\NewTerm{coefficients of the second member}\index{coefficients of the second member}" and the $x_n$ the "\NewTerm{unknowns of the system}\index{unknowns of the system}".

	\item[D2.] If the system has $n$ unknown and $n$ equations and has a unique solution, we then name it a "\NewTerm{Cramer system}\index{Cramer system}" (1750).\label{unique solution linear system}
	
	\item[D3.] If the coefficients of the second member are all zero, then we say that the system is a "\NewTerm{homogeneous system}\index{homogeneous system}" so it has at least the trivial solution where all $x_n$ are equal to zero. Otherwise we say that we have an "\NewTerm{inhomogeneous system}\index{inhomogeneous system}".
	
	\item[D4.] We name "\NewTerm{homogeneous system associated to the system}\index{homogeneous system associated to the system}", the system of equations we get by substituting zeros to the coefficients of the second member ($b_m$).
	\end{enumerate}
Let us now recall the following items:
	\begin{itemize}
		\item The linear equation (\SeeChapter{see section Functional Analysis page \pageref{straight line}}) is given by:
			
		by defining $x=x_1$ and $y=x_2$.
		
		\item The equation of a plane (\SeeChapter{see section Analytical Geometry page \pageref{equation of the plane}}) is given by:
			
		by defining $x=x_1, y=x_2, z=x_3$.
	\end{itemize}
We often write a linear system in the following condensed form:
	
We name "\NewTerm{system solution}\index{linear system solution}" or "\NewTerm{vector system solution}\index{vector system solution}" any $t$-uple $(x_1^0,x_2^0,...,x_n^0)$ such as:
	
Solve a system means finding all the solutions of this system (we find many such systems in economic, operational research or design of experiments). Two system with $n$ unknowns are named  "\NewTerm{equivalent systems}\index{equivalent linear systems}" if all of the solution of one system is the solution of the other, i.e., if they have the same set of solutions. We sometimes say that the equations of a system are "\NewTerm{compatible equations}\index{compatible equations}" or "\NewTerm{incompatible equations}\index{incompatible equations}", depending on whether the system has at least one solution or not admit any.

We can also give for sure a geometric interpretation to these systems. Suppose that the first members of the equations of the system are not zero. So we know that each of these equations represents a hyperplane of an affine space (\SeeChapter{see section Vector Calculus page \pageref{affine space}}) of dimension $n$. Therefore, all solutions of the system, considered as set of $n$-tuples of coordinates is a finite intersection of hyperplanes.

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}
	
	denoted conventionally in high-school classes in the form:
	
	This system has for solutions the points representing the intersection of the three planes defined by the three equations. But as we can see it visually with Maple 4.00b using the following commands:\\

	\texttt{>with(plots):}\\
	\texttt{>implicitplot3d({x-3*z=-3,2*x-5*y-z=-2,x+2*y-5*z=1},x=-3..3,}
	\texttt{y=-3..3,z=-3..3);}
	
	\begin{figure}[H]
	\centering
	\includegraphics[scale=0.35]{img/algebra/system_3_equations_3_unknowns.eps}
	\caption{Graphical representation of the linear system}
	\end{figure}
	
	There is no solutions. You can be checked by hand or with Maple 4.00b by writing:\\
	
	\texttt{>solve({x-3*z=-3,2*x-5*y-z=-2,x+2*y-5*z=1},{x,y,z});}
	\end{tcolorbox}

	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
For "classic" resolution methods of such systems, we refer the readers to the section on Numerical Methods of the chapter on Computing Science.
	\end{tcolorbox}	
	
	Finally, note an important case in practice: "\NewTerm{overdetermined systems}\index{overdetermined systems}" where we have more equations than unknowns. 
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.45]{img/algebra/overdetermined_system.jpg}
		\caption{Overdetermined system}
	\end{figure}
	The first situation of this type dates from the 18th century through the study of the lunar oscillations but we also find this frequently in R\&D laboratory in the context of design of experiments (\SeeChapter{see section Industrial Engineering page \pageref{doe}}) or in structural equation models (\SeeChapter{see section Numerical Methods}).
	
	An overdetermined system may be written as:
	
	where $X$ is $n \times m$ and $\operatorname{rank}(X \mid \vec y)>m ;$ that is, the system is not consistent. We have changed the notation slightly from the consistent system $A x=b$ that we have been using because now we have in mind statistical applications, and in those the notation $y \approx X \beta$ is more common. The problem is to determine a value of $b$ that makes the approximation close in some sense. In applications of linear systems, we refer to this as "fitting" the system, which is referred to as a "model".

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
Consider the special case but very telling following example of three equations with two unknowns:
	
system that will be written in the form of matrix and vector as following:
	\end{tcolorbox}

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	or said in an "\NewTerm{augmented form}\index{linear system augmented form}" as follows:
	
	and with Maple 4.00b:\\
	
	\texttt{>with(plots):}\\
	\texttt{>implicitplot({2*x+3*y=-1,-3*x+y=-2,-x+y=1},x=-3..3,y=-3..3);}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/algebra/overdetermined_system.eps}
		\caption{Representation of a system of 3 equations with two unknowns with Maple 4.00b}
	\end{figure}
	
	We can see on the chart above that this particular system has no complete solution but thus has a  solution if we take only the problem by pair of equations... which does not necessarily help in facts...
	\end{tcolorbox}

	Notice, that we have just seen that the system could be written as following:
	
	This looks like a multiple linear regression system (\SeeChapter{see section of Theoretical Computing page \pageref{linear systems of equations}}) whose column vector of unknowns can be viewed as the coefficient vector $\vec{\beta}$ of the regression line such that:
	
We then proved in detail in the section of Theoretical Computing that:
	

at the condition that the square matrix $X^TX$ is, as we will see further below, invertible (non-singular). Since the works of Gauss on the subject it is considered that this approach (hence implicitly based on ordinary least squares regression\footnote{Such that we found $\vec{\beta}$ that minimize $\|X\vec{\beta}-\vec{y}\|^2$}) remains even today the best approach to solve such systems!

	If the invertible property is satisfied, we find a "\NewTerm{pseudo-solution}\index{pseudo-linear solution}"\index{pseudo-solution} (this is the official terminology...) by making calculations quickly by hand (or with a spreadsheet software like Microsoft Excel):
	
and injecting these values in the initial overdetermined system, the reader will quickly understand why we talk about "pseudo-solution"...

	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
A reader asked us why we don't use for the example above just the relation $\vec{\beta}=\vec{y}X^{-1}$? The answer is simple! Because $X$ is not a square matrix or in other words... it is in our example over-determined this is why we can only found a "pseudo-solution".
	\end{tcolorbox}

This was the pragmatic way of looking at it ... now let us turn to the second slightly more mathematical way ... (but still relatively simple):

	\subsection{Linear Transformations}
	\textbf{Definition (\#\mydef):} A "\NewTerm{linear transformation}\index{linear transformation}" or "\NewTerm{linear application}\index{linear application}" $A$ is a mapping of a vector space $E$ to a vector space $F$ such that with $K$ being in $\mathbb{R}$ or in $\mathbb{C}$\label{linear application}:
	
	this constitutes, for recall, an endomorphism\label{matrix endomorphism} (\SeeChapter{see section Set Theory page \pageref{endomorphism}}).

	The first property specifies that the transformation of a sum of vectors must be equal to the sum of the transformed vectors, so that it is linear. The second property specifies that the transformation of a vector to  which we applied a scale factor (scaling) must also be equal to the same factor applied on the transformation of the original vector. If either of these two properties is not met, the transformation is therefore not linear.

	We now show that any linear transformation can be represented by a matrix!

	Given the basis vectors $\left(\vec{v}_1,\vec{v}_2,...,\vec{v}_n \right)$ of $E$ and $\left(\vec{u}_1,\vec{u}_2,...,\vec{u}_n \right)$ for those of $F$ with $m \geq n$. With these bases, we can represent any vectors $\vec{x} \in E, \vec{y} \in F$ with the following linear combinations (\SeeChapter{see section Vector Calculus page \pageref{linear combinations}}):
	
	Consider the linear transformation $A$ which applies $E$ to $F$ ($A:E\mapsto F$). So:
			
	that we can rewrite as follows:
	
	But since $A$ is a linear operator by definition, we can also write:
	
	Considering now that the vectors $A(\vec{v}_j)$ are elements of $F$, we can rewrite them as a linear combination of its basis vectors:
	
	Therefore, we get:
	
	By reversing the order of summations, we can write:
	
	and rearranging the latter relation, we produce the result:
	
	Finally, remembering that the basis vectors $\vec{u}_i$  must be independent, we can conclude that their coefficients must necessarily be zero, so:
	
	Which corresponds to the "\NewTerm{matrix product}\index{matrix product}":
	
	That we can write:
	
	In other words, any linear transformation can be described by a matrix $A$ that is multiplied with the vector that we want to transform, to obtain the vector resulting from the transformation.
	
	\pagebreak
	\subsection{Matrices}
	So we name a "\NewTerm{matrix}\index{matrix}" with $m$ rows and $n$ columns, or "\NewTerm{type $m\times n$ matrix}" (the first term always correspond to the rows and the second to columns to the second, to remember this there is a good mnemonic trick: President Lincoln - abbreviation of Lin(e) and Col(um)n ...), any array of numbers in a ring $\mathbb{K}$ (that is most of times $\mathbb{R}$):
	
	For example:
	
	is a $5\times 5$ matrix.
	
	We often denote a matrix of type $m\times n$ briefly by:
	
	or simply $(a_{ij})$. In a more formal way:
	
	
	The number $a_{ij}$ is named "\NewTerm{term or coefficient of index $i, j$}\index{term of a matrix}". The index $i$ is named the "\NewTerm{line index}\index{row index}" and the index $j$ the "\NewTerm{column index}\index{column index}".
	
	We denote by $M_{mn}(\mathbb{K})$ all matrices $m\times n$ whose coefficients take values in $\mathbb{K}$ (typically $\mathbb{R}$ or $\mathbb{C}$ for example).
	
	When $m=n$, we say that $(a_{ij})$ is a "\NewTerm{square matrix of order $n$}\index{square matrix}\label{square matrix}":
		
	In this case, the terms $a_{11},a_{22},...,a_{nn}$ are named "\NewTerm{diagonal terms}\index{matrix diagonal terms}" denoted by: 
	
	
	We will assign to the matrices special symbols, ie uppercase Latin letters: $A, B, ...$ for matrices and for columns-matrices symbols that will be vectorial lowercase letters $\vec{a},\vec{b},...$.
	
	We also name a matrix with a single row a "\NewTerm{line-matrix}\index{line -matrix}" and a matrix with a single column a "\NewTerm{column-matrix}\index{column-matrix}". It is clear that a matrix column is nothing but a "\NewTerm{column vector}\index{column-vector}" or simply a "\NewTerm{vector}\index{vector}" (\SeeChapter{see section Vector Calculus page \pageref{vector}}). Thereafter, the rows of a matrix will be treated as lines-matrices and the columns to columns-matrices.
	
	More formally, let $M\in \mathbb{R}^{m\times n}$ then a "\NewTerm{submatrix}" of $M$ is a matrix which is made of some rectangle of elements in $M$. Rows and columns are submatrices. In particular.
	\begin{itemize}
		\item An  $m\times 1$ submatrix is named a "\NewTerm{column vector}" of $M$. The $j$-the column vector is denoted $\text{col}_j(M)$ or $\vec{M}_j$ and $\vec{M}_{ji}=M_{ij}$ for $1\leq i\leq m$. In other words:
		
		
		\item An $1\times n$ submatrix of $M$ is named a "\NewTerm{row vector}" of $M$. The $i$-th row vector is denoted $\text{row}_i(M)$ and $(\text{row}_i(M))_j=M_{ij}$ for $1\leq j\leq n$ in other words:
		
	\end{itemize}
	
	We name "\NewTerm{zero matrix}\index{zero matrix}", and we denote by $0_{mn}$ or simply $\mathbf{0}$ or $\mathds{O}$ (this last notation is preferred in this book!) any matrix in which each term is zero:
		
	Null columns-matrices are designated by the symbol vector: $\vec{0}$.
	
	We name "\NewTerm{identity matrix of order $n$}\index{identity matrix}" or "\NewTerm{unit matrix of order $n$}\index{unit matrix}", and denote it by $I$, or simply $\mathds{1}$, the square matrix of order $n$:
	
	It can also be written using the Kronecker delta notation $\delta_{ij}$ (\SeeChapter{see section Tensors Calculus page \pageref{kronecker symbol}}).
	
	\begin{tcolorbox}[colback=red!5,borderline={1mm}{2mm}{red!5},arc=0mm,boxrule=0pt]
	\bcbombe Caution! When we work with matrices having complex coefficients we must always use the term "identity matrix" rather than "unitary matrix" because in the field of complex numbers the unitary matrix is another mathematical object that should not be confused!
	\end{tcolorbox}

	We will see later that the zero matrix acts as a neutral element of the matrix addition and the unit matrix as neutral element for the matrix multiplication.
	
	The $ij$-th "\NewTerm{standard basis matrix}\index{standard basis matrix}" for $\mathbb{R}^{m\times n}$ is denoted $E_{ij}$ for $1\leq i \leq m$ and $1\leq j\leq n$. The matrix $E_{ij}$ is zero in all entries except for the $(i,j)$-th slot where it has a $1$. In other words, we define $(E_{ij})_{kl}=\delta_{ik}\delta_{jl}$. Hence every matrix in $\mathbb{R}^{m\times n}$ is a linear combination of the $E_{ij}$. 
	
	Indeed, let $M\in \mathbb{R}^{m\times n}$ then:
	
	
	The purpose of the concept of matrix will appear throughout the texts that will follow but the immediate reason for this notion is simply to allow some finite families of numbers to be designed as a rectangular array and to generalize physics theorems to multidimensional spaces.
	
	\subsubsection{Rank of a matrix}\label{rank of a matrix}
	We will now briefly review the definition of "\NewTerm{rank of a finite family}\index{rank of a finite family}" that we saw in the section of Vector Calculus.
	
	As a reminder we name "\NewTerm{rank}" of a free family of vectors the dimension (positive integer number) of the vector subspace $S$ of $E$ that it generates.
	
	\textbf{Definition (\#\mydef):} Given $(\vec{a}_1,\vec{a}_2,...,\vec{a}_n)$ the columns of a matrix $A$, we name "\NewTerm{rank of $A$}\index{rank of a matrix}", and denote by $\text{rk}(A)$, the rank of the family of vectors $(\vec{a}_1,\vec{a}_2,...,\vec{a}_n)$. More precisely, in linear algebra, the rank of a matrix $A$ is the dimension of the vector space generated (or spanned) by its columns. This is the same as the dimension of the space spanned by its rows as we will see late. It is a measure of the "nondegenerateness" of the system of linear equations and linear transformation encoded by $A$ (see earlier above!). There are multiple equivalent definitions of rank. A matrix's rank is one of its most fundamental characteristics!
	
	In a slightly more familiar language (...) the rank of a matrix is given by the number of columns-matrices that can't be expressed by the combination and scalar multiplication of other columns-matrices of the same matrix!!
	
	Given this definition it is almost obvious that the rank of a matrix is zero if and only if the matrix is the zero matrix $\mathds{O}$.
	
	Before we enter in more formal calculations let us see already some introducing examples:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. The matrix:
	
	has rank $2$: the first two rows are linearly independent, so the rank is at least $2$, but all three rows are linearly dependent (the first is equal to the sum of the second and third) so the rank must be less than $3$.\\
	
	E2. The matrix:
	
	has rank $1$: there are nonzero columns, so the rank is positive, but any pair of columns is linearly dependent. Similarly, the transpose:
	
	of $A$ has rank $1$. Indeed, since the column vectors of $A$ are the row vectors of the transpose of $A$, the statement that the column rank of a matrix equals its row rank is equivalent to the statement that the rank of a matrix is equal to the rank of its transpose, i.e.:
	
	\end{tcolorbox}
	The above example show the statement that the column rank of a matrix equals its row rank is equivalent to the statement that the rank of a matrix is equal to the rank of its transpose.
	
	Before continuing we would like to indicate to the reader that later we will prove that if the rows of a matrix are independent, its determinant is non zero $\det(A)\neq 0$ and therefore $\text{rk}(A)=n$ and obviously $\text{rk}(A)=1$ when $\det(A)=0$.
	
	\textbf{Definition (\#\mydef):} A matrix is said to have "\NewTerm{full rank}\index{full rank}" if its rank equals the largest possible for a matrix of the same dimensions, which is the lesser of the number of rows and columns. A matrix is said to be "\NewTerm{rank deficient}\index{rank deficient}" if it does not have full rank.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	If there is difficulty in determining the rank of a matrix there is a technique named "\NewTerm{matrix scaling}\index{matrix scaling}" that we will see just below that can do this work very quickly.
	\end{tcolorbox}
	\textbf{Definition (\#\mydef):} We name "\NewTerm{system associated matrix}\index{system associated matrix}":
	
	The mathematical object define by:
	
	that is to say, the matrix $A$ in which the terms are the coefficients of the system. We name "\NewTerm{matrix of the second member of the linear system}\index{matrix of the second member of the linear system}", or simply "\NewTerm{second member of the system}", the matrix-column $\vec{b}=(b_i)$ whose terms are the coefficients of the second member of this system. We also name "\NewTerm{augmented matrix associated to a system}\index{augmented matrix associated of a system}" the matrix $A$ obtained by adding $\vec{b}=(b_i)$ as the $(n + 1)$-th column.
	
	If we now consider an associated  system matrix $A$ and of second member $\vec{b}$. Let us always denote the columns of $A$ by $(\vec{a}_1,\vec{a}_2,...,\vec{a}_n)$. The system can then be written equivalently as a linear vector equation:
	
	Now remember a theorem that we saw and proved in the section of Vector Calculus: for the rank of a family of vectors $(\vec{x}_1,\vec{x}_2,...,\vec{x}_n)$ to be the equal to rank of augmented family $(\vec{x}_1,\vec{x}_2,...,\vec{x}_n,\vec{y})$, it is necessary and sufficient that the vector $\vec{y}$ is a linear combination of the vectors $\vec{x}_1,\vec{x}_2,...,\vec{x}_n)$.
	
	It follows that our linear system in a vector form has at least one solution $(\vec{x}_1^0,\vec{x}_2^0,...,\vec{x}_n^0)$ if the rank of the family $(\vec{a}_1,\vec{a}_2,...,\vec{a}_n)$ is equal to the rank of augmented family $(\vec{a}_1,\vec{a}_2,...,\vec{a}_n,\vec{b})$ and this solution is unique if and only if the rank of the family $(\vec{a}_1,\vec{a}_2,...,\vec{a}_n)$ is equal to $n$.
	
	Thus, for a linear system matrix of associated matrix $A$ and of second member $\vec{b}$ admits at least one solution, it is necessary and sufficient that the rank of $A$ is equal to the rank of the augmented matrix $(A|\vec{b})$. If this condition is met, the system admits a unique solution if and only if the rank of $A$ is equal to the number of unknowns in other words: if the columns of $A$ are linearly independent!!!
	
	We say that a matrix is "staggered" if its rows (lines) meet the following two conditions:
	\begin{enumerate}
		\item[C1.] Any null line is followed by lines full of zeros
		
		\item[C2.] The leading coefficient (the first nonzero number from the left, also named the "\NewTerm{pivot}") of a nonzero row is always strictly to the right of the leading coefficient of the row above it 
	\end{enumerate}
	These two conditions imply that all entries in a column below a leading coefficient are zeros.
	
	A non-zero row echelon matrix is therefore of the form (by adding and subtracting rows between them):
	
	where $j_1<j_2<...<j_r$ and $a_{1j_1},a_{2j_2},...,a_{rj_r}$ are nonzero terms. The terminal zero lines may be missing.
	
	The columns of index $j_1,j_2,...,j_r$ of an echelon matrix are clearly linearly independent. Considered as vectors-columns of $\mathbb{R}^2$, so they form a basis of this vector space. Considerating the other columns also as vectors-columns of $\mathbb{R}^n$, we deduce that they are necessarily linear combination of those of index  $j_1,j_2,...,j_r$ and therefore that the rank of the echelon matrix $M$ is $\text{rk}(A)=r$.
	
	We will note that $r$ is also the number of nonzero lines of the echelon matrix and also the rank of the lines of this matrix, since the nonzero lines are therefore clearly independent (we proved in the section Vector Calculus that the rank of lines and columns has same value with the same properties of independence).
	
	We can therefore allow ourselves to do a given number of elementary (extra) operations on the lines of matrices that we will be very useful, without changing its rank:
	\begin{enumerate}
		\item[P1.] We can swap the lines.
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		As we know the matrix can be seen just an aesthetic graphic representation of a linear system. So swap two lines does not change the system.
		\end{tcolorbox} 
		
		\item[P2.] Multiply a row by a nonzero scalar.
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		This obvioiusly not altering the linear independence of the vectors lines.
		\end{tcolorbox} 
		
		\item[P3.] Adding to an original line, a multiple of another line.
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		The original line will disappear in favor of the new that is independent of all (former) others. The system thus remains linearly independent.
		\end{tcolorbox} 
	\end{enumerate}
	
	Any matrix can be transformed into a row echelon form by a finite sequence of the previous properties here's how.
	\begin{enumerate}
		\item Pivot the matrix
		\begin{enumerate}
			\item Find the pivot, the first non-zero entry in the first column of the matrix.
			\item Interchange rows, moving the pivot row to the first row.
			\item Multiply each element in the pivot row by the inverse of the pivot, so the pivot equals $1$.
			\item Add multiples of the pivot row to each of the lower rows, so every element in the pivot column of the lower rows equals $0$.				
		\end{enumerate}
		\item To get the matrix in row echelon form, repeat the pivot.
		\begin{enumerate}
			\item Repeat the procedure from Step 1 above, ignoring previous pivot rows.
			\item Continue until there are no more pivots to be processed.
		\end{enumerate}
		\item To get the matrix in reduced row echelon form, process non-zero entries above each pivot.
		\begin{enumerate}
			\item Identify the last row having a pivot equal to 1, and let this be the pivot row.
			\item Add multiples of the pivot row to each of the upper rows, until every element above the pivot equals $0$.
			\item Moving up the matrix, repeat this process for each row.
		\end{enumerate}
	\end{enumerate}
	It is therefore obvious that the elementary operations on the rows of a matrix do not change the rank of the rows of the matrix. However, we know that the rank of lines of a row matrix matrix is equal to the rank of the columns, that is to say to the rank of this matrix (once again see the section Vector Calculus for the proof). We conclude that the column rank of any matrix of the type $m\times n$ is also equal to the rank of the rows of this matrix.
	
	As a corollary of this conclusion, it appears that:
	
	When solving linear systems of $m$ equation with $n$ unknowns it appears, as we have already noted at the beginning of this section (and with practical example in the section of Theoretical Computing), there there must be at least an equal number of equations than unknowns or more rigorously: the number of unknowns must be less or equal to the number of equations such as:
	
	
	\pagebreak
	\subsubsection{Matrix Algebra}
	Remember that we saw during our study of Vector Calculus that the algebraic operations of multiplication of a vector by a scalar, addition or subtraction of vectors between them and the operation of scalar product formed in the context of set theory a "vector space" (\SeeChapter{see section Set Theory page \pageref{vector space}}) possessing therefore a "vector algebraic structure". This under the condition that the vectors of course have the same dimensions (this observation, for recall, is not valid if instead of the scalar product we take the cross product).
	
	Just as vectors, we can multiply a matrix by a scalar and add (subtract) them together (as long as they have the same dimensions..) but in addition, we can also multiply two matrices together under certain conditions which we will define below. This will also make the set of matrices in the set-sense, a vector space on $K$ (being most of time $\mathbb{R}$) having also therefore a "vector algebraic structure".
	
	Thus, a vector may also be viewed as a particular matrix of dimension $m\times n$  and operate in the vector space of matrices. Basically..., vector calculus is only a special case of linear algebra!!! This is way at school people learn (after Calculus) Vector Calculus first and Linear Algebra later (and some will lean after Tensor Calculus).
	
	\begin{enumerate}
		\item[D1.] Given $A,B\in M_{mn} (\mathbb{R})$. We name "sum of $A$ and $B$" the matrix $C\in M_{mn}(\mathbb{R})$ whose coefficients are:
		
		That is to say explicitly:
		
		
		\item[D2.] Given $A\in M_{mn} (\mathbb{R})$ a matrix and a $\lambda\in \mathbb{R}$ a scalar (we can also take in $\mathbb{C}$ if we want). We name "\NewTerm{product of $A$ by $\lambda$}" the matrix whose coefficients are:
		
		That is to say explicitly:
		
		In two previous definitions so we can actually conclude that the space / set of matrices is a vector space and thus has a vector algebraic structure.
		
		\item[D3.] Let $E, F, G$ be three  vector spaces of basis respectively $\mathcal{E},\mathcal{F},\mathcal{G}$ and two linear application $f$ and $g$ (see the section Set Theory for a refresh).
		
		Let us denote by $A$ the matrix of $f$ with respect to basis $\mathcal{E},\mathcal{F}$, and $B$ the matrix of $g$ with respect to basis $\mathcal{F},\mathcal{G}$. Then the matrix $C$ of $g\cdot g$ (see the definition of a composite function in the section of Functional Analysis) relating to the basis $\mathcal{E},\mathcal{G}$ is equal to the product of $B$ by $A$ denoted simply by $BA$.
		
		So let $B\in M_{mn} \in \mathbb{R}$ and $A\in M_{np} \in \mathbb{R}$, we name "\NewTerm{matrix product}\index{matrix product}" or "\NewTerm{matrix multiplication}" of $A$ and $B$ and we denote it by $BA$, the matrix $C\in M_{mp}\mathbb{R}$ whose components are:
		
		It is important to notice that at the opposite to the addition, $A$ and $B$ may have different dimensions. However! the number of rows of $A$ must be equal to the number of columns of $B$, as indicated by the index $n$ of the two matrices. So in the product $BA$, if $B$ is a matrix $m\times n$, $A$ must be a matrix $n\times p$, for any $p$!!!
		
		Schematically:
		{\Huge{
		\[
		\framebox[2.5cm]{\clap{\raisebox{0pt}[1.5cm][1.5cm]{$\mat C$}}\subdims{-2.5cm} n p} =
		\framebox[1.5cm]{\clap{\raisebox{0pt}[1.5cm][1.5cm]{$\mat B$}}\subdims{-2.5cm} m n} \ 
		\framebox[2.5cm]{\clap{\raisebox{5mm}[1.5cm]{$\mat A$}}     \subdims{-1cm} n p} 
		\]}}\\\\
		
		or even more explicity (\NewTerm{Falk's scheme}\index{Falk's scheme}):
		\begin{figure}[H]
		\centering
			\includegraphics[scale=0.9]{img/algebra/falks_scheme.jpg}
			\caption[Matrix Product Falk's scheme]{Matrix Product Falk's scheme (credit: Alain Matthes)}
		\end{figure}
	\end{enumerate}
	By denoting with a capital Latin letters matrices and with lowercase Greek letters scalars, the reader can easily verify with what we have seen \underline{until now} (we can add proofs on request) the following properties of matrix algebra (the matrices are assumed to have adequate dimensions)\label{non-commutativity matrices}:
	\begin{enumerate}
		\item[P1.] Left distributivity: $A(B+C)=AB+AC$
		\item[P2.] Right distributivity: $(A+B)C=AC+BC$
		\item[P3.] Scaling association: $(\lambda A)B=A(\lambda A)$
		\item[P4.] Associativity: $(AB)C=A(BC)$
		\item[P5.] Non-commutativity: $BA\neq AB$
		\item[P6.] Absorbing-Element: $A\mathds{O}=\mathds{O}$
		\item[P7.] Neutral Element for addition: $A+\mathds{O}=A$
	\end{enumerate}
	It is especially important to remember of the property P5 that shows that the multiplication is obviously not commutative (for dimensions greater than $1$ of course!) and also the property P4 such that the matrix multiplication is associative.
	
	Concerning the general proof that the assertion of commutativity if false we must pass through a numerical example (because even the general case without replacing algebraic terms by numerical values will not show you much in our point of view...).
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The set of square matrices $M_{nn}\in \mathbb{R}$ of order $n$ with components in $\mathbb{R}$ provided with the sum and the usual matrix multiplication forms a ring (\SeeChapter{see section Set Theory page \pageref{ring}}). This is true more generally if the coefficients of the matrices are taken in any ring: for example, all the matrices $M_{nn}\in \mathbb{Z}$  with integer components is a ring.
	\end{tcolorbox}
	One reader asked us to prove the property of associative. So let us begin!
	
	Let $A\in M_{kl}(\mathbb{C}),B\in M_{lm}(\mathbb{C}),C\in M_{mn}(\mathbb{C})$, then for $1\leq i\leq k,1\leq j\leq n$, we have well (we use the explicit expression of matrix components multiplication seen above many times as you can see):
	
	
	\subsubsection{Type of Matrices}
	To simplify the notations and length of calculations we introduce now the most common types of matrices that the reader will encounter throughout his reading of this book (and not just in the chapters on pure mathematics!).
	
	Some definitions will only be recalls!
	
	We denote by $M_{mn}(\mathbb{K})$ all matrices $m\times n$ whose coefficients take values in $K$ (typically $\mathbb{R}$ or $\mathbb{C}$ for example).
	
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] When $m=n$, we say that $(a_{ij})$ is a "\NewTerm{square matrix of order $n$}\index{square matrix}":
			

		\item[D2.] We name "\NewTerm{zero matrix}\index{zero matrix}", and we denote by $0_{mn}$ or simply $\mathbf{0}$ or $\mathds{O}$ (this last notation is preferred in this book!) any matrix in which each term is zero:
			
		Null columns-matrices are designated by the symbol vector: $\vec{0}$.	
	
		\item[D3.] We name "\NewTerm{identity matrix of order $n$}\index{identity matrix}" or "\NewTerm{unit matrix of order $n$}\index{unit matrix}", and denote it by $I$, or simply $\mathds{1}$, the square matrix of order $n$:
		
		It can also be written using the Kronecker delta notation $\delta_{ij}$ (\SeeChapter{see section Tensors Calculus page \pageref{kronecker symbol}}).
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		Even if we use it actually for only two practical applications in this book (Principal Component Analysis and Multidimensional Scaling), and that we don't found it often in other textbooks, the latter definition must not be confuse we the matrix named "\NewTerm{matrix of ones}\index{matrix of ones}", and defined for a square $n\times n$ matrix by:
		
		\end{tcolorbox}
		
		\item[D4.] We name "\NewTerm{diagonal matrix}\index{diagonal matrix}" any square matrix $A\in M_{nn}\in\mathbb{C}$ which only the diagonal has non-null elements:
		
		Formally:
		
		The usual notation of a diagonal matrix is:
		
		
		\item[D5.] We name "\NewTerm{lower triangular matrix}\index{lower triangular matrix}" a square matrix if all the entries above the main diagonal are zero:
		
		Formally:
		 
		 Similarly, a square matrix is named "\NewTerm{upper triangular matrix}\index{upper triangular matrix}" if all the entries below the main diagonal are zero:
		 
		Formally:
		 
		A "\NewTerm{triangular matrix}\index{triangular matrix}" is one that is either lower triangular or upper triangular. A matrix that is both upper and lower triangular is named a "diagonal matrix".
		
		If an upper triangular matrix was obtained by a "staggered" matrix we will write it as following:
		
		
		\item[D6.] A "\NewTerm{Toeplitz matrix}\index{Toeplitz matrix}\label{Toeplitz matrix}" or diagonal-constant matrix (named after Otto Toeplitz), is a matrix in which each descending diagonal from left to right is constant. For instance, the following matrix is a Toeplitz matrix:
		
		Any $n\times n$ matrix $A$ of the form:
		
		is a Toeplitz matrix. If the $i,j$ element of $A$ is denoted $A_{i,j}$, then we have:
		
		We will meet such a matrix in our study of time series analysis and especially when we will prove the Yule-Walker equations (\SeeChapter{see section Social Sciences page \pageref{Yule-Walker equations}}).
		
		\item[D7.] Given $M_{nn}$ a square matrix. The matrix $M_{nn}$ is named "\NewTerm{invertible matrix}\index{invertiable matrix}" or "\NewTerm{regular matrix}\index{regular matrix}" or "\NewTerm{non-singular matrix}\index{non-singular matrix}" if and only if $M_{nn}^{-1}$ is such that:
		
		If this is not the case, we say that $M_{nn}$ is a "\NewTerm{singular matrix}\index{singular matrix}". We will prove later that for a square matrix to invertible (non-singular) on sufficient condition is that its determinant is equal to zero.
		
		This definition is fundamental, it has extremely important consequences in all linear algebra and also in physics (solving linear systems, determining, eigenvectors and eigenvalues, etc.), statistics and finance and it is appropriate to remember it.
		
		Let us prove a useful property of invertible matrices associated with the property of associativity of matrix multiplication\label{inverse matrix property}:
		
		Indeed:
		
		
		\item[D8.] Given a matrix $A_{mn}:=A$:
		
		We name "\NewTerm{transposed matrix}\index{transposed matrix}\label{transposed matrix}" of $A=A_{mn}$, the matrix denoted by $A^T=A_{nm}$  (the superscript $T$ is depending of the books and teachers uppercase or lowercase and either left or right but the standard ISO 80000-2:2009 recommends the capital and superscript on the top right\footnote{Sadly many textbooks, especially from USA denoted the transpose with a prime: $A'$}) the matrix for which we transpose the rows into columns and the into rows:
		
		Here are some interesting properties of the transpose of a matrix (which we will be useful to us later in this section for a famous theorem and also in the study of the multiple linear regression methods in the section of Numerical Methods!):
		\begin{enumerate}
			\item[P1.] $(A^T)^T$
			\item[P2.] $(\lambda A+B)^T=\lambda A^T+B^T,\lambda\in \mathbb{R},\lambda\in \mathbb{R}$
			\item[P3.] $(AB)^T=B^TA^T$
			\item[P4.] $(A^{-1})^T=(A^T)^{-1},\exists A^{-1}$
			\item[P5.] $A\vec{x}\circ\vec{y}=\vec{x}A^T\vec{y}$
		\end{enumerate}
		The transposed matrix is very important in physics, statistics and finance and obviously in the field of mathematics for example in the context of the theory of groups and symmetries! So it is also worth remembering its definition.
		
		As the third property is the most used one in the various sections of this book let us demonstrate it by considering $A\in M_{lm}\in\mathbb{C},B\in M_{mn}\in \mathbb{C}$:
		\begin{dem}
		 Remembering the explicit relation of matrix multiplication seen earlier:
		
		But in this last equality, we note that we browse $B$ by line and $A$ in column for a $i$ and a $j$ fixed and this we know then corresponds to the matrix multiplication $AB$, therefore:
		
		Finally we have well:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		And for the same reasons let us prove the before last property.
		\begin{dem}
		First, it is trivial that if $A$ is invertible:
		
		and taking the transpose on both sides of the equality we find (we use the property proved just before):
		
		The latter equality shows obviously that $(A^{-1})^T$ is the inverse of $A^T$, that is to say:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		Finally, a last and important property of transposed matrices, is that given a matrix $A$ (square or not, symmetric or not), the multiplication by it's own transposed gives a symmetric matrix. 
		
		The proof is quite straightforward and very useful of the SVD (Singular Value Decomposition) theorem:
		
		
		\item[D9.] Given:
		
		a matrix of $M_{mn}(\mathbb{C})$. We name "\NewTerm{adjoint matrix}\index{adjoint matrix}\label{adjoint matrix}" of $A$, the matrix of $M_{mn}(\mathbb{C})$ defined by:
		
		which is the complex conjugate of the transposed matrix or if you prefer ... the transposed matrix of the complex conjugate (in the case of real components... we obviously don't need to take the conjugate!). To simplify the notations we simply note this matrix $A^\dagger$ (notation frequently use in Quantum Physics and Set Algebra).
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		Trivial relation (which is often used in Quantum Field Physics) already prove juste before and obviously right when the component are in $\mathbb{C}$:
		
		\end{tcolorbox}
		
		\item[D10.] By definition, a matrix is named "\NewTerm{Hermitian matrix}\index{Hermitian matrix}" or "\NewTerm{self-adjoint matrix}\index{self-adjint matrix}\label{self-ajdoint matrix}"... if it is equal to its own adjoint (conjugate transpose matrix) such that:
		
		
		\item[D11.] Given $A$ as square matrix $M_{nn}(\mathbb{R})$, the "\NewTerm{trace}\index{trace of a matrix}" of $A$ denoted $\text{tr}(A)$ is defined by the sum of the terms of the diagonal (very useful in some statistical techniques):
		
		Some useful related relations (we can add the detailed proof on the demand of the readers):
		
		and:
		

		\item[D12.] A matrix $A$ is named "\NewTerm{nilpotent matrix}\index{nilpotent matrix}" if by multiplying it successively by itself it can give zero. Explicitly, if there exists an integer $k$ such that:
		
		If the matrix $A$ multiplied by itself gives $A$ ... then we talk about an "\NewTerm{idempotent matrix}\index{idempotent matrix}".
		
		Such matrices are for example very common in Markov Chains when the transition matrix contains probabilities (see the sections of Probabilities and Graph Theory).
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		To remember this name, we can decompose it into "nil" that means "zero" and "potent" that means "potential". So something "nilpotent" is therefore something that is potentially zero....
		\end{tcolorbox}
		
		\item[D13.] A matrix $A$ is named "\NewTerm{orthogonal matrix}\index{orthogonal matrix}\label{orthogonal matrix}" if its elements are real and if it obeys to:
		
		which can be translate into (where $\delta_{ij}$ the Kronecker symbol):
		
		The matrix column vectors $A_i$ are thus orthogonal to each other as the resulting operation above can be seen as a row-column dot product such that for a $n\times n$ squared matrix:
		
		Therefore an orthogonal matrix represents also an orthogonal basis!
		
		\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
		When numerical data are stored in the usual waay in a matrix $X$, the matrix $X^TX$, and its properties often plays an important role in statistical analysis and especially in regression techniques. A matrix of this form is named a "\NewTerm{Gramian matrix}\index{Gramian matrix}".
		\end{tcolorbox}
		
		A typical mathematical example is the matrix of the canonical orthonormal basis (\SeeChapter{see section Vector Calculus page \pageref{canonical basis}}):
		
		or a well known matrix in quantum physics (\SeeChapter{see section Quantum Computing page \pageref{hadamard quantum gate}}):
		
		\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
		\textbf{R1.} Therefore this is typically the case of the canonical basis matrix, or any diagonalized matrix.\\
		
		\textbf{R2.} If instead of just taking a matrix with real coefficients, we take complex coefficients with its complex transposed matrix (adjoint matrix). So we say (sadly... because it makes confusion with the name of another matrix already define) that $A$ is a "unitary matrix" if it satisfies the previous relation!
		\end{tcolorbox}
		We will come back later, after having introduced the concepts of eigenvectors and eigenvalues, a particular and very important case of orthogonal matrices (named "translations matrices").
		
		Let us also mention another important property in geometry, physics and statistics of orthogonal matrices.
		
		\begin{theorem}
		Given $f(\vec{x})=A\vec{x}+\vec{b}$, where $A$ is an orthgonal matrix and $\vec{b}\in \mathbb{R}^n$. Then $f$ (respectively $A$) is an isometry. That is to say:
		
		So in other words: Orthogonal matrices are linear mappings which preserve the norm (the distance)!!!
		\end{theorem}
		\begin{dem}
		Remembering that $\vec{x}\circ\vec{y}$ is the same in linear algebra notation as $\vec{x}^T\vec{y}$ we then have:
		
		and we have well:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item[D14.] Given a square matrix $A \in M_{nn}$. The matrix $A$ is named "\NewTerm{symmetric matrix}\index{symmetric matrix}\label{symmetric matrix}" if and only if:
		
		We will meet this definition again in the section of Tensor Calculus.
		
		\item[D15.] Given a square matrix $A \in M_{nn}$. The matrix $A$ is named "\NewTerm{antisymmetric matrix}\index{antisymmetric matrix}" (also sometimes named "\NewTerm{skew-symmetric matrix}"...) if and only if:
		
		which requires that:
		
		In electromagnetism the electromagnetic field tensor has components which can be written as an antisymmetric $4\times 4$ matrix. In classical mechanics, a solid's propensity to spin in various directions is described by the inertia tensor which is symmetric. The energy-momentum tensor from electrodynamics is also symmetric. Most metric in Einstein field equations in General Relativity are symmetric but we can also build antisymmetric metrics. Matrices are everywhere if look for them.
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		We will prove in the section of Tensor Calculus (page \pageref{decomposition square matrix symmetric and antisymmetric}) that every square matrix are formed by the sum of a symmetric and antisymmetric matrix.
		\end{tcolorbox}
		
		
		\item[D16.] Let $E$ be a vector space of dimension $n$ and $ \mathcal{B},\mathcal{B}'$ two basis of $E$:
		
		We name "\NewTerm{transition matrix}\index{transition matrix}" of the basis $\mathcal{B}$ to the basis  $\mathcal{B}'$, and we denote by $P$ the square matrix of $M_{nn}(\mathbb{K})$ which columns are formed of components of vectors of the basis $\mathcal{B}'$ on the basis $\mathcal{B}$ (see further below the detailed treatment of basis changes for more information).
		
		We consider the vector $\vec{x}(x_1,x_2,...,x_n)$ of $E$ which is written in the basis $\mathcal{B}(\vec{e}_1,\vec{e}_2,...,\vec{e}_n)$ and $\mathcal{B}'(\vec{e}_1^{\prime},\vec{e}_2^{\prime},...,\vec{e}_n^{\prime})$ following the relations:
		
		With:
		
		the vector of $K^n$ formed of the components of $\vec{x}$ in the basis $\mathcal{B}$ and of the vector $\vec{x}$ formed of the components in the basis $\mathcal{B}^{\prime}$. So:
		
		relation for which the detailed proof will be given later in our study of basis changes. We also have obviously:
		
		\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
		\textbf{R1.} When a vector is given and its basis is not specified, remember that it is therefore implicitly in the canonical basis:
		
		which remains invariant by the multiplication by any vector and when the basis used is denoted by $(\vec{e}_i)$ and is not specified, then it is also that of the canonical basis.\\
		
		\textbf{R2.} If a vector is given relative to the canonical basis, its components are named "\NewTerm{covariant}\index{covariant components}\label{covariant components}",  if they are expressed in another noncanonical base, then we say that the components are "\NewTerm{contravariant}\index{contravariant components}" (for details on the subject see the section of Tensor Calculus).
		\end{tcolorbox}
		
		\item[D17.] A matrix is named "\NewTerm{positive-definite matrix}\index{positive-definite matrix}\label{positive definite matrix}" (which will be useful in the section of Theoretical Computing for some important engineering techniques and in quantitative finance for qualitative estimation of the correlation matrix) if:
		
		or more simply if $\vec{x}\neq \vec{0}$:
		
		and "\NewTerm{semi-positive matrix}\index{semi-positive matrix}" or "\NewTerm{positive-semidefinite matrix}\index{positive-semidefinite matrix}\label{positive semidefinite matrix}" if:
		
		We have already proved that a semi-positive matrix has its eigenvalues which are all positive OR null, while if it is positive definite its eigenvalues are all positive AND not null (\SeeChapter{see section Analytical Geometry page \pageref{classification of conical by the determinant}}).
		
		Notice that this implies that if a matrix $A$ is invertible, then $B=A^TA$ is positive definite.

		Indeed (even if we have already indirectly seen this proof in the section of Statistics), $B$ is positive definite if for every non-zero $\vec{x}$ we have:
		
		Now if we have $A$ such that $B=A^TA$, then using the properties of transposed matrices:
		
		But:
		
		If $A$ in invertible, $\|A\vec{x}\|=0$ only when $\vec{x}=0$.
		
		\label{inverse definite positive matrix is also definite positive} Let us prove also that the inverse of a positive definite matrix is also positive definite (important property for the Mahalonobis distance seen at page \pageref{Mahalanobis distance} for example!).

		First, let us recall that if $A$ is positive definite, then we have proved (\SeeChapter{see section Statistics page \pageref{positive semi-definite matrix not always invertible}}) that it's determinant is never null and that therefore it's always invertible. 
		
		Secondly, we will prove further below at page \pageref{spectral theorem} during our study of spectral theorem that for a definite positive matrix $M$ we have:
		
		where $\lambda_i$ are the eigenvalues of $i$ (positive and non-null for definite positive matrices!). But also further below during our study of determinant at page \pageref{inverse determinant}, we will prove for a matrix $M$ that:
		
		Therefore:
		
		And as for a definite positive matrix we have all $\lambda_i>0$, then $M^{-1}$ is also positive definite since all $1/\lambda_i>0$ and hence $\det(M^{-1})$ is also strictly positive.
		
		We can extend the above definition so that in general a matrix $M$ is said to by:
		\begin{itemize}
			\item "\NewTerm{positive definite}" if for any vector $\vec{x}\neq\vec{0}$, $\vec{x}^TM\vec{x}>0$. This is sometimes denoted $M\succ \vec{0}$
			\item "\NewTerm{positive semidefinite}" if $\vec{x}^TM\vec{x}\geq 0$. This is sometimes denoted $M\succeq \vec{0}$
			\begin{itemize}
			\item "\NewTerm{nonnegative definite}" if it is either positive definite or positive semi definite\footnote{Nonnegative definite and positive semidefinite are the same but as you can reat the expression from time to time in some textbooks...}
			\end{itemize}
			
			\item "\NewTerm{negative definite}" if for any vector $\vec{x}\neq\vec{0}$ $\vec{x}^TM\vec{x}<\vec{0}$. This is sometimes denoted $M\prec \vec{0}$
			\item "\NewTerm{negative semidefinite}" if $\vec{x}^TM\vec{x}\leq\vec{0}$. . This is sometimes denoted $M\preceq \vec{0}$
			\begin{itemize}
				\item "\NewTerm{nonpositive definite}" if it is either negative definite or negative semi definite
			\end{itemize}
			\item "\NewTerm{indefinite}" if it is nothing of those.
		\end{itemize}
		
		\item[D18.] The "\NewTerm{adjugate}\index{adjugate}\label{adjugate}", "\NewTerm{classical adjoint}\index{classical adjoint}", or "\NewTerm{adjunct}\index{adjunct}" of a square matrix is the transpose of its cofactor (see further below page \pageref{cofactor}) matrix:
		
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Consider a $3\times 3$ matrix:
		
		Its cofactor matrix is:
		
		where:
		
		Its adjugate is the transpose of its cofactor matrix:
		
		\end{tcolorbox}
			
		\item[D19.] A symmetric square matrix having all it components being positive and only zeros on the diagonal is named a "\NewTerm{distance matrix}\index{distance matrix}\label{distance matrix}" or even sometimes a "\NewTerm{pairwise similarity matrix}\index{pairwise similarity matrix}" (we will meet several times this type of matrix in Data Mining techniques during our study of the Numerical Methods section). Most of times this matrix contains the distances, taken pairwise, between the elements of a set (indexed by the row and column number of the corresponding matrix). Depending upon the application involved, the distance being used to define this matrix may or may not be a metric.
		
		A famous type of such matrix is the Euclidean distance matrix. If $D_x^2$ is a Euclidean distance matrix and the points $x_{1},x_{2},\ldots ,x_{n}$ are defined on $m$-dimensional space, then the elements of $A$ are given by:
	
	where $\lVert \ldots \rVert _{2}$ denotes the $2$-norm on $\mathbb{R}^m$ and:
	

		
		\item[D20.] A matrix is named "\NewTerm{sparse matrix}\index{sparse matrix}" if it contains a significant number of null values. In numerical methods, there are algorithms that use this specificity to optimize the storage of this type of matrices (used in OLAP\footnote{Acronym of OnLine Analytical Processing.} cubes and financial engineering).
		
		The above sparse matrix contains only $9$ nonzero elements, with $26$ zero elements. Its sparsity is $74\%$, and its density is $26\%$.
		
		\item[D21.] A "\NewTerm{block matrix}\index{block matrix}"(also called partitioned matrix) is a matrix of the kind:
		
		where $A$, $B$, $C$ and $D$ are matrices, called "\NewTerm{blocks}", such that:
		\begin{itemize}
			\item $A$ and $B$ have the same number of rows
		
			\item $C$ and $D$ have the same number of rows
		
			\item $A$ and $C$ have the same number of columns
		
			\item $B$ and $D$ have the same number of columns
		\end{itemize}
		Ideally, a block matrix is obtained by cutting a matrix two times: one vertically and one horizontally. Each of the four resulting pieces is a block.
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Examples:}\\\\
		E1. The matrix:
		
		can be written as a block matrix:
		
		where:
		
		E2. The matrix:
		
		can be written as a block matrix:
		
		where:
		
		\end{tcolorbox}
		An important fact about block matrices is that their multiplication can be carried out as if their blocks were scalars, by using the standard rule for matrix multiplication:
		
		The only caveat is that all the blocks involved in a multiplication (e.g., $AE$, $BG$, $CE$) must be conformable. For example, the number of columns of $A$ and the number of rows of $E$ must coincide.
		
		Many proofs in linear algebra are greatly simplified if one can easily deal with the determinants of block matrices, that is, matrices that are subdivided into blocks that are themselves matrices.
	\end{enumerate}
	
	\subsubsection{Determinant}\label{determinant}
	We will look at determinants in the point of view of the physicist or of the engineer (the mathematician point of view is rather off-putting ...). In physics (whether in classical mechanics and quantum field physics), chemistry or engineering, we frequently have to solve linear systems. But we have now seen that a linear system:
	
	can be written as:
	
	and we know that the only soluble linear systems (in the sense that they have a unique solution !!!) are those that have as many equations as unknowns and their determinant is not zero! Thus, the matrix must be a square matrix $M_{mm}$.
	
	If a solution exists, then there is a column matrix (or "vector") $X$ such that $AX=B$ which involves:
	
	What imposed this relation? Well this is relatively simple, but at the same time very important: for a linear system to have a unique solution, it is necessary that the matrix is invertible (non-singular)! What relation with the  concept of "determinant" then? It's simple: mathematicians have sought how to write inverse matrices of linear systems for which they knew there was a unique solution and they arrived after trial and error to determine a kind of formula to assess if the matrix is invertible (non-singular) or not. Once this formula found, they formalized (as they know so well how to do it...), with a great rigor, the concept surrounding this formula that they named the "\NewTerm{determinant}\index{determinant}". They did it so well that in fact we sometimes forgot that they have found it by trial and error...
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	If a matrix of a linear system is not invertible (non-singular), this has the consequence that there is no solution or an infinity of solutions (as usual what ...)
	\end{tcolorbox}
	We below first focus on how to build the determinant bydefining a particular type of application. Then, after seeing a simple and readable example of the calculation of a determinant, we will focus on determining the formula of it in the general case. Finally, once this is done, we will see what is the relation between the inverse of a matrix and the determinant!!!
	
	In what will follow all vector spaces will be considered of finite dimension and the field of complex numbers $\mathbb{C}$ (those who prefer the can take $\mathbb{R}$ as basis field, in fact we could take any field).
	
	First of all we will do a little bit of pure math (a bit off putting) before moving on  concrete stuff.
	
	Given $V$ a vector space, we write will write as usual $V^n$ instead of $V\times V\times... \times V$. $(\vec{e}_1,\vec{e}_2,...,\vec{e}_3)$ designate the canonical basis of $\mathbb{R}^n$. $M_n(\mathbb{R})$ is the set of square matrices $n\times n$ with coefficients in $\mathbb{R}$.
	
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] A "\NewTerm{multilinear application}\index{multilinear application}" on a space $V$ is defined by an $\varphi: V^n \rightarrow \mathbb{R}$ which is linear in each of its components. Meaning:
		
		for any $\lambda,\mu\in \mathbb{K}$ and $\vec{x}_i,\vec{u},\vec{v}\in V$ where the $\vec{x}_i$ are vectors.
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		A non-null multilinear application is not a linear application of the space $V^n$ in $\mathbb{R}^n$. Excepted if $n=1$. Indeed, this can be verified by the definition of a linear application  versus the one of the multilinear application:
		
		\end{tcolorbox}
		\item[D2.] An "\NewTerm{alterneted multilinear application}\index{alterneted multilinear application}" on $V$ is by definition a multilinear application that satisfies:
		
		for any $j=1...n,\vec{x}_j\in V$. Therefore the permutation of two vectors that follows change the sign of $\varphi$.
		\begin{theorem}
		Therefore, if $\varphi$ is a multilinear alterneted application, then $\varphi$ is multilinear if and only if $\forall \vec{j}\in V,j=1...n$ we have:
		
		or in a more simple case:
		
		\end{theorem}
		\begin{dem}
		If $\varphi$ is alternated we have by definition:
		
		Therefore by rearranging:
		
		And if $\varphi$ is a multilinear application we can write:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		Now comes the interesting stuff:
		
		\item[D3.]  A "\NewTerm{determinant}\index{determinant}" is by definition a multilinear alterneted application:
		
		satisfying as well:
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		The columns of a matrix form $n$ vectors and we then see that a determinant $D$ on $\mathbb{R}^n$ inducte an application $D$ of $M_n(\mathbb{R})\mapsto \mathbb{R}$ (where ase we know $M_n(\mathbb{R})$ is the set of squatre matrices $n\times n$ with components in $\mathbb{R}$) defined by:
		
		where $\vec{m}_i$ is the $i$-th column of $M$. 
		\end{tcolorbox}	
	\end{enumerate}
	Let us study the case $n=2$. If $D$ is a determinant, for any vector:
	
	we have:
	
	As $D$ is multilinear, we have:
	
	as it is alterneted:
	
	it remains:
	
	and we have:
	
	And finally:
	
	In fact, we just prove that if the a determinant exists, it is unique and of the form indicated previously, we should also check that the defined application satisfy the properties of a determinant, but the latter is immediate.
	
	Thus, if:
	
	is a matrix we have then (notice the three common notations for the determinant that you can found in various textbooks and even in the actual one depending on the context and of the traditions):
	
	Let us give now a geometric interpretation of the determinant. Given $\vec{v}_1,\vec{v}_2$ two vectors of $\mathbb{R}^2$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/determinant_parallelelogram.jpg}
		\caption{Geometric interpretation of the determinant}
	\end{figure}
	The vector $\vec{w}$ is obtained by projecting $\vec{v}_1$ on $\vec{v}_2$ and we have therefore:
	The vector $\vec{w}$ is obtained by projecting $\vec{v}_1$ on $\vec{v}_2$ and we have therefore:
	
	The area of the above parallelogram is therefore:
		
	if:
	
	then:
	
	and finally:
	
	Therefore the determinant, in absolute value, represents the area of the parallelogram defined by the vectors $\vec{v}_1,\vec{v}_2$ when these vectors are linearly independent. We can generalize this result to a $n$-dimensional space, in particular, for $n=3$, the determinant of three linearly independent vectors represents the volume of the parallelepiped defined by these as we already prove it during our study of the mixed product in the section of Vector Calculus page \pageref{mixed product}.
	
	The more general case of the expression of the determinant is a little trickier to ascertain. This requires that we define a particular bijective application but simple that we have already met in the section Statistics.
	
	\textbf{Definition (\#\mydef):} Given $F_n=\left\lbrace 1,2,...,n\right\rbrace,n\in \mathbb{N}^{*}$ we name "\NewTerm{permutation}\index{permutation}" of $F_n$ any bijective application of $F_n$ in $F_n$:
	
	Given $\mathcal{S}_n$ the set of possible permutations (bijective applications) of $\left\lbrace 1,2,...,n\right\rbrace$.  $\mathcal{S}_n$ obviously contains ... (see our study of Combinatorics in the section of Probabilities) $n!$ elements. The information an element $\sigma$ of $\mathcal{S}_n$ is defined by the successive information of:
	
	Given an ordered sequence of elements (ascending) $\left\lbrace 1,2,...,n\right\rbrace,n \in \mathbb{N}^{*}$ we name "inversion", any permutation of elements in the ordered sequence (so the result will not be ordered at all...). We denote by $I(\sigma)$ the number of inversions.
	
	We say that the permutation $\sigma$ is even (odd) if $I(\sigma)$ is even (odd). We name "\NewTerm{signature}\index{signature of a matrix}" of $\sigma$, the number $\varepsilon(\sigma)$ defined by $\varepsilon(\sigma)=(-1)^{I(\sigma)}$, that is to say:
	
	We now have the necessary tools to set up the general relation of the determinant:
	
	\textbf{Definition (\#\mydef):} Given:
	
	We name "\NewTerm{determinant of a square matrix $A$}\index{determinant of a square matrix}" of dimension $n$, and we denote by $\det (A)$ (or sometimes by $D(A)$ or especially in the field of Statistics by $|A|$), the scalar defined by (we'll see a few examples further below):
	
	sometimes named "\NewTerm{Leibniz formula of determinants}\index{Leibniz formula of determinants}\label{leibniz formula}" or "\NewTerm{Laplace's formula}\index{Laplace's formula}". This relation seems to have been obtained in the past by trial and error and by induction for larger dimensions.
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Given $A=(a_{ij})_{1\leq i,j\leq 2}\in M_{22}(\mathbb{K})$, let us consider the $2!=2$ permutations of the second indices (of integers $1,2$) taken in their whole\label{determinant of two by two matrix}:
	
	We calculate the signature of $\sigma$. Here is the scheme of this rule (recall: we say that there is an "inversion", if in a permutation, a greater integer precede a smallest integer):
	\begin{table}[H]	
		\begin{center}
			\begin{tabular}{|c|c|c|}
			\hline
			  \rowcolor[gray]{0.75}$\sigma(1)\sigma(2)$ & $\underbrace{12}_{\sigma(1)\sigma(2)}$ & $\underbrace{21}_{\sigma(1)\sigma(2)}$ \\
			  \hline
			  \cellcolor{black!30}Number of inversions & $0$ & $1$ \\\hline
			  \cellcolor{black!30}Permutation & even & odd  \\\hline
			  \cellcolor{black!30}$\varepsilon(\sigma)=(-1)^{I(\sigma)}$ & $+1$ & $-1$ \\\hline
			\end{tabular}
		\end{center}
		\caption{Inversions and permutations of a determinant of order $2$}
	\end{table}
	Therefore we have:
	
	This corresponds well to what we saw initially. Remember also on the way that we will soon prove that the determinant of a square matrix must be zero so that the matrix is invertible (non-singular)!\\
	
	E2. Given $A=(a_{ij})_{1\leq i,j\leq 3} \in M_{33}(\mathbb{K})$, let us consider the $3!=6$ permutations of the second indices (integers $1,2,3$) taken in their whole\label{determinant of three by three matrix}:
	\begin{align*}
	123 \quad 132 \quad 213 \quad 31 \quad 312 \quad 321
	\end{align*}
	We calculate the signatures of $\sigma$. Here is a scheme of this rule (recall: we say that there is an "inverse", if in a permutation, a greatest integer precedes a lower integer):
	\begin{table}[H]	
		\begin{center}
			\begin{tabular}{|c|c|c|c|c|c|c|}
			\hline
			  \rowcolor[gray]{0.75}$\sigma(1)\sigma(2)$ & $123$ & $132$ & $213$ & $231$ & $312$ & $321$\\
			  \hline
			  \cellcolor{black!30}Number of inversions & $0$ & $1$ & $1$ & $2$ & $2$ & $3$\\\hline
			  \cellcolor{black!30}Permutation & even & odd & odd & even & even & odd  \\\hline
			  \cellcolor{black!30}$\varepsilon(\sigma)=(-1)^{I(\sigma)}$ & $+1$ & $-1$ & $-1$ & $+1$ & $+1$ & $-1$\\\hline
			\end{tabular}
		\end{center}
		\caption{Inversions and permutations of a determinant of order $3$}
	\end{table}
	\end{tcolorbox}
	
	\pagebreak
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	Therefore we have\label{3x3 matrix determinant}:
	
	From the above formula, each term is in the form of $a_{1j_1}a_{2j_2}a_{3j_3}$ where the second indices $j_1,j_2,j_3$ is a permutation of $1,2,3$. For example, the first term has $1,2,3$ as the second indices; the second term has $1,3,2$; the third term has $2,1,3$, etc.\\
	
	The sign of each term is the sign of the permutation. For example, the sign of $1,2,3$ is clearly $1$. For the second term, $1,3,2$ is obtained from $1,2,3$ by one transposition $2,3\rightarrow3,2$, so the sign is $-1$, etc.
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Some people learn by heart a method named "\NewTerm{rule of Sarrus}\index{rule of Sarrus}" to calculate the determinants of order three as the previous one given by:
	\begin{center}
	\begin{tikzpicture}
    \matrix [%
      matrix of math nodes,
      column sep=1em,
      row sep=1em
    ] (sarrus) {%
      a_{11} & a_{12} & a_{13} & a_{11} & a_{12} \\
      a_{21} & a_{22} & a_{23} & a_{21} & a_{22} \\
      a_{31} & a_{32} & a_{33} & a_{31} & a_{32} \\
    }; 

    \path ($(sarrus-1-3.north east)+(0.5em,0)$) edge[dotted] ($(sarrus-3-3.south east)+(0.5em,0)$)
          (sarrus-1-1)                          edge         (sarrus-2-2)
          (sarrus-2-2)                          edge         (sarrus-3-3)
          (sarrus-1-2)                          edge         (sarrus-2-3)
          (sarrus-2-3)                          edge         (sarrus-3-4)
          (sarrus-1-3)                          edge         (sarrus-2-4)
          (sarrus-2-4)                          edge         (sarrus-3-5)
          (sarrus-3-1)                          edge[dashed] (sarrus-2-2)
          (sarrus-2-2)                          edge[dashed] (sarrus-1-3)
          (sarrus-3-2)                          edge[dashed] (sarrus-2-3)
          (sarrus-2-3)                          edge[dashed] (sarrus-1-4)
          (sarrus-3-3)                          edge[dashed] (sarrus-2-4)
          (sarrus-2-4)                          edge[dashed] (sarrus-1-5);

    \foreach \c in {1,2,3} {\node[anchor=south] at (sarrus-1-\c.north) {$+$};};
    \foreach \c in {1,2,3} {\node[anchor=north] at (sarrus-3-\c.south) {$-$};};
  \end{tikzpicture}
	\end{center}
	We prefer in this book the general formulation of the determinant because applicable to all orders.
	\end{tcolorbox}
	Let's us see some properties and corollaries of this formulation of the determinant:
	\begin{enumerate}
		\item[P1.] Given a square matrix of order $n$, we do not change the value of its determinant if:
		\begin{enumerate}
			\item By performing an elementary operation on the columns of $M_n$.

			\item By performing an elementary operation on the rows of $M_n$.
		\end{enumerate}
		\begin{dem}
		If $M_n=(a_{ij})_{i,j=1,...,n}$ the $M_n$ is composed of $n$ column vectors:
		
		Doing an elementary operation on the columns of $M_n$ is equivalent to add $\lambda v_i,i \in \{1,...,n\}$ to one of the columns $v_n$ of $M_n$. Given $M_n^{\prime}$ the matrix obtained by adding $\lambda v$ to the $j-th$ column of $M_n$, we get:
		
		By multilinearity (finally the proof in not really difficult):
		
		and as the determinant is alternated:
		
		About the elementary operations on the rows we just need to consider the transpose (that is to cry such it is simple, but we had to thing about this trick).
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item[P2.] Given $M_n(\mathbb{K})$ a squared matrix of order $n$ and given $\lambda \in \mathbb{K}$:
		
		\begin{dem}
		As before, it is enough to simply noticed that if $v_1,...,v_n$ are the column vectors forming the matrix $M_n$ then $\lambda v_1,...,\lambda v_n$ are those that constitute $\lambda M_n$ and:
		
		The application being $n$-linear, we arrive at the equality:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item[P3.] Given is a square matrix of order $n$. We change the sign of the determinant of $M_n$ if:
		\begin{itemize}
			\item We permute two of its columns
	
			\item We permute two of its rows
		\end{itemize}
		\begin{dem}
		$M_n$ is constituted by $n$ vectors $v_1,..,v_n$. The determinant of $M_n$ is equal to the determinant of these $n$. Permute two columns of $M_n$ is same as permuting the two corresponding vectors. Let us suppose that the permuted vectors are the $i$-th and $j$-th, the determinant being an alternate application, we have:
		
	About the rows, we just have to consider the transposed of $M_n$ to arrive to the same result!
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item[P4.] Given $A,B\in M_n\in (\mathbb{C})$ then:\label{determinant product}
		
		As far as we know the proof can be done in at least two ways, the first is rather indigestible and abstract... so we will let it to mathematicians (...) even if it has the advantage of being general, the second one (much more easier) is to check this assertion for various squared matrices (the way engineers would do it...):
		\begin{dem}
		
		and:
		
		The calculations therefore produce results that are identical. We can check for square matrices of higher dimensions.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item[P5.] A square matrix $A\in M_n(\mathbb{C})$ is invertible (non-singular) if and only if $\det(A)\neq 0$ (this is the most important property among all).
		\begin{dem}
		If $A$ is invertible (non-singular), we have:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		Notice that as $\det(\mathds{1})=1$, if follow immediately that \label{inverse determinant}:
		
		As we already said it, this is the most important property of matrices as part of theoretical physics because if $A$ is a linear system, the calculation of the determinant indicates whether it has unique solutions or not. Otherwise, as we have already mentioned and study it, the system has no solution, or an infinity of solutions!
		
		We must consider an important special case! Given the following system:
		
		where $A\in M_n(\mathbb{K})\neq 0$ and $B\in M_n(\mathbb{K})\neq 0$ are to be determined. It is obvious that $A$ is invertible (non-singular) or not, the trivial solution is $A\cdot B=0$. However, let us imagine a case of theoretical physics where we have $A\cdot B=0$ but for which we know that $A\in M_n(\mathbb{K})\neq 0$ for which we impose $B\in M_n(\mathbb{K})\neq 0$. In this case, we must eliminate the trivial solution $B=0$. Furthermore, calculate the inverse (if it exists) of the matrix $A$ will bring us to nothing concrete except that $B=0$ which obviously does not satisfy us. The only solution is then to play such that the coefficients $a_{ij}$ of the matrix $A$ are such that its determinant is zero and therefore the matrix in invertible! The advantage? Just to have an infinite number of possible solutions (of $B$ then!) that satisfy $A\cdot B=0$. We will need this methodology in the section of Wave Quantum Physics, when we will determine the existence of antiparticles through the linearized Dirac equation. It must therefore be remember.
		
		\item[P6.] Two "\NewTerm{conjugated}\index{conjugated matrices}" matrices (be careful! it is not the "conjugate" in the complex sense) have the same determinant.
		\begin{dem}
		
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item[P7.] For any matrix $A\in M_n(\mathbb{C})$:
		\begin{dem}
		
		But as (trivial... simple product of all coefficients):
		
		As (trivial) $\varepsilon(\sigma^-1)=\varepsilon(\sigma)$ and that $x,y\in\mathbb{C}:\bar{x}\cdot \bar{y}=\overline{x\cdot y}$ (\SeeChapter{see section Numbers page \pageref{module product complex numbers}}) then we can write:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item[P8.] For any matrix $A\in M_n(\mathbb{R})$:
		
		\begin{dem}
		Well... it's the same as the previous property but without the conjugate values... In fact, we prove in the same way, the same property for $A\in M_n(\mathbb{C})$.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item[P9.] Given a matrix $A=(a_{ij})\in M_n(\mathbb{C})$ we denote by $A_{ij}$ the matrix obtained from $A$ by removing the $i$-th row and the $j$-th column (very important notation to remember for what will follow!!!). The $A_{ij}$ belongs therefore to $M_{n-1}(\mathbb{C})$. Then for any $i=1...n$:
		
		where the term:
		
		is named the "\NewTerm{cofactor}\index{cofactor}\label{cofactor}\footnote{From all the cofactors we can build a "\NewTerm{cofactor matrix}\index{cofactor matrix}" containing all $C_{ij}$.}" or "\NewTerm{partial determinant}\index{partial determinant}\label{partial determinant}", $\det(A_{ij})$ is named the "\NewTerm{minor}\index{minor}", and the whole boxed relation above is named the "\NewTerm{cofactor expansion theorem}\index{cofactor expansion theorem}".
		\begin{dem}
		For the proof let us define the application:
		
		It could be almost easy to see that $\varphi$ is multilinear (you just have to consider that $(-1)^{i+j}a_{ij}$ as a simple constant and after by extension of the definition of the determinant... too easy...).

		Let us show however that this application is alternated (in this case it is a determinant hat has all the properties of a... determinant!).
	
		Given $a_k,a_{k+1}$ two column vectors $A$ that follows each other. Let us suppose that $a_k=a_{k+1}$, we have to show in this case that $\varphi(A)=0$ (which comes from the definition of an alternate application).
	
		We have first (it is mandatory by the definition itself) if we don't erase any of the columns $j$ being $k$ or $k+1$:
		
		and we have obviously if we don't remove respectively the column $k$ and the column $k+1$:
		
		Therefore:
		
		It is therefore OK. The application $\varphi$ is alternated and multilinear, it is indeed well a determinant.
		
		We have just prove that $\varphi$ is a determinant and by unicity we have $\varphi(A)=\det(A)$ for any $A\in M_n(\mathbb{C})$.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		So finally we have seen now in details the concepts of cofactor that should permit the reader to understand the concept of "adjugate matrix" (or "comatrix") that we had introduce at the beginning of this section at the page \pageref{adjugate}.
		\end{tcolorbox}
		If we take the following $2\times 2$ matrix:
		
		we get:
		
		Or if we take the following $3\times 3$ matrix (already used in an examples earlier!):
		
		Then the cofactor matrix is given by:
		
		And we notice something that can be generalized: if $A$ is symmetric, then $C=C^T$.
	
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Let us see an example of this method by calculating the determinant of: 
		
		Let us develop the second line ($i=2$). We get:
		\end{tcolorbox}
		
		\pagebreak
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
			
		Let us develop following the first column for verification (we never know...):
		
		Or the reader can check with Maple 4.00 by using:\\
	
		\texttt{>with(linalg):}\\
		\texttt{>det(array([[1,2,3],[2,1,0],[1,-1,I]]));}\\
		
		The calculation determined above is therefore "exponential" as if for example we must calculate the determinant of a square matrix of order (dimension) $n=10$, then the determinant will be developed in a sum of $10$ terms, which each contains the determinant of a matrix of dimension $n=9$, which is a cofactor of the starting matrix. If we develop any of this determinant, we get a sum of $9$ determinants where each contains the determinant of a matrix of dimension $n=8$. At this level, there is therefore $90$ determinants of matrices of dimension $8$ to calculate. The process could continue until it remains only determinant of order $2$. And therefore we guess that the number of determinants of order $2$ is important!
		\end{tcolorbox}
		
		Notice that as the adjugate of the $2\times 2$ matrix:
		
		
		\item[P10.] As we will prove it during our study of the spectral theorem further below at page \pageref{spectral theorem}, for a positive definite matrix $M$ we have:
		
		where the $\lambda_i$ are the eigenvalues of the matrix $M$. And for a definite semi-positive matrix $M$ we have:
		
		 
	\end{enumerate}

	\textbf{Definition (\#\mydef):} Given $m,n$ two positive integers and $A$ a $m\times n$ matrix with coefficients in $\mathbb{C}$. For any $k\leq \min(m,n)$ a "\NewTerm{minor of order $k$}\index{minor}\label{minor}" is a determinant of the type:	
	
	with $1\leq i_1< \ldots \leq m$ and $1\leq j_1 <\ldots <j_k\leq n$.
	
	In the particular case of a matrix of order $n>1$ the definition is simpler: the minor $M_{ij}$ of the element $a_{ij}$ is the determinant of the matrix of order $n-1$ that we get by remove the row $i$ and the column $j$. Therefore, to calculate the minor of an element, we remove the line and the column to which the element belongs to, and we calculate the determinant of the remaining square matrix.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The Leibniz formula of determinants is not the way you computer libraries calculate determinants. It is more of a theoretical tool, in fact it is probably one of the slowest ways to calculate the determinant. Gaussian Elimination is a fast and relatively simple way to do such a calculation!
	\end{tcolorbox}
	
	\paragraph{Derivative of a Determinant relatively to a parameter}\label{derivative of a determinant}\mbox{}\\\\
	Let us see now a result that will be quite useful to us in the section General Relativity:

	Given a square matrix $n\times n$ with functions $g_{ij}:\mathbb{R}\mapsto \mathbb{R}$ that can be derivate at least one time. Let us put $g:=\det(G)$ with $G=(g_{ij})$. We want to calculate $\mathrm{d}_t g$. Given $g_i$ the $i$-th column vector of the matrix $G$. Let us use the formula:
	
	Knowing that the derivative of $g_{\sigma(1),1}\cdot \ldots \cdot g_{\sigma(n),n}$ is (derivative of $n$ products):
	
	Therefore we have:
	
	If we take a look closely to the first above sum, we notice that:
	
	where $g_1^{\prime}$ is the derivative of the vector $g_1$. Same for the following sums. Therefore:
	
	Let us develop again. Let us consider the term $\det(g_1^{\prime},g_2,\ldots,g_n)$ above. If we develop it relatively to the first column, we get:
	
	 Also, by developing the $j$-term of the above sum relatively to the $j$-th column, we get:
	
	If we put:
	
	We get:
	
	Which is written in tensor notation (\SeeChapter{see section Tensor Calculus page \pageref{tensor notation}}):
	
	We also have:
	
	where $b_{ji}$ is the coefficient being at the row $j$-th, columen $i$-th of the matrix $G^{-1}$. If we denoted $g^{ij}$ the coefficient $i,j$ of the matrix $(G^{-1})^t$ then:
	
	The expression of the derivative is then finally:
	
	which is written in tensor notation:
	
	This result, finally quite simple, we will be helpful to us in the section of Tensor Calculus to build the tools necessary for the study of General Relativity and in the context of the determination of Einstein field equations. It is therefore appropriate to remember it!
	
	\paragraph{Derivative of logarithm of a determinant}\label{derivative of logarithm of a determinant}\mbox{}\\\\
	Let $M \in \mathbb{R}^{n \times n}$ be a square matrix. For a function $f: \mathbb{R}^{n \times n} \mapsto \mathbb{R}$, define its derivative $f'$ as an $n \times n$ matrix where the entry in row $i$ and column $j$ is $\partial f/\partial m_{ij}$.

	For some functions $f$, the derivative $f'$ has a nice form. We will show now a matrix derivation that we will meet in the section of Numerical Methods during our study of Gaussian Mixture Model and Factor Analysis:
	
	Here, we restrict the domain of the function to $M$ with positive determinant (typically the case of variance-covariance matrices!). Notice that it is not intuitive that the derivative of a scalar function of a matrix is equal to a matrix...
	
	Before we get move to the proof, we need to recall some terms. For a matrix $M$:
	\begin{itemize}
		\item The $(i,j)$ minor of $M$, denoted $\det(M_{ij})$, is the determinant of the $(n-1) \times (n-1)$ matrix that remains after removing the $i$th row and $j$th column from $M$
	
		\item The cofactor matrix of $M$, denoted $C$, is an $n \times n$ matrix such that $C_{ij} = (-1)^{i+j} \det(M_{ij})$
	
		\item The adjugate matrix of $M$, denoted $\text{adj}(M)=C^T=\left((-1)^{i+j}\det(M_{ij})\right)^T_{1\leq i,j\leq n}$, is simply the transpose of $C$.
	\end{itemize}
	These terms are useful because they related to both matrix determinants and inverses. If $M$ is invertible, then as we have proved earlier above:
	
	so\label{eqn:inverse matrix determinant cofactor relation}:
	
	On the other hand, by the cofactor expansion theorem of the determinant proved earlier above (as for a symmetric matrix $C=C^T$):
		
	So by the derivative product rule:
	
	If $k \neq j,$ then $\frac{\partial m_{ij}}{\partial X_{ik}}=0,$ otherwise it is equal to $1$. This means that the first term above reduces to $C_{ik}$. For any $k$, the elements of $M$ which affect $C_{ij}$ are those which do not lie on row $i$ or column $j$. Hence, $\partial C_{ij}/\partial m_{ik}=0$ for all $j$! Therefore:
	
	Putting all this together with an application of the derivative chain rule, we get (using the for the last equality the relation \ref{eqn:inverse matrix determinant cofactor relation}):
	
	as required!
	
	\paragraph{Cramer's rule}\label{Cramer's rule}\mbox{}\\\\
	Let us first solve a simple general $2$ by $2$ linear system using substitution to see how Cramer's rule out pops (this will be considered as an informal proof obviously!).

	For this purpose we start with the following system (don't forget that such a system is traditionally written $A\vec{x}=\vec{b}$):
	
	Multiplying both sides of the first equation by $a_2$, and both sides of the second equation by $a_1$, then subtracting, we find that:
	
	Assuming that $a_1b_2-b_1a_2$ is not $0$, we find that:
	
	The formula for $x$ can be derived similarly so that we have:
	
	If we denoted $x$ by $x_1$ and $y$ by $x_2$, then we see that we have:
	
	where we define $A_k$ to be the $n\times n$ matrix obtained by replacing the $k$-th column of $A$ by the inhomogeneous term $\vec{b}$.
	
	The rules for $3\times 3$ matrices are similar. Given:
	
	which in matrix format is:
	
	Then the values of $x$, $y$ and $z$ (after some boring algebra) can be found as follows:
	
	Then denoting again $x$ by $x_1$, $y$ by $x_2$ and $z$ by $x_3$ we have again (don't forget that such a system is traditionally written $A\vec{x}=\vec{b}$):
	
	where we define $A_k$ to be the $n\times n$ matrix obtained by replacing the $k$-th column of $A$ by the inhomogeneous term $\vec{b}$.
	
	But now let us deal with the general proof. But before... some readers may argue that it is useless! In fact not really! They are some quite important applications in Tensor Calculus (for example the divergence of a tensor field that has some application in advanced General Relativity\footnote{Don't look for it in this book!}), or in computing derivatives implicitly\footnote{Actually also not needed to read this book entirely!}. But this are all quite abstract applications. Indeed! So the reader has to know that the most well known practical application is the use in forecasting and times series analysis in business (more generally in any advanced finance field) for the study of the partial autocorrelation coefficient of autoregressive process (through the Yule-Walker equations).
	
	\begin{theorem}
	If $A\vec{x}=\vec{b}$ is a linear system of equations with $\vec{x}=[x_1\; x_2\; \ldots\; x_n]^T$ and $A\in \mathbb{R}^{n\times n}$ such that $\det(A)\neq 0$ then we find the solutions:
	
	where we define $A_i$ to be the $n\times n$ matrix obtained by replacing the $i$-th column of $A$ by the inhomogeneous term $\vec{b}$.
	\end{theorem}
	
	As there a lot of way to make the proof, the best an easiest one according to us is that provided by PlanetMath.org and written by the pseudonyme rmilson the 2013-03-22 for which we provide here a simple copy paste (as everything seems perfect to us and very clever way to do the proof...!).
	\begin{dem}
	Since we assume that $\det(A)\neq 0$, by the properties of the determinant we know that $A$ is invertible.
	
	We claim that this implies that the equation $A\vec{x}=\vec{b}$ has a unique solution. Note that $A^{-1}\vec{b}$ is a solution since:
	
	so we know that a solution exists.
	
	Let $\vec{s}$ be an arbitrary solution to the equation, so $A\vec{s}=\vec{b}$. But then:
	
	so we see that $A^{-1}\vec{b}$ is the only solution.
	
	For each integer $i$, $1\leq i\leq n$, let $a_i$ denote the $i$th column of $A$, let $e_i$ denote the $i$th column of the identity matrix $I_n$, and let $X_i$ denote the matrix obtained from $\mathds{1}_n$ by replacing column $i$ with the column vector $\vec{x}$.

	We know that for any matrices $A$, $B$ that the $k$th column of the product $AB$ is simply the product of $A$ and the $k$th column of $B$. Also observe that:
	
	for $k=1,...,n$. Thus, by multiplication, we have:
	
	Since $X_i$ is $\mathds{1}_n$ with column $i$ replaced with $\vec{x}$, we will compute the determinant of $X_i$ with the cofactor expansion gives. For this, let us recall the cofactor expansion relation that we have just proved earlier and that was for recall:
	
	can be used to calculated the determinant only for the cofactor (row) $j=i$ such that for $X_i$:
	
	If we choose to calculate this sum only for a given row $i$, we know that $x_{ij}\neq 0$ only for $i=j$, then the previous relation reduce to:
	
	Thus by the multiplicative property of the determinant:
	
	we get:
	
	and if we denote $M_i$ by $A_i$ we get the famous "\NewTerm{Cramer's rule}\index{Cramer's rule}":
	
	as required!
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\paragraph{Determinant Cofactor and Matrix Inverse}\label{determinant matrix inverse}\mbox{}\\\\
	Let us finish our study of the determinants with the "icing on the cake" by giving a very important relation in many fields of engineering, physics and mathematics that connects the inverse of the coefficients of a matrix with miners of order $n$ (we will use this relation further below).
	
	Given $A\in M_n(\mathbb{C})$ an invertible matrix (non singular). Let us write $A=(a_{ij})$ and $A^{-1}=(b_{ij})$. Then:
	
	\begin{dem}
	Let us denote by $a_k$ the $k$-th column vector of the matrix $A$. Knowing that $A\cdot A^{-1}=\mathds{1}$ (under known assumptions), we have (trivial):
	
	Let us calculate now $\det(a_1,\ldots,a_{k-1},e_j,a_{k+1},\ldots,a_n)$. First by developing relatively to the $k$-the column we found (as only one of the coefficient of $e_j$ is not null and that the unique non-null one is equal to the unit):
	
	Furthermore (properties of the determinant):
	
	Therefore:
	
	That is to say\label{inverse matrix}:
	
	That later relation is sometimes denoted:
	
	where "adj" means adjacent.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\label{some matrix inverse}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	E1. Inverse of a general $2\times 2$ matrix:
	
	
	E2. Inverse of a general $3\times 3$ matrix:
		
	
	E3. Inverse of a general $4\times 4$ matrix:
	
	and:
	
	then there exists an inverse matrix of $A$, and it is:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	where:
	
	\end{tcolorbox}
	
	For a simple, detailed, and important practical application in the industry (because otherwise in this entire book we will rarely inverse small matrices), the reader can refer to the section of Theoretical Computing in the part concerning the multiple linear regression.
	
	Let us also indicate the following important properties where $A$ and $B$ are both square invertible matrices $M_{n}(\mathbb{C})$ and $\lambda\in\mathbb{C}$ (the first should be obvious, the second has already been presented earlier but unproven and the third one is important for the proof of the variance inflation factor that we will prove in the section of Theoretical Computing on page \pageref{variance inflation factor}) :
	
	Let us prove the last property using the property of associativity:
	\begin{dem}
	
	Which prove that $B^{-1}A^{-1}$ is indeed the inverse of $AB$ where $I_n$ (also denoted $\mathds{1}$ for recall) is a diagonal matrix (also square) of dimension $n$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\paragraph{Determinant of block-diagonal or block-triangular matrices}\label{determinant of block-diagonal or block-triangular matrices}\mbox{}\\\\
	For our study of Factor Analysis we will need to compute the determinant of two particular type of block-triangular matrices.
	
	First we will need to compute the determinant of block-upper-triangular matrix of the form:
	
	where where $A$ and $D$ are square matrices. And secondly of a block-lower-triangular matrix of the form:
	
	
	However to calculate the determinant of these matrices we should first prove how to calculate the determinant of a diagonal block matrix of the following type:
	
	where $\mathds{1}$ is in this special case a $1\times 1$, that is, $\mathds{1}=1$. Suppose $A$ is $k \times k$. Then $\Gamma$ is of dimension $(k+1)(k+1)$.  
	
	For the proof, we will use the relation derived earlier above:
	
	\begin{dem}
	So we need to compute:
	
	where $\mathcal{S}$ is the set of all permutations of the first $k+1$ natural numbers.
	
	In the special case above we can rewrite this as:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Let us recall to help to understand what we just did that we are typically in a situation similar to the following below where $a_{33}=1,a_{31}=a_{13}=a_{23}=a_{32}=0$:
	
	From the above formula, each term is in the form of $a_{1j_1}a_{2j_2}a_{3j_3}$ where the second indices $j_1,j_2,j_3$ is a permutation of $1,2,3$. For example, the first term has $1,2,3$ as the second indices; the second term has $1,3,2$; the third term has $2,1,3$, etc.\\
	
	The sign of each term is the sign of the permutation. For example, the sign of $1,2,3$ is clearly $1$. For the second term, $1,3,2$ is obtained from $1,2,3$ by one transposition $2,3\rightarrow3,2$, so the sign is $-1$, etc.\\
	
	We see obviously that in the case $a_{33}=1,a_{31}=a_{13}=a_{23}=a_{32}=0$, the above relation reduces to:
	
	\end{tcolorbox}
	The result for the case in which $\mathds{1}$ is not $1\times 1$ is proved recursively. For example, if $\mathds{1}$ is $2\times 2$, we have:
	
	and analogously for larger dimensions.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	The proof for the case in which:
	
	is similar to the one just provided.
	
	Ok now that we have this result, let us prove the following determinant:
	
	
	\begin{dem}
	Let us assume that $A$ is $k \times k$ and $D$ is $l \times l$, so that $C$ is $l \times k$ and $\mathds{O}$ is $k \times l$. In what follows, we will denote by $\mathds{1}_k$ a $k \times k$ identity matrix and by $\mathds{O}_{lk}$ an $l\times k$ zero matrix. Note that (in the purpose to decompose the above matrix $\Gamma$ in product of triangular block matrices!):
	
	Thus, similarly to the previous proof with the diagonal block matrices and triangular block matrices:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\paragraph{Inverse and Determinant of a partitioned symmetric matrix}\label{inverse of a partitioned symmetric matrix}\mbox{}\\\\
	We will prove here a set of relations that we will need (once again...) for our study of Factor Analysis in the section of Statistics.
	
	Let us divide an $n\times n$ symmetric matrix $A$ into four blocks:
	
	The inverse matrix ${ B}={ A}^{-1}$ can also be divided into four blocks:
	
	Here we assume the dimensionalities of these blocks are:
	\begin{itemize}
		\item $A_{11}$ and $B_{11}$ are $p\times p$
		\item $A_{22}$ and $B_{22}$ are $q\times q$
		\item $A_{12}=A_{21}^T$ and $B_{12}=B_{21}^T$ are $p\times q$
	\end{itemize}
	with $p+q=n$. 
	
	Then what we want to prove first (and after we will be able to focus on the inverse determinant problem) is that the components of the inverse of a partitioned symmetric matrix are given by:
	
	ie (it is in this form that we found it in most textbooks):
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The term $A_{11}-A_{12} A_{22}^{-1} A_{21}$ seems to be named the "\NewTerm{Schur complement}\index{Schur complement}" of $A_{22}$ in $A$. Same for the term $A_{22}-A_{21} A_{11}^{-1} A_{12}$ that seems also be named "Schur complement" of $A_{11}$ in $A$.
	\end{tcolorbox}
	\begin{dem}
	We start with:
	
	Let us equate each of the four blocks to get:
	
	Plug $B_{21}$ into $B_{11}$ to get:
	
	Solve for $B_{11}$ to get:
	
	Applying the Woodburry matrix identity (see below page \pageref{Woodbury matrix identity}) given for recall by:
		
	to this expression we also get the other expression in the theorem, ie:
		
	Similarly we can get:
	
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Let us see now how to calculate the determinant of a partitioned symmetric matrix\label{determinant of a partitioned symmetric matrix}, given by:
	
	\begin{dem}
	First, let us consider a matrix of the form:
	
	where $A,B,C,D$ have size $n\times n$. We start from consider multiplication on the left by a matrix such that:
	
	Now let us consider the following left multplication:
	
	Therefore (starting from the beginning) in one line, that means we have:
	
	So that means after rearranging:
	
	And it is therefore quite immediate that:
	
	So that finally:
	 
	Similarly we get:
	
	To summarize:
	
	This is a result we needed before attacking the real deal!
	
	Indeed, according to the result above, this means that we can write:
	
	
	As seen above (page \pageref{determinant product}), we have:
	
	and (page \pageref{determinant of block-diagonal or block-triangular matrices}):
	
	Therefore the equality:
	
	is proved.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\pagebreak
	\subsection{Change of basis (frames)}\label{change of basis}
	A basis for a vector space of dimension $n$ is a sequence of $n$ vectors $(\vec{e}_1, …, \vec{e}_n)$ with the property that every vector in the space can be expressed uniquely as a linear combination of the basis vectors (\SeeChapter{see section Vector Calculus page \pageref{vector basis}}). The matrix representations of operators are also determined by the chosen basis! Since it is often desirable to work with more than one basis for a vector space, it is of fundamental importance in linear algebra to be able to easily transform coordinate-wise representations of vectors and operators taken with respect to one basis to their equivalent representations with respect to another basis. Such a transformation is named a "\NewTerm{change of basis}\index{change of basis}".
	
	Let us now suppose that we move from a frame $\mathcal{E}=(\vec{e}_1,\vec{e}_2,...,\vec{e}_n)$ of a space $V^n$ to another space $\mathcal{F}=(\vec{f}_1,\vec{f}_2,...,\vec{f}_n)$ of this same space sharing the same origin O. Thus in two dimension:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/algebra/basis_change.jpg}
		\caption[A vector can be represented in two different bases (purple and red arrows]{A vector can be represented in two different bases (purple and red arrows) (source: Wikipedia)}
	\end{figure}
	Let us decompose the $\vec{f}_i$ in the basis $\mathcal{E}$:
	
	\textbf{Definition (\#\mydef):} We name "\NewTerm{transition matrix}\index{transition matrix}" the matrix (linear application) that allows to pass from $\mathcal{E}\mapsto \mathcal{F}$ given by:
	
	\begin{theorem}
	Now let us consider the vector given by:
	
	So we intend to prove that the components of $y_1,y_2,...,y_n$ of $\vec{v}$ in the basis $\mathcal{F}$ are given by:
	
	Thus explicitly:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The matrix $P$ is invertible (non-singular), because its columns are linearly independent (they are the vectors $\vec{f}_i$ decomposed in the basis $\mathcal{E}$ and the $\vec{f}_i$ base and the $\vec{f}_i$ are linearly independent as they form a base!).
	\end{tcolorbox}
	\end{theorem}
	\begin{dem}
	Let us take the case to simplify the case $n=2$ (the proof being quite easily generalized) with $\mathcal{E}=(\vec{e}_1,\vec{e}_2)$ and $\mathcal{F}=(\vec{f}_1,\vec{f}_2)$.
	Then we have:
	
	We therefore have $\vec{v}=x^i\vec{e}_i$ and we seek to express $\vec{v}$ in the basis $\mathcal{F}$ as $\vec{v}=y^i\vec{f}_i$. We'll search the linear application that link these two relation such that:
	
	Thus written in an explicit way:
	
	Therefore:
	
	That is to say:
	
	So $P$ (if it exists) is indeed the matrix that can express the components of a vector of a basis in those of another basis such that we write in vector notation:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{theorem}
	Let us now consider a linear application $g:V^n\mapsto V^n$. Let $A$ be the matrix in the basis $\mathcal{E}$, and $B$ its matrix in the basis $\mathcal{F}$ (of same dimension). Then we might have:
	
	which is equivalent:
	
	or even:
	
	If there exists such a matrix $P$ satisfying these relations, we say that $A$ and $B$ are "\NewTerm{similar matrices}\index{similar matrices}".
	\end{theorem}
	\begin{dem}
	Let us take back the fact that we proved that it was eventually possible to build a transition matrix $P$ from the fact that:
	
	and let us put:
	
	We have then a function that bring us to write:
	
	On the other hand, we have (that we proved earlier):
	
	Therefore:
	
	hence:
	
	and as we saw it in our study of the determinant, the determinants of $A$, $B$ are equal and therefore invariant. We will  return later back on a similar formulation in our study of the Spectral Theorem below.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	At the vocabulary level we say when we are in the presence of a such a matrix relation that: the matrix $A$ is "\NewTerm{conjugated}" to the matrix $B$.
	
	\pagebreak
	\subsection{Eigenvalues and Eigenvectors}\label{eigenvector}
	\textbf{Definition (\#\mydef):} An "\NewTerm{eigenvalue}\index{eigenvalue}" is by definition (we will find again this definition in the introduction to quantum algebra in the section of Wave Quantum Physics) a value $\lambda$ belonging to a field $\mathbb{K}$ such that given a squared matrix $A\in M_{mm}(\mathbb{K})$ we have:
	
	and conversely a vector $\vec{X}\in M_{m1}(\mathbb{K})$ is an "\NewTerm{eigenvector}\index{eigenvector}" if and only if:
	
	The major advantage of these concepts will be able the possibility to study a linear application, or any other item linked to a matrix representation, in a simple representation through a basis change on which the restriction of $A$ is a single homothetic transformation (typically solving simple systems of differential equations). 
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	This definition can be generalized to functional analysis (therefore it does not only applied to Matrix Algebra) too as by defining the "\NewTerm{spectrum of an endomorphism}\index{spectrum of an endomorphism}\label{spectrum of an endomorphism}" as the set:
	
	\end{tcolorbox}
	Thus, all the eigenvalues of a matrix $A\in M_{mm}(\mathbb{K})$ is named "\NewTerm{spectrum of $A$}\index{spectrum of a matrix}" and satisfies the following homogeneous system named a "\NewTerm{matrix-eigenvalue equation}\index{matrix-eigenvalue equation}\label{matrix-eigenvalue equation}":
	
	or (whatever it is the same!):
	
	where $I_n$ (also sometimes denoted by the symbol: $\mathds{1}$) is for recall a diagonal unit matrix (and therefore also square) of dimension $n$. This system we know (proved above) has nontrivial solutions, therefore $\vec{X} \neq\vec{0}$ or $(\lambda I_n-A)\neq \vec{0}$, if and only if (we'll see many examples in various section related to physics in this book):
	
	that is to say that the matrix $A-\lambda I_n$ is not inversible (singular).
	
	The determinant $\det(A-\lambda I_n)$ is a polynomial on $\lambda$ of degree $n$ and can have at maximum $n$ solutions/eigenvalues as we have proved it in our study of polynomials (\SeeChapter{see section Calculus page \pageref{polynomial}}) and is named "\NewTerm{characteristic polynomial}\index{characteristic polynomial}\label{characteristic polynomial determinant}" of $A$ and the equation $\det(A-\lambda I_n)$ is named "\NewTerm{characteristic equation}\index{characteristic equation}" of $A$ or "\NewTerm{eigenvalues equation}\index{eigenvalues equation}\label{eigenvalue equations}".
	
	For the small parenthesis, it is nice to notice that we always have in the development of $\det(A-\lambda I_n)$ the trace of the matrix $\text{tr}(A)$ and the determinant $\det (A)$ that appear. Let us see two examples of this:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Let us begin with the case $n=2$:
	
	Therefore for a square matrix of dimension $2$, the eigenvalues are (simple resolution of a polynomial of the 2nd degree):
	
	E2. For a matrix of dimension $n=3$, we have:
	
	and here... the final solution (roots) are quite less easy in the general case...
	\end{tcolorbox}
	On the path let us notice (we will generalize the result coming from this during our study of the spectral theorem) that as multiplying the homogeneous system:
	
	by $-1$ on the both sides of the equality doesn't change anything to the problem, then we get:
	
	So we can see that is multiplication doesn't change anything to the final result!
	
	Thus by a term by term correspondence it comes the very important result in Statistics (and also in Numerical Methods!) that we will prove later in a more general way with the spectral theorem:
	
	If we look at $(\lambda I_n-A)$ as a linear application $f$, as it is non-trivial solutions that interest us, we can say that the eigenvalues are the elements $\lambda$ such that:
	
	and that the kernel constitutes the eigenspace  of $A$ of the eigenvalue $\lambda$ from which non-null elements are the eigenvectors!
	
	It corresponds to the study of the main axes, according to which the application behaves like an expansion (homothetic application) multiplying the vectors by the same constant. This homothetic ratio is then the "eigenvalue", the vectors to which it applies the "eigenvectors" are asembled in a "eigenspace".
	
	Another way of looking at it:
	\begin{itemize}
		\item A vector is said to be an "eigenvector" by a linear application if it is not zero and if the application does only change its size (norm) without changing its direction.

		\item An "eigenvalue", associated to an "eigenvector", is the size modification factor (homothetic ratio), ie the number by which we must multiply the vector to get its image. This factor can be negative (reverse direction of the vector) or zero (vector transformed into a vector of zero length).
		
		We can say that therefore that the eigenvalue $\lambda$ "scalar" the application $A$ for the eigenvector $\vec{X}$.

		\item An "eigenspace" associated to an "eigenvalue" is the set of eigenvectors that have the same eigenvalue value and a zero vector. They suffer all from the multiplication by the same factor.
	\end{itemize}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In mechanics, we study the eigenfrequencies and eigenmodes of oscillating systems (\SeeChapter{see section Wave Mechanics page \pageref{eigenmodes}}). In Functional Analysis, an eigenfunction is an eigenvector for a linear operator, that is to say a linear application acting on a space of function (\SeeChapter{see section Functional Analysis page \pageref{functional analysis}}). In geometry and optics, we speak of eigendirections to take into account the curvature of the surfaces (\SeeChapter{see section Non-Euclidean Geometry page \pageref{non-euclidean geometry}}). In graph theory, an eigenvalue is simply an eigenvector of the adjacency matrix of the graph (\SeeChapter{see section Graph Theory page \pageref{adjacency matrix}}).
	\end{tcolorbox}
	So as the determinant of $\det(A-\lambda I_n)$ is a polynomial on $\lambda$ then the $\lambda_i$ are also the roots of the characteristic polynomial:
	
	Therefore:
	
	This is a relation used sometimes in some statistical models (form example the MANOVA!). 
	
	Before closing this short introduction to the eigenvalues and eigenvectors (we will discussed this further below), let us indicate that since an eigenvector must satisfy the homogeneous system:
	
	Nothing avoid us then from multiplying the eigenvector by a constant $k$ which normalizes it to the unit (technique often used in statistics and numerical methods to improve the floating precision of the algorithms) since:
	
	Thus in practice it is customary that if the eigenvector is given for example by:
	
	To normalize it at the unit by writing:
	
	
	For the section of Wave Quantum Physics and more especially for the study of the angular momentum and spin we need to prove that given an operator acting on an eigenvector, then if we square the operator, this result in squaring the eigenvalue.
	\begin{dem}
	Given:
	
	Then:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\subsubsection{Rotation Matrices and Eigenvalues}\label{rotation matrix in linear algebra}
	Now that we have seen what was an eigenvalue and an eigenvector, let us come back on a particular type of orthogonal matrices that we will be particularly useful to us in our study of quaternions (\SeeChapter{see section Numbers page \pageref{quaternions}}), of groups and symmetries (\SeeChapter{see section Set Algebra page \pageref{set algebra}}) and particle physics (\SeeChapter{see section Elementary Particle Physics page \pageref{elementary particle physics}}).
	
	We denote, as what has been seen in the section of Set Algebra, $\text{O}(n)$ the set of $n\times n$ (square) orthogonal matrices with coefficients in $\mathbb{R}$, that is to say, satisfying:
	
	That will denote also sometimes for recall sometimes as:
	
	The columns and rows of an orthogonal matrix the form the orthonormal basis of the usual space $\mathbb{R}^2$ for the usual dot product.
	
	The determinant of an orthogonal matrix is equal to $\pm 1$ (rotation conserves angles and volumes), indeed $A^T A=I$  leads to:
	
	A rotation matrix with determinant $+1$ is a "\NewTerm{proper rotation}\index{eigenvalue equations}", and one with a negative determinant $-1$ is an "\NewTerm{improper rotation}\index{improper rotation}", that is a reflection combined with a proper rotation.
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us calculate now explicitly the determinant of a $2\times 2$ rotation matrix (\SeeChapter{see section Numbers page \pageref{2d rotation matrix}}) and $3\times 3$ rotation matrix (\SeeChapter{see section Euclidean Geometry page \pageref{3d rotation matrix around}}) as it is ask by many student on various Internet forums.\\

	So first we consider the $2\times 2$ rotation matrix and using the relation of the determinant proved earlier, we get:
	
	And for one randomly chosen rotation matrix of the three $3\times 3$ rotation matrices (\SeeChapter{see section Euclidean Geometry page \pageref{3d rotation matrix around}}), we get:
		
	\end{tcolorbox}
	
	We denote by $\text{SO}(n)$ the set of orthogonal matrices of determinant $1$ (for more details see the section of Set Algebra page \pageref{set algebra}). Let us show in three points that if $A\in \text{SO}(3,\mathbb{R})$  then $A$ is the rotation matrix relative to an axis passing through the origin.
	
	\begin{enumerate}
		\item Any eigenvalue of a rotation matrix $A$ (real or complex) is of module $1$. In other words it conserve the norm:

		Indeed, if $\lambda$ is an eigenvalue of eigenvector $\vec{X}$, we have:
		
		or noting the dot product with the book usual notation:
		
		
		\item  It exists a straight line in the space that is used a rotation axes and any vector on this line is not modified by any rotation.

		Let us denote by $\vec{X}$ an eigenvector  of eigenvalue $1$ (that is to say such that $A\vec{X}=\vec{X}$). As the reader may have perhaps already understand it (read until the end please!), the straight line generated by $\vec{x}$ that we will denote by $\langle \vec{X} \rangle$ constitutes our rotation axes.
	
		Indeed, any vector $\langle \vec{X} \rangle$ is send on itself by the application $A$. In this case, the orthonormal space denoted by $\langle \vec{X} \rangle^\perp$ that is of dimension $2$ is the perpendicular plane to the rotation axes.
	
		\item Any vector perpendicular to the rotation axes remains, after rotation, perpendicular to this axes. In other words,  $\langle \vec{X} \rangle$ in invariant through the application of $A$.
		
		Indeed, if $\vec{w}\in \langle \vec{x} \rangle$ the, $w=A^TA w=A^Tw$ and for all $\vec{y}\in \langle \vec{X} \rangle^\perp$:
		
		that is to say $A \vec{y} \langle \vec{X} \rangle^\perp$. Therefore $\langle \vec{X} \rangle^\perp$ is invariant by $A$.
		
		Finally, the restriction of $A$ to the space $\langle \vec{X} \rangle^\perp$ is a rotation!
	\end{enumerate}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Given $e^{\mathrm{i}\alpha}$ (see the section Numbers where the rotation by the complex number is proven) an eigenvalue (which module is $1$ as we proved it during our study of complex numbers) of $A$ restraint to $\langle \vec{X} \rangle^\perp$.\\
	
	Let us write $\vec{w}=\vec{u}+\mathrm{i}\vec{v}$ an eigenvector with $\vec{u},\vec{v}\in \mathbb{R}^2$ such as:
	
	with (as we already proved it in our study of complex numbers):
	
	where we know by our study of complex numbers, that the vectors $\vec{u},\vec{v}$ generate an orthogonal basis (not necessarily normalized at the unit!) of $\langle \vec{X} \rangle^\perp$.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We think that it could by easy at this level of the reader to check that this matrix is orthogonal (if it not the case contact us and this will be detailed!).
	\end{tcolorbox}
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Spectral Theorem}\label{spectral theorem}
	Let us now see a very important theorem relatively to the eigenvalues and eigenvectors which is named the "\NewTerm{spectral theorem}\index{spectral theorem}" which will be very useful to us for the various sections of physics of this book and also the section  Statistics as well as in the section of Theoretical Computing and Industrial Engineering.
	
	To summarize, mathematicians say in their language that the spectral theorem give the possibility to affirm the diagonalization  of endomorphism\footnote{What the also named an "endomorphism reduction"}\index{diagonalization  of endomorphism}\index{endomorphism reduction} (of matrices) and also justify the decomposition in eigenvalues (also named "\NewTerm{singular value decomposition S.V.D.}\index{singular value decomposition}") that we will see further below during our presentation of some matrices decomposition techniques.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The singular value decomposition theorem (S.V.D.) is however very general, in the sense that it applies to any rectangular matrices. The eigenvalue decomposition (see further below the details), however, only works for some square matrices.
	\end{tcolorbox}
	
	To simplify the proof, we will deal here only real matrices (component in $\mathbb{R }$) and also avoiding up the language of mathematicians.
	
	We will denote in a first time $M_n(\mathbb{R})$ the set of all $n\times n$ (square) matrices with real coefficients.
	
	We will confuse the matrix $M\in M_n (\mathbb{R})$ with the linear application on the vector space $\mathbb{R}^n$ by:
	
	with $\vec{v}\in \mathbb{R}^n$.
	
	Reminder: We have seen above during our study of basis changes that if $(\vec{c}_1,\ldots,\vec{c}_n)$ is a basis of $\mathbb{R}^n$ and $M\in M_n(\mathbb{R})$ then the matrix of the linear map $M$ in the basis $(\vec{c}_1,\ldots,\vec{c}_n)$  is:
	
	where $S$ is the matrix formed by the column vectors $\vec{c}_1,...,\vec{c}_n$.
	
	First, we simply check that if $A$ is a symmetric matrix then (this should be trivial but it can be verified with an example of dimension $2$ very quickly):
	
	\begin{enumerate}
		\item[P1.] All eigenvalues of $M$ are reals.
		\begin{dem}
		Given:
		
		an a priori complex eigenvector of the eigenvalue $\lambda \in \mathbb{C}$. Let us denote:
				
		the conjugate vector of $\vec{z}$. Then we have:
		
		On the other hand since $M=\overline{M}$ we have:
		
		As $\vec{z} \neq \vec{0}$, we have $\lambda=\vec{\lambda}$ and therefore, $\lambda\in \mathbb{R}$.
		\begin{flushright}
		$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		Before going further, we also have to prove that if $M\in M_n(\mathbb{R})$ is a symmetrical matrix and $V$ and vectorial subspace of $\mathbb{R}^n$ invariant relatively to $M$ (that is to say that satisfies for any $\vec{v}\in V: \; M\vec{v}\in V$) then we have the following properties:
		
		\item[P3.] The orthogonal of $V$ denoted obviously by $V^\perp$ (obtained by applying the Gram-Schmidt method seen in the section of Vector Calculus page \pageref{gram-schmidt procedure}) is also invariant through $M$.
		
		\begin{dem}
		Given $\vec{v}\in V$ and $\vec{w}\in V^\perp$ then:
		
		this shows well that $M\vec{w}\in V^\perp$.
		\begin{flushright}
		$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}

		\item[P4.] If $(\vec{w}_1,\ldots,\vec{w}_k)$ is an orthonormal basis of $\vec{V}^\perp$ then the restriction matrix of $M$ to $V^\perp$ in the basis $(\vec{w}_1,\ldots,\vec{w}_k)$  is also symmetrical.
		\begin{dem}
		
		Let us denote $A=(a_{ij})_{1\leq i,j\leq k}$ the matrix of the restriction of $M$ to $V^\perp$ in the basis $(\vec{w}_1,\ldots,\vec{w}_k)$. We have by definition for any $j=1...k$ (as the vector resulting of a linear application such as $M$ can be express in its basis):
		
		Or:
		
		as:
		
		if $i\neq m$ in the orthonormal basis.
		
		On another side:
		
		Therefore:
		
		This shows that:
				
		\begin{flushright}
		$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
	\end{enumerate}
	\begin{theorem}
		We will now be able to show that any symmetric matrix $M \in M_n(\mathbb{R})$ is diagonalizable. That is to say that there is an invertible matrix $S$ such that the result of the calculation:
	
	gives a diagonal matrix!
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In fact we will see, to be more precise, that there exists an orthogonal matrix $S$ such that $S^{-1}MS$ is diagonal.
	\end{tcolorbox}
	Reminder: Say that $S$ is "orthogonal" means that $SS^T=I$ (where $I$ is the identity matrix) which is equivalent to say that the columns of $S$ form an orthonormal basis of $\mathbb{R}^n$.
	\end{theorem}
	\begin{dem}
	We prove the assertion by induction on $n$. If $n=1$ there is nothing to prove. Let us suppose that the assertion is satisfied for $k\leq n$ and let us prove it for $k=n+1$. Then given $M\in M_{n+1}(\mathbb{R})$ a symmetric matrix and $\lambda$ an eigenvalue of $M$.
	
	We easily verify that the eigenspace:
	
	is invariant by $M$ (just take any numerical application) and that by the proof seen earlier, that $W^\perp$ is also invariant by $M$. Moreover, we know (\SeeChapter{see section Vector Calculus page \pageref{direct sum}}) that $\mathbb{R}^{n+1}$ can be decomposed into a direct sum:
	
	If:
	
	then:
	
	and it is sufficient to take an orthonormal basis of $W$ to diagonalise $M$. Indeed, if $(\vec{w}_1,\ldots,\vec{w}_{n+1})$ is such a basis, the matrix $S$ formed by the column vectors $\vec{w}_j$ ($j=1\ldots n+1$) is orthogonal and satisfies:
	
	and $S^{-1}MS$ is indeed diagonal.
	
	Let us now suppose that $\dim(W^\perp)>0$ and given $(\vec{u}_1,\ldots,\vec{u}_m)$ with $m\leq n$ an orthonormal basis of $W^\perp$. Let us denote by $A$ the restriction matrix of $M$ to $W^\perp$ in the basis  $(\vec{u}_1,\ldots,\vec{u}_m)$ . $A$ is also  symmetric (as proved in one of the preceding properties).
	
	By induction hypothesis there exists an orthogonal matrix $H\in M_m(\mathbb{R})$ such that $H^{-1}AH$ is diagonal.
	
	Let us denote by $(\vec{w}_1,\ldots,\vec{w}_{n+1-m})$ an orthonormal basis of $W$ and $G$ the matrix formed by the column vectors: $\vec{w}_1,\ldots,\vec{w}_{n+1-m},\vec{u}_1,\ldots\vec{u}_m$. So we can write that:
	
	and $G$ is also orthogonal by construction.
	
	Let us consider the following block matrix (matrix of matrices):
	
	and let us put:
	
	It is almost obvious that $S$ is orthogonal as $G$ and $L$ are also orthogonal. Indeed, if:
	
	then (remember that matrix multiplication is associative !!!):
	
	Also $S$ satisfies:
	
	and then:
	
	is indeed diagonal.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Finally here is finally the famous "\NewTerm{spectral theorem}\index{spectral theorem}" (real case):
	\begin{theorem}
	Given $M\in M_n(\mathbb{R})$ a symmetric matrix, then there exists an orthonormal basis made of eigenvectors of $M$.
	\end{theorem}
	\begin{dem}
	So we have seen in the preceding paragraphs that there exists an orthogonal matrix $S$ such that $S^{-1}MS$ is diagonal if $M$ is symmetric! Let denote by $\vec{c}_1,\ldots,\vec{c}_n$ the columns of $S$. The basis $(\vec{c}_1,\ldots,\vec{c}_n)$ is an orthonormal basis of $\mathbb{R}^2$ as $S$ is orthogonal. Denoting the $\vec{e}_i$ the $i$-th vector of the canonical basis of $\mathbb{R}^n$ and $\lambda_i$ and the $i$-th diagonal coefficient of $S^1{M}S$ we have without directly supposing that $\lambda_i$ is an eigenvalue for now:
	
	by multiplying by $S$ on both sides of the equality we have:
	
	and therefore:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	To finish about the spectral theorem in this book, let us so reprove a result we get earlier but that was presented in a quite ugly way and poorly  rigorous (the sum of the eigenvalues equals the trace of a matrix):
	
	Remember that spectral theorem therefore tells us that for any symmetric matrix $M$, there exists an orthogonal matrix $S$ such that:
	
	is diagonal. Nothing prevents us to choose the resulting diagonal matrix as a matrix of eigenvalues in the diagonal. What we denote usually:
	
	and as $S$ is a real orthogonal matrix and that by definition we have that a matrix is orthogonal if and only if $A^{-1}=A^T$, then we find the following relation:
	
	Commonly named a "\NewTerm{spectral decomposition}\index{spectral decomposition}" or "\NewTerm{eigendecomposition}\index{eigendecomposition}".
	
	So obviously have we will have to find $S$ if $M$ is known or vice versa. Anyway, let us come back on our topic and take track of this relation:
	
	Then by using the property of the trace $\text{tr}$, of the associativity of the matrix multiplication, and the orthogonality of $S$ we have:
	
	This reprove the results seen earlier above with a condition that was not trivial at this time: the matrix must be symmetrical (or symmetrizable)!
	
	We also have by extension:
	
	and therefore by using the proven property relatively to the determinant (during our proofs of the main determinant properties) and  the conjugated matrices we get:
	
	and therefore if $M$ is symmetric we have the important property:
	
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	 This result is in this text a particular form of the more general case (thus also applicable to rectangular or also non-orthogonal matrices) and that seems to be named the "\NewTerm{Eckart-Young theorem}\index{Eckart-Young theorem}".
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We want to show that:
	
	we assume that we know that the eigenvalue-eigenvector pairs are:
	
	We therefore introduce $S$ and $\Lambda$ as follows:
	
	We must show that $M=S\Lambda S^{-1}$. This is indeed the case, since:
	
	Therefore $M$ is indeed diagonalizable.
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Matrix Decompositions}
	In the mathematical discipline of linear algebra, a "\NewTerm{matrix decomposition}\index{matrix decomposition}" or "\NewTerm{matrix factorization}\index{matrix factorization}" is a factorization of a matrix into a product of matrices in the purpose to simplify problems (typically to simplify the resolution of linear systems especially when the matrix to invert are huge!) or to make emerge some interesting properties in various cases.
	
	There are many different matrix decompositions. Each finds use among a particular class of problems. There is almost two dozens of matrix decomposition techniques as far as we know. However in the texts that will follow below we will focus only on techniques that are used for practical explicit applications in this book (mainly in the field of Statistics and by extensions in financial engineering and also Machine Learning).
	
	\subsubsection{Singular Value Decomposition (SVD)}\label{singula value decomposition}
	The singular value decomposition of a matrix $M$ ($m\times n$) is the factorization of $M$ into the product of three matrices:
	
	where the columns of $U$ ($m\times r$) and $V$ ($r\times n$) are orthonormal such that\footnote{In other words, $U$ and $V$ are rotation matrices such that $U^TU=\mathds{1}_{m\times m}$ and $V^TV=\mathds{1}_{r\times r}$.}:
	
	and the matrix $D$ ($n\times n$) is diagonal with positive real entries.  
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	If $V$ is orthogonal, then as we have seen at page \pageref{orthogonal matrix}, we have $V^{-1}=V^T$, then that latter relation can also be written:
	
	\end{tcolorbox}
	The SVD is useful in many tasks as Data Mining, Image Processing and Advanced Numerical Methods.

	To gain insight into the SVD, we treat the rows of an $m\times n$ matrix $M$ as $m$ points in a $n$-dimensional space and consider the problem of finding the best $k$-dimensional subspace with respect to the set of points. Here "best" means minimize the sum of the squares of the perpendicular distances of the points to the subspace. 

	Let us begin with a special case of the problem where the subspace is 1-dimensional: a line through the origin. We will see later that the best-fitting $k$-dimensional subspace can be found by $k$ applications of the best fitting line algorithm. Finding the best fitting line through the origin with respect to a set of points $\{x_i|1 \leq i \leq m\}$ in the plane means minimizing the sum of the squared distances of the points to the line. Here distance is measured perpendicular to the line. The problem is then named as we know: the "best least squares fit" (\SeeChapter{see section Numerical Methods page \pageref{least squares method}}).
	
	Consider projecting a point $\vec{x}_1$ onto a line through the origin:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/algebra/svd.jpg}
		\caption{The projection of the point $\vec{x}_i$ onto the line through the origin in the direction of $\vec{v}$}
	\end{figure}
	Then us Pythagorean theorem we get:
	
	That is:
	
	Therefore (see figure):
	
	To minimize the sum of squares for the distance to the line, one could minimize:
	
	minus the sum of the square of the lengths of the projections of the points to the line. However,  $\sum_{i=1}^m (x_{i1}^2+x_{i2}^2+\ldots+x_{in}^2)$ is constant! (independent of the line), so minimizing the sum of the squares of the distances is equivalent to maximizing the sum of the squares of the lengths of the projections onto the line. Similarly for best-fit subspaces, we could maximize the sum of the squared lengths of the projections onto the subspace instead of minimizing the sum of squared distances to the subspace.
	
	\paragraph{Singular Vectors and Values}\mbox{}\\\\
	We will now build the "\NewTerm{singular vectors}\index{singular vectors}" (and also "\NewTerm{singular values}\index{singular values}") of a $m\times n$ matrix $M$. 

	Consider the rows of $M$ as $m$ points in a $d$-dimensional space. Consider the best fit line through the origin. Let $\vec{v}$ be a unit vector along this line.

	The length of the projection of $\vec{x}_i$, the $i$-th row $M$, onto $\vec{v}$ is (\SeeChapter{see section Vector Calculus page \pageref{dot product}}):
	
	That we will denote for what follows as (\SeeChapter{see section Vector Calculus page \pageref{dot product}}):
	
	So in our case:
	
	From this we denote the sum of all lengths of the projections by:
	
	The best fit line is the on maximizing $\|M\vec{v}\|^2$ (ie: $\|M\vec{v}\|$) and hence minimizing the sum of the squared distances of the points to the line.
	
	With this in mind, we define the "\NewTerm{first singular vector $\vec{v}_1$}\index{first singular vector}", of $M$, which is a vector, as the best fit through the origin for the $m$ points in $n$-space that are the rows of $M$. Thus:
	
	The scalar value:
	
	is named the "\NewTerm{first singular value}\index{first singular value}" of $M$. Notice that $\sigma_1^2$ is therefore implicitly the sum of the squares of the projections of the points to the line determined by $\vec{v}_1$.
	
	The greedy approach to find this time not the best fit $1$-dimensions but $2$-dimensional subspace for a matrix $M$, takes $\vec{v}_1$ as the first basis vector for the $2$-dimensional subspace and finds the best $2$-dimensional subspace containing $\vec{v}_1$.

	Thus, instead of looking for the best $2$-dimensional subspace containing $\vec{v}_1$, look for a unit vector, denoted $\vec{v}_2$, perpendicular to $\vec{v}_1$ that maximizes $\|M\vec{v}\|^2$ among all such unit vectors.

	Using the same strategy to find the best three and higher dimensional subspaces, defines $\vec{v}_3,\vec{v}_4,\ldots$ in similar manner.
	
	The "\NewTerm{second singular vector $\vec{v}_2$}", is defined by the best fit line perpendicular to $\vec{v}_1$:
	
	The value:
	
	is named the "\NewTerm{second singular value}" of $M$. 

	The "\NewTerm{third singular vector $\vec{v}_3$}" is defined similarly by:
	
	and so on...
	
	The process stop theoretically when we have found $\vec{v}_1,\vec{v}_2,\ldots,\vec{v}_r$ as singular vector that satisfies:
	
	
	As the $\vec{v}_i$ are perpendiculars, if we apply $M$ on all this vectors, the resulting vectors will also be perpendicular between them!  

	Therefore we build the vectors:
	
	that are all perpendiculars vectors between them as already mentioned and named "\NewTerm{left singular vectors}\index{left singular vectors}" of $M$ when the $\vec{v}_i$ will be named "\NewTerm{right singular vectors}\index{right singular vectors}". The SVD theorem will fully explain the reason for these terms.
	
	\begin{theorem}
	Let $M$ be an $m\times n$ matrix with right singular vector $\vec{v}_1,\ldots,\vec{v}_r$, left singular vectors $\vec{u}_1,\ldots,\vec{u}_r$, and corresponding singular values $\sigma_1,\ldots,\sigma_n$. Then the "\NewTerm{singular value decomposition theorem}\index{singular value decomposition theorem}" states that:
	
	\end{theorem}
	\begin{dem}
	We start naturally from:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Don't forget that $\vec{x}^T\vec{x}$ in Linear Algebra gives a scalar that is equivalent to the "dot product" ("inner product"), when instead $\vec{x}\vec{x}^T$ gives a square matrix named the "\NewTerm{outer product}\index{outer product}\label{outer product}", or sometimes "\NewTerm{Gram matrix}\index{Gram matrix}\label{Gram matrix}", defined by:
	
	\end{tcolorbox}
	Now let us take the a special case with (as I don't like the general proof):
	

	and let us wee what gives:
	
	So if we look closely the result is the same as if we define the matrix;
	
	Therefore we can see that:
	
	leads to the same result. Therefore we can write:
	
	But we must not forget that if  $V$ is an orthogonal matrix, then it represents an orthonormal basis and then we have proved already earlier above that in this case:
	
	Therefore:
	
	And this finish the proof!
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	The reader should also notice that:
	
	and:
	
	also often denoted $M=U\Sigma V^T$ (as $\Sigma$ is the upper case letter of $\sigma$), are equivalent notation for the same thing (just develop the last one explicitly and you will see you fall back on the same result\footnote{On request we can write the details})! The difference is that the notation with the sum is most used in Data Mining and that with the matrices in Statistics.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/algebra/svd_multiplication.jpg}
		\caption{The SVD decomposition of a $m\times n$ matrix}
	\end{figure}
	It is usage to build the matrix $D$ such that the diagonal is in descending order of amplitude and to order the vectors $\vec{v}_i$ in the corresponding order. The reason is quite easy to understand as you can see in the example further below.
	
	This is important to know that this not the only possible decomposition of a matrix. The are many other one but we will focus in this book only the decomposition that are directly useful for engineering topics presented in this book.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Some authors prefers to work with the norm of the singular values, that is, with $\sqrt{\sigma_i}$, therefore the left singular value are defines as:
	
	Without that it change our previous proof result that will just be:
	
	That in Europe is frequently written (but can bring to confusion with the notation of eigenvalues...):
	
	or in matrix form (...):
	
	\end{tcolorbox}
	Let us take a practical example in image processing of SVD (made by Jason Liu in MATLAB™). The image below is an image made of $400$ unique row vectors (the reader can found the equivalent example in our R companion book):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/algebra/svd_feynman_original.jpg}
		\caption{SVD MATLAB™ example original image}
	\end{figure}
	What happens if in the sum:
	
	we take only the first biggest singular vector?:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/algebra/svd_feynman_r_equal_1.jpg}
		\caption[]{SVD MATLAB™ SVD with $r=1$}
	\end{figure}
	What happens if we take the first two singular vectors?:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/algebra/svd_feynman_r_equal_2.jpg}
		\caption[]{SVD MATLAB™ SVD with $r=2$}
	\end{figure}
	...and if we take the first ten singular vectors?:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/algebra/svd_feynman_r_equal_10.jpg}
		\caption[]{SVD MATLAB™ SVD with $r=10$}
	\end{figure}
	...and if we take the first fity singular vectors?:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/algebra/svd_feynman_r_equal_50.jpg}
		\caption[]{SVD MATLAB™ SVD with $r=50$}
	\end{figure}
	There we have it! Using $50$ unique values and we get a decent representation of what $400$ unique values look like.
	
	So as we can see SVD is a great space reduction technique!
	
	\paragraph{Special case of SVD for symmetric matrices}\mbox{}\\\\
	The SVD for symmetric matrices is the unique case that interest us for practical purposes in this book (for the PCA: Principal Component Analysis). Therefore we will do a special focus on this latter case.
	
	We will prove here in two steps that first, for a symmetric matrix, the eigenvectors are orthogonal and secondly that the Singular vectors and Singular values are respectively equal to the eigenvectors and eigenvalues for symmetric matrices.
	
	Let us see the first step! In general, for any matrix, it's eigenvectors (when the exist) are NOT always orthogonal to each other. But for a special type of matrix, symmetric matrix, the eigenvalues are always real and the corresponding eigenvectors are always orthogonal!
	
	\begin{dem}
	Let us take a real symmetric matrix $M$ and two distinct eigenvalues of $M, \lambda_{1}$ and $\lambda_{2}$, such that $M \vec{x}_{1}=\lambda_{1} \vec{x}_{1}$ and:
	
	where $\vec{x}_{1}$ and $\vec{x}_{2}$ are obviously eigenvectors.

	From $M \vec{x}_{1}=\lambda_{1} \vec{x}_{1},$ we get:
	
	From $M \vec{x}_{2}=\lambda_{2} \vec{x}_{2},$ we similarly get:
	
	But since $M$ is symmetric:
	
	Also, clearly $\vec{x}_{1}^{T} \vec{x}_{2}=\vec{x}_{2}^{T} \vec{x}_{1} .$ Thus: 
	
	But $\lambda_{1}-\lambda_{2} \neq 0$. Hence:
	
	as required.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Now we treat the second step, for this let us consider $A$ be a symmetric $n \times n$ matrix. Then the maximum value of $|A \vec{x}|,$ where $\vec{x}$ ranges over unit vectors in $\mathbb{R}^{n},$ is the largest singular value $\sigma_{1},$ and this is achieved when $\vec{x}$ is an eigenvector of $A^{T} A$ with eigenvalue $\sigma_{1}^{2}$.
	
	\begin{dem}
	Let $\vec{v}_{1}, \ldots, \vec{v}_{n}$ be an orthonormal basis for $\mathbb{R}^{n}$ consisting of eigenvectors of $A^{T} A$ (indeed as $A^{T} A$ is symmetric as $(AA^T)^T=(A^T)^TA^T=AA^T$ - this even if $A$ is not symmetric itself - then according to the result just seen below, the eigenvectors are orthogonal) with eigenvalues $\sigma_{i}^{2} .$ If $\vec{x} \in \mathbb{R}^{n},$ then we can expand $\vec{x}$ in this basis as:
	
	for scalars $\{c_{1}, \ldots, c_{n}\}\in \mathbb{R}$ since $\vec{x}$ is a unit vector, $\|\vec{x}\|^{2}=1$, which (since the vectors $\vec{v}_{1}, \ldots, \vec{v}_{n}$ are orthonormal) means that:
	
	On the other hand:
	
	By:
	
	since the $\vec{v}_{i}$ are eigenvectors of $A^{T} A$ with eigenvalues $\sigma_{i}^{2},$ we have:
	
	Taking the dot product with:
	
	and using the fact that the vectors $\vec{v}_{1}, \ldots, \vec{v}_{n}$ are orthonormal, we get:
	
	Since $\sigma_1$ is the largest singular value, we get:
	
	Equality holds when $c_{1}=1$ and $c_{2}=\ldots=c_{n}=0$. Thus the maximum value of $\|A \vec{x}\|^{2}$ for a unit vector $\vec{x}$ is the eigenvalue $\sigma_{1}^{2},$ which is achieved when $\vec{x}=\vec{v}_{1}$ (equality between the vector and one of the eigenvector of $A^{T} A$).
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	One can similarly show that $\sigma_{2}$ is the maximum of $\|A \vec{x}\|$ where $\vec{x}$ ranges over unit vectors that are orthogonal to $\vec{v}_{1}$. Likewise, $\sigma_{3}$ is the maximum of $\|A \vec{x}\|$ where $\vec{x}$ ranges over unit vectors that are orthogonal to $\vec{v}_{1}$ and $\vec{v}_{2} ;$ and so forth.
	
	So that why in the case of symmetric (obviously square) matrices, the SVD denoted often:
	
	is then denoted (remember that $\Lambda$ is the upper case letter of $\lambda$, that latter being the traditional notation of eigenvalues):
	
	A common question that we can found on Internet forums is: \textit{Why are singular values always non-negative?}

	To answer let us take some, non zero singular value $\sigma_{i}$. We can reverse the sign if it is positive. That is, $-\sigma_{i}=-\sqrt{\lambda_{i}^{2}}=-\lambda_{i}$ where $\lambda_{i}$ is an eigenvalue of $A^{T} A$ corresponding to an eigenvector $v_{i}$ . That is $A^{T} A v_{i}=\lambda_{i}^{2} v_{i}$. 
	
	Who can stop us to write instead $A^{T} A\left(-v_{i}\right)=\lambda_{i}^{2}\left(-v_{i}\right) ?$ What this means is that we can reverse the sign of a singular value, but then we need to go to the matrix $V$ and reverse the sign of its corresponding eigenvector column.

	Hence, there is not a unique way to write $A=U D V^{T}$. But if we decide that all $\sigma_{i}$ are non-negative, then "yes" there is a unique way to write $A=UD V^{T}$. Of course all $\sigma_{i}$ are sorted from largest to smallest (otherwise there would be a bunch of possibilities by permuting any two columns of $U$ and $V$ and their corresponding eigenvalues).
	
	\paragraph{Orthogonal decomposition (eigenvalue spectral decomposition)}\mbox{}\\\\
	Let us come back to:
	
	If $M$ is symmetric, than it is a square matrix. Then obviously $U$ and $V$ are also square matrices have the same dimensions as those of $M$.
	
	If $M$ symmetric, then it is also obviously equal to it's transpose $M^T$, so if we do the SVD of the transpose we should get the same decomposition as $M^T$ such that:
	
	But as we have proved during our study of matrix transposition, we have:
	
	But we should have the equality:
	
	Therefore:
	
	So that for symmetric matrices $M$ we have:
	
	As before $V$ (so was $U$) is orthogonal but now it is also a square matrix. Hence it is invertible. In this case we have proved in the section of Linear Algebra that:
	
	So that finally for symmetric matrices $M$, we get the special case of SVD, named the "\NewTerm{orthogonal decomposition}\index{orthogonal decomposition}\label{orthogonal decomposition}" or "\NewTerm{eigenvalue spectral decomposition}\index{eigenvalue spectral decomposition}":
	
	Also often denoted for the obvious reason already mentioned above:
	
	
	\begin{tcolorbox}[colback=red!5,borderline={1mm}{2mm}{red!5},arc=0mm,boxrule=0pt]
\bcbombe Be careful to not make a confusion between Spectral Value Decomposition (SVD), also named Singular Value Decomposition, and Eigenvalue Spectral Decomposition (EVD) that is the special case of SVD but for square symmetric matrices. So that means in order for the SVD of a matrix $M$ to be equal to its eigendecomposition we need $M$ to:
	\begin{itemize}
		\item Have orthonormal eigenvectors (hence be a symmetric real matrix)
	
		\item Have positive eigenvalues (colloquially, it must be a real matrix and not "flip" anything)
	\end{itemize}	
	\end{tcolorbox}
	Notice that also the square root of $M$ (written here as $M^{1/2}$) - such that $M^{1/2}M^{1/2}=M$ - can be easily found to be:
	
	Indeed, we can easily check that is equality is correct:
	
	Another useful result for later (study of the statistical distribution of the Mahalanobis distance) is the eigenvalue decomposition of the inverse of a matrix $M$ using the relation about invertible matrices proved at page \pageref{inverse matrix property} and the associative property of the matrix product\label{inverse eigendecompsosition}:
	
	
	\subsubsection{$LU$ Decomposition}\label{lu decomposition}
	As an alternative to Gaussian elimination, a non-singular matrix $A$ may be written in the form $A = LU$ where $L$ is a lower triangular matrix (all entries above the main diagonal are zero) and all the entries on the main diagonal are unity:	
	
	This is known as "\NewTerm{$LU$ decomposition}\index{$LU$ decomposition}" or "\NewTerm{$LU$ factorization}" or "\NewTerm{triangular triangularization}".
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	$LU$ decomposition is not unique: if $A = LU$, then $A = LDD^{-1}U=(LD)(D^{-1}U) = L' U'$ is again an $LU$ decomposition, if $D$ is a diagonal matrix. An additional assumption $l_{ii} = 1$, $\forall i = 1, \ldots, n$, (impose that $L$ is a lower triangular unit matrix) guarantees the uniqueness.
	\end{tcolorbox}
	
	For example	(three by three case):
	
	$U$ is an upper triangular matrix (all entries below the main diagonal are zero). For example (three by three case):
	
	The method of LU decomposition involves writing the system $A\vec{x} = \vec{b}$ as $LU\vec{x} = \vec{b}$. If $U\vec{x}$ is written as $\vec{y}$, the system becomes $L\vec{y} = \vec{b}$. Due to the form of $L$ (triangular matrices make solution straightforward), $\vec{y}$ may be found quickly. Once this is done, the equation $U\vec{x} = \vec{y}$ may be solved for $\vec{x}$. 
	
	The tricky bit is probably finding $L$ and $U$. For a $3 \times 3$ matrix $A$.
	
	Let:
	
	Equating coefficients:
	
	These equations may be solved as follows:
	
	\begin{itemize}
		\item Solve [1] for $u_{11}$ 
		\item Solve [2] for $l_{21}$
		\item Solve [3] for $l_{31}$
		\item Solve [4] for $u_{12}$
		\item Solve [7] for $u_{13}$
		\item Solve [5] for $u_{22}$
		\item Solve [6] for $l_{32}$
		\item Solve [8] for $u_{23}$
		\item Solve [9] for $u_{33}$ 
	\end{itemize}
	This procedure seems to be named the "\NewTerm{Crout's algorithm}\index{Crout's algorithm}".
	
	As companion example let's solve:
	
	\hspace{6.1cm} $A$ \hspace{1.6cm} $\vec{x}$ \hspace{1.4cm}
	$\vec{b}$
	
	Let:
	
	i.e.:
	
	so, comparing coefficients:
	
	Or written in a more general way for $U$:
	
	and for $L$:
	
	We see that there is a calculation pattern, which can be expressed as the following relations:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We notice in the second relation that to get the $l_{ij}$ below the diagonal, we have to divide by the diagonal element (pivot)$u_{jj}$ , so we get problems when $u_{jj}$ is either $0$ or very small, which leads to numerical instability.
	\end{tcolorbox}
	
	Therefore:
	
	The equation $LU\vec{x} = \vec{b}$ becomes $L\vec{y} =
	\vec{b}$ i.e.:
	
	So (this procedure is named "forward substitution"):
	
	Where in general we can see that the pattern is:
	
	The equation $U\vec{x} = \vec{y}$ becomes 
	
	So (this procedure is named "backward substitution":
	
	Where in general we can see that the pattern is:
	
	For large matrices, this procedure involves fewer operations than Gaussian elimination. However, Gaussian Elimination is probably easier to program.
	
	The matrix $U$ is identical to the triangular matrix found in Gaussian Elimination;	$\vec{y}$ is identical to the right hand side immediately before the back-substitution.
	
	$LU$ decomposition can be viewed as the matrix form of Gaussian elimination. Computers usually solve square systems of linear equations using $LU$ decomposition, and it is also a key step when inverting a matrix or computing the determinant of a matrix. $LU$ decomposition was introduced by mathematician Tadeusz Banachiewicz in 1938.
	

	\subsubsection{Cholesky's Decomposition}\label{Cholesky decomposition}
	The Cholesky is another way of solving systems of linear equations. It can be significantly faster and uses less memory than the $LU$ decomposition by exploiting the property of symmetric matrices. However, it is required that the matrix being decomposed be Hermitian (or real-valued symmetric and thus square) and positive definite.
	
	For many symmetric matrices (strictly speaking for positive definite matrices\footnote{Don't forget that the variance-covariance matrix is a positive definite matrix!} $\vec{x}^{T} A \vec{x} > 0$ for all $x \ne 0$ as detailed at page \pageref{positive definite matrix}), $LU$ decomposition can be carried out using $U = L^{T}$. Therefore:
		
	This is known as "\NewTerm{Cholesky's decomposition}\index{Cholesky's decomposition}". While the Cholesky decomposition only works for symmetric, positive definite matrices, the more general LU decomposition works for any square matrix!
	
	\pagebreak
	As companion example, let us use Cholesky's method to solve:
	
	\hspace{6.5cm} $A$ \hspace{1.4cm} $\vec{x}$ \hspace{1.4cm}
	$\vec{b}$ \\
	Let:
	
	i.e.:
	
	So:
	
	Plus duplicates of [2], [3] and [5]
	\begin{itemize}
		\item From [1], $l_{11} = 2$,
		\item From [2], $l_{21} = -1$,
		\item From [3], $l_{31} = 1$,
		\item From [4], $l_{22} = 4$,
		\item From [5], $l_{32} = 0$,
		\item From [6], $l_{33} = 3$ 
	\end{itemize}
	So:
	
	The system becomes:
	
	i.e:
	
	so:
	
	$\vec{x}$ satisfies i.e.:
	
	
	
	
	We can see that from the above example that the diagonal elements $l_{kk}$ of $L$ there is a calculation pattern:
	
	For the elements below the diagonal ($l_{ik}$ , where $i>k$) there is also a calculation pattern:
	
	We see that the bothe case have a pattern that be written respectively:
	
	We can now understand for what the positive definiteness was necessary! Indeed, the positive definiteness of $A$ guarantees that all square roots are real. We can also notice from the above developments that $L$ needs to have positive diagonal entries\footnote{The reader should however not forget that from what we have seen during our study of positive definite matrices, the fact that all entries are positive makes an that the matrix is symmetric is not enough to make it positive definite}!
	
	Suppose now a matrix $A$ factors as $A=L^TL$. Then:
	
	This shows that $A$ is positive semidefinite.
	
	If we further assume that $L$ is square and triangular with positive real diagonal entries, then $L$ is invertible, so $L\vec{x}=\vec{0}\Leftrightarrow \vec{x}=\vec{0}$. In this case, we see that $A$ is positive definite.
	
	Some textbooks use Cholesky decomposition to define what makes a matrix positive definite. Indeed, such textbooks only say that any matrix $A$ that can \underline{uniquely} be written under a Cholesky decomposition $A=L^TL$ are positive definite matrices.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The above algorithms show that every positive definite matrix $A$ has a Cholesky decomposition. This result can be extended to the positive semi-definite matrix! However for positive semi-definite, it is possible but not for sure the Cholesky decomposition is not unique.
	\end{tcolorbox}
	
	
	\subsubsection{$QR$ Decomposition}\label{QR decomposition}
	A "\NewTerm{QR decomposition}\index{$QR$ decomposition}", also named a "\NewTerm{$QR$ factorization}", of a matrix is a decomposition of any real matrix $A$ into a product $A = QR$ of an orthogonal matrix $Q$ and an upper triangular matrix $R$:
	
	with $\text{col}_i(Q)\circ \text{col}_j(Q)=0, \forall i\neq j$ and (we will show if further below) $r_{ij}=\text{col}_i(Q)\circ \text{col}_j(A), \forall 1\leq 1\leq i\leq n$.
	
	There are several methods for actually computing the $QR$ decomposition, but we will focus here on the Gram–Schmidt process whose idea is described on page \pageref{gram-schmidt procedure}.
	
	So Consider the Gram–Schmidt process applied to the columns of the matrix $A$ and let us recall the result that we have derived during our study of the Gram–Schmidt process:
	
	That we will rewrite in our case:
	
	And in the $QR$ decomposition it is traditional to take:
	
	Therefore with this tradition we have ($Q$ is then not only orthogonal but orthonormal!):
	
	Before we continue, let us show an companion example. Let us consider the matrix:
	
	Let us carry out the Gram-Schmidt process with the columns $\text{col}_1(A)$ and $\text{col}_2(A)$:
	
	That is how the Gram-Schmidt process produces the matrix $Q$. Here are one of the methods to find $R$.
	
	For this the reader must understand (and see!) that at each step of Gram-Schmidt procedure, the operations on the vectors correspond to column operations $A$, which correspond to multiplying by elementary matrices on the right. Let us write all those matrices:
	
	From this we get (using the properties on the inverse matrices):
	
	We have found:
	
	Let us check the factorization:
	
	Let us rewrite the equalities using symbols, in order to obtain the general relation:
	
	From this we get:
	
	This relation for $R$ generalizes to any value of $n$. For example, if $n$ were $3$, $R$ would be given
by:
	
	Knowing this, there is no need to rewrite the whole computation every time. We can just
build the matrix $R$ as we go along the Gram-Schmidt process. So more generally:
	

	There is also another way to get $R$.  Indeed, the fact that $Q$ has orthonormal columns can be restated as we know as $Q^TQ=\mathds{1}$. In particular, $Q$ has a left inverse, namely $Q^T$. From this find $R$:
	
	In other words, the relation:
	
	holds, no matter what the dimensions of the matrix.
	
	If we take bake our example, let us recall that we had:
	
	Then:
	
	
	So to summarize:
	\begin{table}[H]
		\begin{tabular}{lll}
		\rowcolor[HTML]{C0C0C0} 
		\textbf{Factorization} & \textbf{Restrictions} & \textbf{Properties of Factors} \\ \hline
		\begin{tabular}[c]{@{}l@{}}SVD\\ $A_{nm}=U_{nn}D_{nm}V^T_{mm}$\end{tabular} & \multicolumn{1}{c}{none} & \begin{tabular}[c]{@{}l@{}}$U$ orthogonal\\ $V$ orthogonal\\ $D$ non-negative\end{tabular} \\
		\multicolumn{3}{c}{variations: for symmetric $A$, $A=VCV^T$} \\ \hline
		\begin{tabular}[c]{@{}l@{}}$LU$\\ $A_{nn}=L_{nn}U_{nn}$\end{tabular} & $A$ square, (others) & \begin{tabular}[c]{@{}l@{}}$L$ full-rank lower triangular\\ $U$ upper triangular\end{tabular} \\
		\multicolumn{3}{c}{\begin{tabular}{ll}
variations: & with partial pivoting, $A=LP$ \\
 & with full pivoting, $P_1AP_2=LU$ \\
 & $A=LDU$, with $D$ diagonal and $U_{ii}=1$
\end{tabular}} \\ \hline
		\begin{tabular}[c]{@{}l@{}}$QR$\\ $A_{nm}=Q_{nn}R_{nm}$\end{tabular} & \multicolumn{1}{c}{none} & \begin{tabular}[c]{@{}l@{}}$Q$ orthogonal\\ $R$ upper triangular\end{tabular} \\
		\multicolumn{3}{c}{variations: skinny $QR$ for $n>m$, $A=Q_1R_1$} \\ \hline
		\begin{tabular}[c]{@{}l@{}}Cholesky\\ $A_{nn}=L_{nn}U_{nn}$\end{tabular} & $A$ non-negative definite & \begin{tabular}[c]{@{}l@{}}$L$ full-rank lower triangular\\ $U$ upper triangular\end{tabular} \\ \hline
		\begin{tabular}[c]{@{}l@{}}Diagonal (ie orthogonal)\\ $A_{nn}=V_{nn}C_{nn}V^T_{nn}$\end{tabular} & $A$ symmetric & \begin{tabular}[c]{@{}l@{}}$V$ orthogonal\\ $C$ diagonal\end{tabular} \\ \hline
		\begin{tabular}[c]{@{}l@{}}Square root\\ $A_{nn}=(A^{1/2}_{nn})^2$\end{tabular} & $A$ non-negative definite & $A^{1/2}_{nn}$ non-negative definite \\ \hline
		\end{tabular}
		\caption{Matrix Factorizations}
	\end{table}
	
	\pagebreak
	\subsection{Woodbury Matrix identity and Sherman-Morrison relation}
	The "\NewTerm{Woodbury matrix identity}\index{Woodbury Matrix Identity}\label{Woodbury matrix identity}" gives the inverse of an $n\times n$ square matrix ${ A}$ modified by a perturbation term ${ UBV}^T$:
	
	It is a very useful useful result for our study on the inverse and determinant of a partitioned symmetric matrix (itself useful for the Factor Analysis statistical tool). It has also other practical application like for PPCA (Probabilistic Principal Component Analysis) or for hierarchical linear models!
	
	The proof is straightforward:
	\begin{dem}
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Consider some special cases:
	\begin{itemize}
		\item If ${ U}={ V}={ \mathds{1}}$, then we get:
		
		
		\item If ${ B}={ \mathds{1}}$, and let ${ U}=[{\vec u}_1,\ldots,{\vec u}_m]$ and ${ V}=[{\vec v}_1,\ldots,{\vec v}_m]$, then we get the inverse of a rank-$n$ modified matrix:
		
		
		\item More specially, when $m=1$, ${ U}={\vec u}$ and ${ V}={\vec v}$ we get the inverse of rang-$1$ modified matrix:
		
	\end{itemize}
	Let us see two possible derivations of that later case! First possible proof:
	\begin{dem}
	We first show the following is an identity ($A=\mathds{1}$):
	
	Pre-multiplying ${ \mathds{1}}+{\vec w}{\vec v}^T$, the right side becomes ${ \mathds{1}}$ as well as the left side:
	
	We next let ${\vec u}={ A\vec w}$ in:
	
	and the left side of the relation to be proven becomes:
	
	Substituting $\vec{w}={A}^{-1}{\vec u}$ we get the "\NewTerm{Sherman-Morrison relation}\index{Sherman-Morrison relation}\label{Sherman-Morrison relation}":
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	And second possible proof:
	\begin{dem}
	This second proof is based on the following problem. Assuming a linear equation system ${\ A}{\vec y}={\vec b}$ is solve to get ${\vec y}={ A}^{-1}{\vec b}$, we want to solve this system:
	
	We first pre-multiply both sides of this equation by ${\ A}^{-1}$ to get:
	
	If we define ${\vec w}={ A}^{-1}{\vec u}$ and $\alpha={\vec v}^T{\vec x}$, the above equation can be written as:
	
	Pre-multiplying both sides by ${\vec v}^T$, we get:
	
	Solving for $\alpha$ we get:
	
	Substituting $\alpha$ into the previous equation for ${\vec x}$ we get:
	
	But solving the equation $({\vec A}+{\vec u}{\vec v}^T){\vec x}={\vec b}$ we also get:
	
	we therefore have:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{95} & \pbox{20cm}{\score{4}{5} \\ {\tiny 36 votes,  79.94\%}} 
	\end{tabular} 
	\end{flushright}
	
	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Tensor Calculus}\label{tensor calculus}
	\lettrine[lines=4]{\color{BrickRed}T}he conventional vector calculus is a simple and effective technique that adapts perfectly to the study of mechanical and physical properties of matter in an Euclidean space of three dimensions. However, in many fields of physics, it appears experimental quantities that can't be easily represented by simple column vectors of Euclidean vector spaces. This is the case for example in continuum mechanics (fluids or solids), electromagnetism, General Relativity, etc.
	
	Thus, since the late 19th century, the analysis of forces acting within a continuous medium led to highlight the physical quantities characterized by nine numbers representing the pressure forces or internal stress (see section Continuum Mechanics page \pageref{continuum mechanics} for the details). The representation of these quantities required the introduction of a new mathematical tool who was named "\NewTerm{tensor}\index{tensor}", by reference to its physical origin. Subsequently, starting the years 1900, it was R. Ricci and T. Levi-Civita who developed the tensor calculus; then the study of tensor allowed a deepening of the theory of vector spaces and contributed to the development of differential geometry (see section of the same name page \pageref{differential geometry}).
	
	Tensor calculus, also sometimes named "\NewTerm{absolute differential geometry}\index{absolute differential geometry}" also has the advantage to free itself from all coordinate systems and the results of  the mathematical developments are thus invariant (huge simplification in calculations but in compensation we have a huge increase in abstraction and notation complexity). We therefore don't need to be concerned in what type of referential frame we work and this is very interesting in General Relativity.
	
	We advise strongly the reader to very well master the basics of vector calculus and linear algebra as they have been presented before in previous sections (especially because linear algebra forms the skeleton of tensor calculus!). If necessary, we have chosen when writing this section to come back on certain points seen in the section of Vector Calculus and Linear Algebra (covariant components, contravariant components, etc.).
	
	Furthermore, if the reader has already covered the study of constraints in solids (\SeeChapter{see section Continuum Mechanics page \pageref{constraints}}) or of the Faraday tensor (\SeeChapter{see section Electrodynamics page \pageref{faradey tensor}}) or the energy-momentum tensor (\SeeChapter{see section General Relativity page \pageref{energy momentum tensor}}) this will be a practical advantage before reading what follows. Furthermore, the redaction of the above items (tensors) was made so that the concept of tensor is  introduced if possible (...) intuitively.
	
	We will only do a very few practical examples in this section. Indeed the examples, you have probably already guess it..., will come when we will study the continuum mechanics, General Relativity, quantum field theory, electrodynamics, etc.
	
	An advice maybe: you have seen many time in the section of Statistics that write with vectors and after think with matrix was a powerful way to generalize some important results. For this section on tensors remember that the idea is the same but we think matrix and we write tensor! (you will better understand this little adage once you will be finished to read this whole section).
	
	\subsection{Tensor}
	\textbf{Definition (simplistic \#\mydef):} The "\NewTerm{tensors}\index{tensor}" are mathematical objects generalizing the concepts of vectors and matrices. They were introduced in physics to represent the state of stress and deformation of a volume subjected to forces, hence their name (tensions).
	
	The rigorous definition requires (I personally think...) to have first read this section in its whole. But you must know that in fact a tensor is roughly like a determinant... (\SeeChapter{see section Linear Algebra page \pageref{determinant}}). Eh yes! It is simply a multilinear application on a space of a given size (corresponding to the number of columns of the matrix/tensor) which finally gives a scalar (of a given field).
	
	\label{tensor notation}For example, we have proved in the section of Continuum Nechanics that normal and tangential forces in a fluid were given by the relation:
	
	what was noted in the traditional condensed form as following (where we no longer distinguish what is tangential to what is normal so there is a loss of clarity):
	
	We thus make appear a mathematical quantity $\sigma_{ij}$ with $9$ components, while a vector in the same space $\mathbb{R}^3$ has $3$ components.
	
	This notion is also much used in the section of General Relativity where we have proved that the energy-momentum tensor in a particularly simple case was given by:
	
	and satisfies the non-less important equation of conservation:
	
	Or otherwise, still in the section of General Relativity, we have shown that the tensor of the Schwarzschild metric was given by:
	
	and therefore gives us the "equation of the metric"\index{metric equation} or "invariant interval"\index{invariant interval} (\SeeChapter{see section Special Relativity page \pageref{interval invariant}}):
	
	Note also that in the section of Special Relativity we have shown that the Lorentz transformation tensor is given by:
	
	which in a condensed form gives the following components transformation:
	
	As regards the transformation of the electromagnetic field we have also proved that the Faraday tensor is given by:
	
	and therefore permits switch from one repository frame to another using the relation:
	
	But these are very simple tensor that can be represented in the form of matrices. You should also remember that it is not because you are reading a variable with indices suggests that we are dealing with a tensor that it is necessarily one. For example, the famous relation (widely used in the section of General Relativity and we will prove far further below):
	
	might suggest that the first member of the far left is a tensor but in fact it is not... this is just a symbol... hence its name: Christoffel \underline{symbol} (not: Christoffel \underline{tensor}).
	
	The interest of tensors in physics is that their characteristics are independent of the chosen coordinates. Thus, a relation between tensors in a base will be true regardless of the base used thereafter. This is a fundamental and powerful characteristic of General Relativity (among others)!
	
	\pagebreak
	\subsection{Indicial Notation}
	We will use thereafter many mathematical symbols: coordinates, components of vectors and tensors, matrix components, etc., whose number in each category is large or indeterminate. To distinguish the various symbols of a category we use indices. For example, instead of the traditional variables $x, y, z$ we will use the variables $x_1,x_2,x_3$ (as we have already done in the section of Linear Algebra). This rating becomes essential when we an undetermined number of variables.
	
	Thus, if we have $n$ variables, we denote them by $x_1,x_2,...,x_n$.
	
	We will also use superscripts, when required; eg $x^1,x^2,x^3$. To avoid confusion with writing powers, the quantity $xî$ to the power $p$ will be written $(x^i)^p$. When the context eliminates any potential ambiguity, the use of parentheses however is not fundamentally necessary.
	
	In tensor calculus there is a summation convention using the fact that the repeated index, below for example the index $i$, will become itself an indication of the summation. We write then, with this convention:
	
	thereby this condense relatively well the notations!
	
	Thus, to represent the linear system:
	
	we will write (notice carefully how are written the components of the associated matrix!):
	
	specifying that's for $n=3$.
	
	We see in this example, how the summation convention allows a condensed and thus powerful writing.
	
	The summation convention covers all the mathematical symbols having repeating indices. Thus the decomposition of a vector $\vec{x}$ on a basis $(\vec{e}_1,\vec{e}_2,\vec{e}_3)$ will be therefore written for $n=3$:
	
	In summary, any term that has a repeated index represents a sum over all possible values of the repeated index.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We name, for obvious reasons we will detail below, the $x^i$ "\NewTerm{contravariant component}\index{contravariant components}" of the vector $\vec{x}$.
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{Summation on multiple index}\label{einstein summation convention}
	The summation convention (due to Einstein) extends to the case where we have, in general, several repeated indices in upper and lower positions so-named "\NewTerm{dumb indices}\index{dumb indices}" in the same monomial (often physicists omit the rule of set their position opposite as it will be the case often on this book too!). Thus, for example, the quantity $A_i^jx^ix^j$, represents the following sum for $i$ and $j$ taking the values from $1$ to $2$:
		
	Thus we see easily that an expression with two summation indices that take values respectively $1,2,...,n$, will have $n^2$; will have $n^2$ terms, $n^3$ if there are three sommation indices, etc.
	
	However, we must be careful to substitutions with this kind of notation because if we assume that we have the relation:
	
	then to get the expression of $A$ only in function of the variables $y^j$ we cannot write:
	
	because it does not return to the same expression as the dumb indices after development are systematically sum in an identical and rigid way (we leave to the reader make a little application case to see this, if need you can contact us and we will do an example). In other words, a dumb index can not be repeated more than $2$ times.
	
	\subsubsection{Kronecker Symbol}\label{kronecker symbol}
	This symbol introduced by the mathematician Kronecker, is the following (often used in physics and in many other fields):
	
	This symbol is named "\NewTerm{Kronecker symbol}\index{Kronecker symbol}". It conveniently allows you to write, for example, the dot product of two vectors $\vec{e}_1$ and $\vec{e}_2$, of unit norm and orthogonal to each other, in the form:
	
	We will find this symbol in many examples of theoretical physics in this book (wave quantum physics, quantum field theory, general relativity, fluid mechanics, etc.).
	
	It should be noted that there is a generalized version of the Kronecker symbol:
	
	We have also, for example:
	
	where $\varepsilon_{ijk}$ is the Levi-Civita symbol that will define right now:
	
	\subsubsection{Antisymmetric Symbol (Levi-Civita symbol)}\label{levi civita symbol}
	Another useful symbol is the "\NewTerm{symbol of antisymmetry}\index{symbol of antisymmetry}" or also named "\NewTerm{antisymmetry tensor}\index{antisymmetry tensor}" that we will find in the sections of Electrodynamics,  General Relativity and Relativistic Quantum Physics in this book.
	
	In mathematics, particularly in linear algebra, tensor analysis, and differential geometry, the Levi-Civita symbol represents a collection of numbers; defined from the sign of a permutation of the natural numbers $1, 2, \ldots, n$, for some positive integer $n$.
	
		 The values of the Levi-Civita symbol are independent of any metric tensor and coordinate system. Also, the specific term "symbol" emphasizes that it is not a tensor because of how it transforms between coordinate systems, however it can be interpreted as an antisymmetric tensor (a tensor is antisymmetric on (or with respect to) an index subset if it alternates sign (+/-) when any two indices of the subset are interchanged).
	 
	 In the case $n=2$ the Levi-Civita symbol is defined by:
	 
	The values can be arranged into a $2\times 2$ antisymmetric matrix (we can see we fall back on the definition of an antisymmetric tensor):
	
	Use of the 2D symbol is relatively uncommon, although in certain specialized topics like supersymmetry and twistor theory it appears in the context of 2-spinors. The 3D and higher-dimensional Levi-Civita symbols are used more commonly.
	
	In three dimensions, the Levi-Civita symbol is defined as follows:
	
	i.e.  $\varepsilon_{ijk}$  is $1$ if $(i, j, k)$ is an even permutation of $(1,2,3)$ or in the natural order $(1,2,3)$, $-1$ if it is an odd permutation, and $0$ if any index is repeated. In three dimensions only, the cyclic permutations of $(1,2,3)$ are all even permutations, similarly the anticyclic permutations are all odd permutations. This means in 3D it is sufficient to take cyclic or anticyclic permutations of $(1,2,3)$ and easily obtain all the even or odd permutations.

	It can also be express with the Kronecker symbol:
	
	
	An illustrative representation gives:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/algebra/levi_civita_symbol.jpg}
		\caption[3D Levi-Civita symbol illustration]{3D Levi-Civita symbol illustration (source: Wikipedia)}
	\end{figure}
	By using this symbol, a determinant of order two (\SeeChapter{see section Linear Algebra page \pageref{determinant}}) is then written in the advantageous form:
	
	and the vector cross product:
	
	where of course, $j$ and $k$ are summed and where the dummy index $i$ is the line number of the resulting vector (if requested we will make the developments). In particular, the rotational (curl) of a vector field (\SeeChapter{see section Vector Calculus page \pageref{rotational}}) is then:
	   
	As an example, let us calculate in index notation the double vector product $\vec{A}\times\vec{B}\times\vec{C}$:
	 
	where again, the dumy index $i$ is the line number of the resulting vector. Let us eee detailed demonstration of these equalities (the order of the equalities below does not need to follow the sequence of equalities of the previous relation).
	\begin{dem}
	We have proved in the section of Vector Calculus the following identity:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The latter relation is sometimes, and for recall, named the "\NewTerm{Grassmann rule}\index{Grassmann rule}" or more commonly "\NewTerm{dual vector product}\index{dual vector product}".
	\end{tcolorbox}	
	To prove the relation:
	
	to a change of indices let us first prove that:
	
	which give us:
	
	We do the development only for the first line (this is already bor... euh long enough...):
	
	This is the first step that was necessary to be proven.
	
	Now let us prove that for the $n$th line we have well:
	
	with the help of a result obtained in the section of Vector Calculus (vector product of three different vectors) we have the first term (the first line of the vector resulting from the calculation):
	
	It is immediate that ($i$ being equal to $1$):
	
	Let us show now that for $i$ equal $1$ we also have:
	
	Indeed:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	As a second example, letus prove that the divergence of a curl vanishes:
	
	By the Schwarz's theorem (\SeeChapter{see section Differential and Integral Calculus page \pageref{Schwarz theorem}}) $\partial_i\partial_j$ is symmetric (so invert the indices has no impact) in the indices and that $\varepsilon_{ijk}$ is antisymmetric (by definition) in the same indices, the sum on $i$ and $j$ must necessarily cancel. For example, the contribution to the sum of the $i=1,j=2$ is the opposite of this with $i=2,j=1$.
	
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} The symbol of antisymmetry is often named "\NewTerm{Levi-Civita tensor}\index{Levi-Civita tensor}" in the literature. In fact, although it is a tensor in the form of its notation, it's more of a mathematical tool that a mathematical "being" hence the preference of some physicists to name it "symbol" rather than "tensor". But it's up to you ...\\
	
	\textbf{R2.} By abuse of writing we do not write the basic vector but rigorously, and to avoid forgetting it, remember that in order to balance the members of the equality and in order to clarify that the vectors are expressed in the same base, we should write:
	
	\end{tcolorbox}
	Let us now see the first simple practical applications of this index notation using the example of the base change that we have seen in the section of Vector Calculus:
	
	Given two bases $(\vec{e}_1,\vec{e}_2,\ldots,\vec{e}_n)$ and $(\vec{e}_1^{\prime},\vec{e}_2^{\prime},\ldots,\vec{e}_n^{\prime})$ of an Euclidean vector space $\mathcal{E}^n$. Each vector of a base can be decomposed on the other base in the form of a linear application (base change matrix - see section Linear Algebra page \pageref{change of basis}):
	
	where we obviously use the summation convention for $i,k=1,2,\ldots,n$.
	
	Let us recall that the base change matrix (or "\NewTerm{transformation matrix}\index{transformation matrix}") should have as many columns as the basic vector have lines (dimensions or components). Small example with three dimensions gives:
	
	and obviously it is much more funny to write this as:
	
	so where on $A$, we have the $k$ that represents the column of the matrix and $i$ the row of the matrix.

	Any vector $\vec{x}$ of $\mathcal{E}^n$ can be decomposed (we have already prove this in the section of Vector Calculus) on each basis $\mathcal{E}^n$ under the form:
	
	If we seek for the relations between the components $x^i$ and ${x'}^k$ it is enough to take again the relations prove in the section of Linear Algebra and we have then:
	
	Immediately by the uniqueness of the decomposition of a vector on a base, we can equalize the components of the basis vectors and we get (the must be careful to rearrange again the order of the terms because the matrix multiplication is in general, not commutative as we already know!):
	
	By construction we also have the trivial relation:
	
	As:
	
	A way to prove in a quite general way the previous relation using tensor calculus notation is to remember the following result proved in the section of Linear Algebra:
	
	and by using:
	
	We therefore have:
	
	The basis vectors being linearly independent, this last relation implies that when $i\neq j$:
	
	and when $i=j$:
	
	Therefore it comes:
	
	And for the dot product, the results obtained with the index notation are very interesting and extremely powerful. We have already defined the scalar product in the section of Vector Calculus but let us see how we handle this with the index notation:

	Let us consider an Euclidean vector space $\mathcal{E}^n$ reported any basis ${\vec{e}_i}$. We already know that vectors are written on this basis:
	
	\label{metric tensor euclidean space}The scalar product with respect to its properties and the index notation is then written:
	
	This is a fundamental relation for advanced physics (General Relativity and String Theory) that makes appear the "\NewTerm{metric covariant tensor}\index{metric covariant tensor}" (\SeeChapter{see section Non-Euclidean Geometry page \pageref{riemann spaces}}):
	
	and to satisfy the commutative property of the dot product (\SeeChapter{see section Vector Calculus page \pageref{dot product}}) we must obviously have the equality (at least in Euclidean space or approximated as...):
	
	The prior-previous relation is sometimes written in the form:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	When the basis vectors $\vec{e}_i$ form an orthogonal vector space (not necessarily orthonormal) the quantities:
	
	are obviously zero when $i \neq j$. The dot product of two vectors $\vec{x}$ and $\vec{y}$ is then reduce to:
	
	We the have in this particular case\label{condensed flat metric space notation}:
	
	and therefore when the basis vectors form an orthonormal vector space it is clear that $g_{ij}$ is equal to the Kronecker symbol alone such that:
	
	\end{tcolorbox}
	
	\subsection{Metric and Signature}\label{metric and signature}
	As we have seen in it in the section of Vector Calculus (and Topology), the dot product of a vector $\vec{x}$ can be used to define the concept of norm of a vector (and also the concept of distance).
	
	Let us recall that we have by definition the norm of a vector which is given by (\SeeChapter{see section Vector Calculus page \pageref{dot product}})\label{norm tensor notation}:
	
	where the numbers $g_{ij}$ define somehow a "measure" of the vectors; we then say in the language of tensor calculus that they are the "metrics" of the selected vector space.
	
	In the space of classical geometry, the norm is a number that is always strictly positive and which becomes zero if the measured vector is also zero. By cons the previous expression of the norm of a vector, may eventually be negative for any numbers $g_{11},g_{12},\ldots,g_{nn}$ (complex spaces for example). So we can distinguish two kinds pre-Euclidean vector spaces\label{pre euclidean vector space} (Euclidean space in which we define a scalar product for recall) depending on the fact that the norm is positive or not. However when in theoretical physics we want to make the analogy with a vector space structure we need that the condition:
	
	is satisfied ($g_{ij}$ can be written as a matrix, nothing avoid us to do it).
	
	Explanations: We know that the dot product must satisfy the commutative property such that:
	
	On the other hand, if for any nonzero $y^j$ we have:
	
	this implies $x^i=0$ (that is one of the properties of the norm we saw in the section of Vector Calculus). We can then write:
	
	We are here simply with a system of $n$ equations with $n$ unknowns (having to admit by hypothesis that only the solution $x^i=0$), it is necessary and sufficient for this that the determinant of the system, denoted $g$, to be is different from zero (\SeeChapter{see section Linear Algebra page \pageref{determinant matrix inverse}}). So we must have:
	
	It is one of the condition for an expression comparable to a norm under a tensor index notation form in the context of theoreticla physicsa vector space of the states of the system !!
	
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} The number of $+$ and $-$ signs found in the expression of the dot product is a is a characteristic of a given vector space $E^n$. It is named the "\NewTerm{signature of the vector space}\index{signature of a vector space}".\\
	
	\textbf{R2.} A practical application of calculation of the metric is presented in details in the section of General Relativity.\\
	\end{tcolorbox}
	From the coefficients of the covariant metric tensor $g_{ij}$ defining the metric of the space $E^n$, we can introduce the coefficients of the "\NewTerm{contravariant metric tensor $g^{ij}$}\index{contravariant metric tensor}" defining the metric of a "\NewTerm{dual space}\index{dual space metric}" $E_{*}^n$ by the relation:
	
	In other words, the metric tensor twice covariant is its own inverse by its equivalent twice contravariant tensor. We will prove it explicitly later by showing during our study of the of the Gram's determinant that contravariant and covariant components of a Euclidean space are equal and both space have the same number of dimensions.

	A well known special case that meets the above equality is the Minkowski's metric tensor of (\SeeChapter{see section Special Relativity page \pageref{minkowski metric}}), where we have:
	 
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The space $E^n$ is also named "primal space" and if it is of Euclidean type let us recall that it is denoted $\mathcal{E}^n$.
	\end{tcolorbox}
	The dual space is underpinned by $n$ basis vectors $\vec{e}^i$ constructed from the vectors $\vec{e}_i$ such that:
	
	It is therefore easy to see that the scalar product of the vectors $\vec{e}^i$ defines the metric $g^{ij}$ of the dual space:
	
	while the vectors $\vec{e}^i$ (contravariant) and $\vec{e}_j$ (covariates) are orthogonal:
	
	We can also express a vector in the dual base by the next writing by noting that obviously the position of the dummy indices is reversed:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The components $x_i$ (orthogonal projections of the vector on the axes) are named, for reasons that we will see further below, the "covariant" components.
	\end{tcolorbox}
	So we finally have the possibility to change the vectors of a base to another one:
	
	where it is important to remember that to make a contravariant a covariant  component, we bring up the index:
	
	and conversely, to make it covariant:
	
	A "\NewTerm{covariant vector}\index{covariant vector}" or "\NewTerm{cotangent vector}\index{cotangent vector}" (often abbreviated as "\NewTerm{covector}\index{covector}") has components that co-vary with a change of basis. That is, the components must be transformed by the same matrix as the change of basis matrix. The components of covectors (as opposed to those of vectors) are said to be "covariant".
	
	So, still in the case of the example of the Minkowski metric, if we consider the contravariant four-vector:
	
	Then we have:
	
	
	\subsection{Gram's Determinant}
	Let us see another approach to determine the base vectors of the dual space that can allow also a better understanding of the concept and will allow us to get an interesting result that we will use during certain calculations of General Relativity and String Theory (mainly its study using to the Lagrangian formalism).
	
	So we have for $i=j=1$:
	
	This scalar product can be seen as a normalization condition for the two bases and two scalar products $\vec{e}^2\circ\vec{e}_1=0$,$\vec{e}^2\circ\vec{e}_1=0$ as orthogonalization conditions . Thus, as $\vec{e}_1$ is perpendicular to $\vec{e}^2,\vec{e}^3$ we can write:
	
	where $c^{te}$ is a constant of proportionality. Now let us play around with the prior previous relation:
	
	Then we get:
	
	where we see appear the mixed product as we had defined it in the section of Vector Calculus.
	
	Thus we get very easily:
	
	and even for contravariant vectors (without proof as may be too obvious?):
	
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} The reader will have perhaps noticed that the relations above are only valid for a three-dimensional space.\\
	
	\textbf{R2.} The notation of the two previous relations is mathematically a bit unfair because in reality it is not an equality between two vectors but an application of a vector space in the other one!\\
	
	\textbf{R3.} As in physics is very frequently considered Cartesian , cylindrical and spherical orthonormal base and that denominator of the two previous relations is always equal to $1$ in these bases than the contravariant basis vectors are identified with covariant basis vectors (and vice versa ). So the covariant coordinates are equal to the contravariant coordinates for these special cases!!!!!!!!!! 
	\end{tcolorbox}	
	Now let us come back on something that will seem very old of us... In the section of Vector Calculus, we have defined and studied what were the cross product and mixed product. We will now see another way of representing them and see that this representation provides a result for the less quite relevant!
	We saw in the section of Vector Calculus that the vector product was given by:
	
	But what we did not see and we will now that we can trivially this expression is only the vector determinant of the following matrices:
	
	Yes ... so the result does not give a scalar! It is just a usage rating.
	
	But as we do the tensor calculus, we must now properly distinguish covariant and contravariant components. We'll rewrite it properly with contravariant components:
	
	Similarly, the mixed product can be written using this relation and notation:
	
	Or, looking at the expression of the determinant we see quite easily, without having to do developments, that
	
	Indeed (we calculate the determinant making use of the demonstration of the determinant with three components proved in the section of Linear Algebra):
	
	The prior-previous relation is also is frequently written:
	
	with obviously:
	
	named "\NewTerm{Euclidean volume}\index{Euclidean volume}" (indeed let us recall that the mixed product is a volume as we show it in the section of Vector Calculus!)
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Let us recall again that if the basis vectors are orthonormal, whether they are expressed in Cartesian , cylindrical or spherical coordinates then:
	
	\end{tcolorbox}
	Moreover, we also have the important relation:
	
	Moreover, we also have the important relation:
	
	Indeed, using the relation see in the of Vector Calculus:
	
	But we have seen previously that $\vec{e}^i\circ\vec{e}_j=\delta_j^i$:
	
	and thus finally:
	
	This having been done let us come back our relation of the vector product:
	
	and let express the components of the 1st line of the determinant in their dual basis (in contravariant coordinates):
	
	Obviously, if the cross product is expressed in covariant components then we have:
	
	Now let us apply the mixed product:
	
	knowing the expression of the determinant of a square $3\times 3$ matrix  (\SeeChapter{see section Linear Algebra page \pageref{3x3 matrix determinant}}) it comes immediately (we can detail on request as always in this book):
	
	Conversely, it comes almost immediately:
	
	But, we have proved in the section of Vector Calculus that $x_i=\vec{x}\circ\vec{e}_i$. It comes then:
	
	and therefore:
	
	The latter relation is often named "\NewTerm{Gram determinant}\index{Gram determinant}". A special very interesting case gives us (we use the relation between the metric components and the dot products of the vector basis we proved during our study of the metric just earlier above):
	
	written in another way:
	
	Thus, the Euclidean volume  is given by what name call the "\NewTerm{functional determinant}\index{functional determinant}" of the system (expression that we will see again and use in the section of General relativity to calculate the real volume and also in the section of String Theory):
		
	that is without units (so you have to multiply it by a factor of elementary volume to get volume units). If we note in another way the determinant:
		
	We get the common relation we can found in many books on General Relativity and String Theory but given without proof and named the "\NewTerm{Riemannian volume}\index{Riemannian volume}\label{Riemannian volume}" form or simply "\NewTerm{volume form}":
		
	or written in the following "\NewTerm{invariant volume element}\index{invariant volume element}" form:
	
	The reader can verify normally easily enough that for the orthonormal Cartesian reference frame we fall back on the volume of a cube and that for the spherical case we fall back well on the expression of the infinitesimal volume of the sphere as used in the section of Geometric Shapes (but on request we can add the details here).
	
	If we use the result we get in the section of Differential Geometry we have therefore for any surface patch:
	

	
	\subsection{Contravariant and Covariant Components}\label{contravariant and covariant components}
	So far we wrote the dummy indices arbitrarily on superscript or subscript at our discretion. However, this is not always allowed and sometimes the fact that a dummy index is in superscript or subscript has a special significance! This is often the major difficulty in the study of some theorems, because if we do not study those indices from the beginning, we do not really know how to interpret the position of the dummy indices. The reader should then be extremely careful at this level.

	For an Euclidean vector space $\mathcal{E}^n$ reported to any base $(\vec{e}_i)$, the scalar product of a vector $\vec{x}=x^i\vec{e}_i$ by a vector its base is written as we know by (remember that this is equivalent as projecting the components on the axis corresponding to $\vec{e}_j$):
	
	Therefore:
	
	This relation is of major importance in theoretical physics and tensor calculus. It is important to remember it when we will study the contraction of indexes later (you can observe in the previous relation that we have "lowered" in the left side the index of the component of the right member of the equality).
	
	These scalar products denoted $x_j$, are named "\NewTerm{covariant components}\index{covariant components}" in the base $(\vec{e}_i)$, of the vector $\vec{x}$. These components are therefore defined by:
	
	They will denoted by lower indices !!! We will see later that these components are naturally introduced for some vectors of physics, for example the gradient vector. Moreover, the notion of covariant component is essential for tensors.
	
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} Never forget that this is therefore the projection of a vector on a vector of its own base!!!!\\
	
	\textbf{R2.} The basic vectors always have their indices noted down because they are their own covariant components (they project on themselves by scalar product). This is the main trick used by beginners to remember when to put lower indices (and therefore they know when to put the upper one...)!
	\end{tcolorbox}
	Conversely, the "\NewTerm{contravariant components}\index{contravariant components}" (in other words: the non-projected components) can be calculated by solving with respect to the $n$ unknowns, the system of $n$ equations:
	
	The previous relations show that the covariant components $x_j$ are related to the conventional components $x^i$ and that the contravariant components $x^i$ are therefore numbers such that:
	
	They will be indicated with superscripts !! The study of the basis changes will further justify the appellation of these different components.
	
	In a canonical orthonormal basis (very special case and corresponding to the classical cartesian, polar, cylindric and shperical coordinates), the covariant and contravariant components are the same as we already know after our study of Gram's determinant. Indeed:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We see above, that the incessant writing  of dummy superscript or subscript indices  can sometimes lead to some confusion and serious headaches ...
	\end{tcolorbox}
	Incidentally, when we refer to a vector (or, more generally, a tensor) as being either contravariant or covariant we're abusing the language slightly, because those terms really just signify two different conventions for interpreting the components of the object with respect to a given coordinate system, whereas the essential attributes of a vector or tensor are independent of the particular coordinate system in which we choose to express it. In general, any given vector or tensor (in a metrical manifold) can be expressed in both contravariant and covariant form with respect to any given coordinate system!
	
	Furhtermore It may seem that the naming convention is backwards, because the "contra" components go with the axes, whereas the "co" components go against the axes, but historically these names were given on the basis on the transformation laws that apply to these two different interpretations.
	
	Most of the physical definitions refer to the contravariant components. But we must be aware in physics that we have a for example a singularity in a tensor, if it is physical or only due to the choice of coordinate system! There is nothing inherent in the contravariant, covariant, or mixed components of for example on stress energy tensor that cannot be changed with a coordinate transformation, EXCEPT for the scalars. 
	
	\pagebreak
	\subsection{Operation in Basis}
	The interest of physicist for the tensor calculus, is passing parameters from one base to another for some given reasons (often the aim is to simplify the study of problems or simply because the studied states depend - or may depend - on the geometry of the space in question). It is therefore necessary to introduce the main tools relating thereto. We will also take this opportunity to present the developments that could have been already addressed in the section of Vector Calculus.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	As I like to say... Tensor Calculus is to physics with is XML to computing science. A background independent language!
	\end{tcolorbox}
	
	\subsubsection{Gram-Schmidt Orthogonalization Method}
	The "\NewTerm{Schmidt orthogonalization method}\index{Schmidt orthogonalization method}\label{gram-schmidt procedure}" (also named "\NewTerm{Gram-Schmidt orthogonalization method}\index{Gram-Schmidt orthogonalization method}") allows the actual determination of an orthogonal basis for any pre-Euclidean vector space $\mathcal{E}^n$ (we could introduce this method in the section of Vector Calculus but it seemed more interesting to us to present this method in a general and aesthetic framework using tensor calculus).
	
	For this, let us consider a set of $n$ linearly independent vectors $(\vec{x}_1,\vec{x}_2,\ldots,\vec{x}_n)$ of $\mathcal{E}^n$ and suppose that for each vector we have the dot product (square of the norm):
	
	Let us seek $n$ vectors $\vec{e}_i$ orthogonal between them. Let us start for this with $\vec{e}_1=\vec{x}_1$ and let us seek for $\vec{e}_2$ orthogonal to $\vec{e}_1$ under the form (this is a choice!!!):
	
	The mental visualization of the process is not quit easy so the reader has to trust (anyway...) the mathematical results (if once we have the time we will draw the process of the classical three dimension case).
	
	The coefficient $\lambda_1$ is calculated by writing the orthogonality relation:
	
	We deduce without too much troubles:
	
	The parameter $\lambda_1$ being determined, we get the vector $\vec{e}_2$ that is orthogonal to $\vec{e}_1$ and not zero, since the system is linearly independent $(\vec{e}_1,\vec{x}_2,\ldots,\vec{x}_n)$.
	
	Thus so far we have:
	
	The parameter $\lambda_1$ being determined, we get the vector $\vec{e}_2$ that is orthogonal to $\vec{e}_1$ and not zero, since the system $(\vec{e}_1,\vec{x}_2,\ldots,\vec{x}_n$ is linearly independent.
	
	The vector $\vec{e}_2$ is sought in the form:
	
	The two relations of orthogonality: $\vec{e}_1\circ\vec{e}_e=0$ and $\vec{e}_2\circ\vec{e}_3=0$, enables the calculation of the coefficients $\mu_1$ and $\mu_2$ . We get therefore:
	
	what determines the vector $\vec{e}_3$, orthogonal to $\vec{e}_1$ and $\vec{e}_2$, and not zero, since the $(\vec{e}_1,\vec{e}_2,\vec{x}_3,\ldots,\vec{x}_n)$ are independent. By continuing the same type of calculation, we get step by step a system of orthogonal vectors $(\vec{e}_1,\vec{e}_2,\vec{x}_3,\ldots,\vec{e}_n)$ between them and none of them are zero.
	
	So finally:
	
	
	In case where some vectors are such like $\vec{x}_i\circ\vec{x}_i=0$ (their norm is zero), we replace then $\vec{x}_i$ by $\vec{x'}_i+\lambda\vec{x}_j$, choosing a vector $\vec{x}_j$ so that we get $\vec{x'}_i\circ\vec{x'}_i\neq 0$.
	
	We therefore conclude that any pre-Euclidean space admits orthogonal bases!
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Suppose:
	
	We want to find an orthonormal set of vector that spans $\{\vec{x}_1,\vec{x}_2,\vec{x}_3\}$. For this we use the Gram-Schmidt followed by a normalisation, let $\vec{e}_1=\vec{x}_1=(1,1,1)$ then we calculate:
	
	As a quick check we have indeed:
	
	Next:
	
	again it's good to check that $\vec{e}_1\circ\vec{e}_2=0$ and $\vec{e}_1\circ\vec{e}_3=0$ as we desire. Finally we notice that:
	
	Hence:
	
	are orthonormal vectors.
	\end{tcolorbox}
	
	This system of calculation of bases is of primary importance! It can be used to study physical systems from a pre-Euclidean repository whose properties change over time. Which is typical in General Relativity. 
	
	The reader interested in computer science and numerical methods can refer to the $QR$ matrix decomposition further below (see \pageref{QR decomposition}) to see another application of the Gram-Schmidt rationalization method.

	\subsubsection{Change of Basis}\label{change of basis tensor calculus}
	Given two bases $(\vec{e}_1,\ldots,\vec{e}_n)$ and $(\vec{e'}_1,\ldots,\vec{e'}_n)$ of a vector space $\mathcal{E}^n$. Each vector of a base can be decomposed on the other basis as follows (we have already prove it):
	
	A vector $\vec{x}$ of $\mathcal{E}^n$, when its contravariant components known, can be decomposed in each base in the form:
	
	and we have already proved that:
	
	We notice that the transformation relations of the components of a contravariant vare the opposite of those of the basis vectors, the quantities $A$ and $A '$ being permutted, hence also the origin of the name "\NewTerm{contra}"-"\NewTerm{variants}" of these components!

	Let $x_i$ and ${x'}_k$ be the covariant components of a vector $\vec{x}$ respectively in the bases $(\vec{e}_i)$ and $(\vec{e'}_k$. Let us replace the basis vectors expressed by the relations:
	
	in the expression of the definition of the covariant components, therefore we get:
	
	Hence the relation between the covariant components in each base:
	
	We get also:
	
	We notice that the covariant components transform as the basis vectors, hence also the name of these components!
	
	Once again, unless the basis is orthonormal, never forget that the covariant and contravariant components are different!!!
	
	\subsubsection{Reciprocal Basis (Dual Basis)}
	Now let us come back on the concept of dual space but as seen in the vector calculation point of view. This second approach can perhaps help some readers to better understand the concepts seen previously but against hides the underlying reasoning for the origin of the names "covariant" and "contravariant". But it is still the most common presentation used in the literature...
	
	Given a basis ($\vec{e}_i$) of an Euclidean vector space $\mathcal{E}^n$. By definition, $n$ vectors $\vec{e}^k$ which satisfy the following relations:
	
	are named "\NewTerm{reciprocal vectors}\index{reciprocal vectors}" of the vectors $\vec{e}_i$. They will be denoted with higher indices. By definition, each reciprocal vector $\vec{e}^k$ must therefore be orthogonal to all the vectors $\vec{e}_i$, except for $k=i$.
	
	Let us first show that the reciprocal vectors $\vec{e}^k$ of a given base $(\vec{e}_i)$ are linearly independent. For this, we must show that a linear combination $\lambda\vec{e}^k$ gives a zero vector if and only if each coefficient $\lambda_k$ is zero.
	\begin{dem}
	Given $\vec{e}=x^i\vec{e}_i$ any vector of $\mathcal{E}_n$. Let us make a dot product by $\vec{x}$ the previous linear combination $\lambda_k \vec{e}^k$, we get:
	
	The latter equality must be verified whatever the $x^i$, it is therefore necessary that each $\lambda_i$ is zero and thus the vectors $\vec{e}^k$ are indeed linearly independent vectors.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem} 
	The system of $n$ reciprocal vectors forms a basis named the "\NewTerm{reciprocal basis}\index{reciprocal basis}" (which is just the dual basis) of the vector space $\mathcal{E}^n$.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Given three vectors $\vec{e}_1,\vec{e}_2,\vec{e}_3$ forming a basis (not necessarily orthonormal!) of an euclidean space. We decide to define first:
	
	where, for recall, the symbol $\times$ represents the cross product (\SeeChapter{see section Vector Calculus page \pageref{cross product}}) and the whole is the  mixed product as also seen in the section Vector calculus and represents an oriented volume.\\
	
	The following vectors:
	
	form the dual basis!\\
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We recognize here the relations we have just proved earlier above during our study of the Gram's determinant!!!
	\end{tcolorbox}
	\end{tcolorbox}
	Now, let us consider a vector on the original base $\vec{e}_1,\vec{e}_2,\vec{e}_3$ that we will denote by (as seen above):
	
	with therefore by definition the contravariant components of the vector that appear as we defined earlier above (and that we had at the same time explained the origin of the name). We also saw above that each contravariant component will also (naturally and by extension) be given by:
	
	Similarly, so we have the covariant components that appear:
	
	In this approach, we then define the contravariant  metric tensor and respectively covariant:
	
	It comes therefore for example for the contravariant components (in the case of a three-dimensional space), knowing that the approach is the same for the covariant components:
	
	And so we find the transformation relations between the  contravariant and covariant components already seen above with the difference that it seems coming out of a hat by successive definitions and that therefore hides the origin of the name of these components (at least in our point of view). But perhaps some readers prefer this approach...???
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	As an example, consider the basis:
	
	Notice that it is not orthogonal because $\vec{e}_1\circ \vec{e}_2=4\neq 0$.\\

	In this case we have by applying the previous Gram's relations:
	
	As we can see in the figure below where $\vec{e}_1$ and $\vec{e}_2$ are shown in green, and the reciprocal vectors  $\vec{e}^1$ and  $\vec{e}^2$ are shown in blue:\\
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/algebra/reciprocal_basis_contravariant_components.jpg}
		\caption[]{Basis vectors, reciprocal basis vectors and contravariant components}
	\end{figure}
	Notice that by construction we have indeed that $\vec{e}^1$ is orthogonal to $\vec{e}_2$ and $\vec{e}^2$ is orthogonal to $\vec{e}_1$!
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	For a given vector $\vec{a}$, say:
	
	we can use the relation proved earlier to find its contravariant components in the basis $(\vec{e}_1,\vec{e}_2,\vec{e}_3)$. We get:
	
	So that (see figure above):
	
	Now observe that the original basis vectors are reciprocal of the reciprocal ones. Thus we can just as well expand the same vector $\vec{a}$ along the basis vectors:
	
	with $a_i=\vec{a}\circ\vec{e}_i$.\\

	The components $a_i$ are the covariant components of $\vec{a}$. In our example, we obtain:
	
	so we have:
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/algebra/reciprocal_basis_covariant_components.jpg}
		\caption[]{Basis vectors, reciprocal basis vectors and covariant components}
	\end{figure}
	\end{tcolorbox}
	
	\subsection{Euclidean Tensors (cartesian tensor)}
	The generalization of the concept of vector has led us to the study of vector spaces to $n$ dimensions. Tensors are also one-dimensional vectors but possess additional properties compared to vectors.

	For the theoretical physicist, tensor calculus is interesting primarily in how the components of the tensor are transformed during a change of basis vector spaces from which they come. We will begin to study them vis-à-vis the properties of bases changes (because it is the most interesting case).

	A tensor is in practice often only defined and used in the form of its components. These can be expressed in covariant or contravariant form like any vector. But a new type of components will appear in the tensor, it is the "mixed components". These three types of components are decomposition of Euclidean tensor on different bases.
	
	\textbf{Definition (\#\mydef):} A "\NewTerm{Cartesian tensor}\index{Cartesian tensor}" uses an orthonormal basis to represent a tensor in a Euclidean space in the form of components.

	Use of Cartesian tensors occurs in physics and engineering, such as with the Cauchy stress tensor (\SeeChapter{see section Continuum Mechanics page \pageref{cauchy stress tensor}}) and the moment of inertia tensor in rigid body dynamics (\SeeChapter{see section Classical Mechanics page \pageref{inertia tensor}}). Sometimes general curvilinear coordinates are convenient, as in high-deformation continuum mechanics, or even necessary, as in General Relativity (\SeeChapter{see section General Relativity page \pageref{general relativity}}).
	
	\subsubsection{Fundamental Tensor}
	During the theory view earlier above, we used the quantities $g_{ij}$, defined from the scalar product of the basis vectors $(\vec{e}_i)$ of a $n$-dimensional pre-Euclidean  vector space $\mathcal{E}^n$, by:
	
	These $n^2$ quantities are the covariant components of a tensor named the "\NewTerm{fundamental tensor}\index{fundamental tensor}" or as we already know the "\NewTerm{metric tensor}\index{metric tensor}".
	
	Let us study how vary the quantities $g_{ij}$ when we make a basis change:

	Given $({e'}_k)$ another based linked to the previous by the known relation:
	
	Substituting the relation $\vec{e}_i={A'}_i^k\vec{e'}_k$ in the expression of $g_{ij}$, it come (we change the indices as it should be done during a substitution):
	
	In the new base $(\vec{e'}_k)$, the dot products of the basis vectors are therefore quantities such that:
	
	So we finally have for the expression of the covariant components $g_{ij}$ in a basis change:
	
	Identically we have:
	
	In general, a sequence of $n^2$ quantities $t_{ij}$ that transforms, during a base change of $\mathcal{E}^n$, according to the two previous relations, namely:
	
	are, by definition, the "\NewTerm{covariant components of a tensor of order two}" (with two indices) on $\mathcal{E}^n$.

	We can therefore manipulate quantities expressing the intrinsic properties of bases as standard tensors!
	
	\subsubsection{Tensor product (dyadic) of two vectors and matrices}\label{tensor product}
	Let us consider an Euclidean vector space $\mathcal{E}^n$ of base $(\vec{e}_i)$ and given two vector of $\mathcal{E}^n$:
	
	Let us form the two by two products of contravariant components $x^i$ and $y^j$, namely:
	
	We thus get $n^2$ quantities, if the two vectors have the same number of components, which are also the contravariant components of a tensor of order two named the "\NewTerm{tensor product}\index{tensor product}" of the vector $\vec{x}$ by the vector $\vec{y}$.
	
	For example for $\vec{x}$ of dimension $2$ and $\vec{y}$ of dimension $3$ we have:
	
	We can also tensorally multiply two matrices $A$ and $B$. Then, the matrix describing the tensor product $A\otimes B$ is the "\NewTerm{Kronecker product}\index{Kronecker product}\label{kronecker product}" of the two matrices as we use it in Relativistic Quantum Physics.
	
	For example, if:
	
	Then:
	
	The most famous case is the covariant tensor of rank $2$ in a space of $4$ dimensions as it is the most used one in tensor calculus:
	
	The reader can also now better understand the origin of the name of the gradient of a vector field (giving a "tensor field") as we saw in the section of Vector Calculus because we can rewrite it now:
	
	 The reader will have certainly notice that through the examples above, the tensor product is non-commutative. That is:
	
	We can obviously build tensor products of order three (thus with $n^3$ terms) such as with the following tensor three times  contravariant vectors:
	
	etc.

	Let us study the properties of the basis changes of these components. Let us use for the basis changes relations of contravariant components of a vector, namely:
	
	Let us replace in the relation $u^{ij}=x^iy^j$ the components $x^i$ and $y^i$ by their basis change expression, we get:
	
	The quantities ${u'}^{kl}$ are the new components:
	
	The transformation formula of the $n^2$ quantities $u^{ij}$ on a change of basis change of $\mathcal{E}^n$ is finally (very similar to metric tensor):
	
	Such a change basis relation characterizes the contravariant components of a tensor of order two. Conversely, we get:
	
	So the $n^2$ quantities are the "\NewTerm{contravariant components of a tensor of order two}\index{contravariant components of a tensor of order two}".

	We can the build the same products by pairs for covariant components $x_i$ and $y_i$ of the vectors $\vec{x}$ and $\vec{y}$ thus:
	
	The formulas of basis change of the covariant components of the vectors are given by the following relations that we have already proved previously:
	
	Substituting the first relation in the product $u_{ij}=x_iy_i$, we get:
	
	This is the basis change relation of covariant components of a tensor of order two. We also easily check that we have:
	
	Identically we have of course ${u'}_{kl}={x'}_k{y'}_l$ since $u_{ij}=x_iy_i$.

	So the $n^2$ quantities are then the "\NewTerm{covariant components of a tensor of order two}\index{covariant components of a tensor of order two}".

	Let us now create $n^2$ quantities my multiplying two by two the covariant components of a vector $\vec{x}$ by contravariant components of a vector $\vec{y}$, we get:
	
	Let us perform a basis change in this last relation taking into account the expressions $x_i={A'}_i^k{x'}_k$ and $x^i={A}_k^i {x'}^k$, we get:
	
	This basis change relation characterizes the "\NewTerm{mixed components}\index{mixed components}" of an order two tensor. Conversely, we can verify that we have:
	
	These mixed components also constitutes the components of a tensor product of $\vec{x}$ by $\vec{y}$, according to a given basis.
	
	In general, a sequence of  $n^2$ quantities that transforms, during a basis of $\mathcal{E}^n$, just as previously established relation are therefore, by definition, "\NewTerm{mixed components of a tensor of order two}\index{mixed components of a tensor of order two}".
	
	\pagebreak
	\subsubsection{Tensor Spaces}
	In the previous study, we used as system of $n^2$ number, created from a vector space $\mathcal{E}^n$. When these numbers satisfy some basis change relations, we name these quantities, by definition, "\NewTerm{components of a tensor}\index{components of a tensor}".

	We have seen that any linear combination of these components constitutes the components of a new tensor. We can therefore add together the components of the tensor and multiply by a scalar, to get other components of a new tensor. These addition and multiplication properties mean that we can use these tensors quantities as vector components.
	
	To clarify how we define a tensor on a base, let us study the particular case of a tensor product of two vectors formed by triplets of numbers (that is to say in $\mathbb{R}^3$ typically). Consider therefore the Euclidean vector space $\mathcal{E}^3$ whose vectors are triplets of number of the form $\vec{x}=(x_1,x_2,x_3)$. The canonical orthonormal basis of $\mathcal{E}^3$ consists of three vectors that we know very well but written in tensor calculs as:
	
	with $i=1,2,3$ (nice way to write simple things isn't it...).
	Vectors of $\mathcal{E}^3$ gives the possibility to form the nine quantities that we have named the "\NewTerm{components of the tensor product}\index{components of a tensor product}" of the vectors $\vec{x}$ and $\vec{y}$.
	
	If we make all possible tensor products between vectors of $\mathcal{E}^3$, we get sequences of $9$ numbers that can be used to define the following vector:
		
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We see immediately with the above relation and the previous relation that the tensor product is therefore not commutative.
	\end{tcolorbox}
	We are left then with the elements of a vector space $\mathcal{E}^9$ with nine-dimensional space, whose elements all combinations by pairs of three numbers.
	
	We then say that $\mathcal{E}^9$ has a "\NewTerm{tensor product structure}\index{tensor product structure}" which is denoted obviously and in standard calculus by $\mathcal{E}^9:\; \mathcal{E}^3\otimes \mathcal{E}^3$ or sometimes $\mathcal{E}_3^{(2)}$.

	These vectors can be decomposed, for example, on an orthonormal canonical basis:
	
	with $k=1,2,\ldots,9$.

	If we rewrite the quantities $x^iy^j$ according to their place in the expression of $\vec{U}$, ie:
	
	with $k=1,2,\ldots,9$ and $i,j=1,2,3$, the vectors $\vec{U}$ are then written:
	
	and is an example as we know of tensor of order $2$ (obviously we can generalize this approach).
	How do these tensor differ from ordinary vectors? Although they are identical to some vectors of $\mathcal{E}^9$ in our example but were formed by the vectors $\vec{x}$ and $\vec{y}$ of $\mathcal{E}^3$. To remember this fact, we write then as we already know:
	
	and they are named as we already know "tensor products of order two" of the vectors $\vec{x}$ and $\vec{y}$. The symbol $\otimes$ is defined in the way we have formed the quantities $x^iy^j=u^{ij}$ and in the order in which they were classified them to form the vector $\vec{U}$.

	To recall the dependence between a quantity $x^iy^j=u^{ij}$ and the basis vector $\vec{e}_i$ to which he is assigned, les us rewrite these vectors by putting in place of the index $k$ the two indices $i$ and $j$, relative to the components, namely:
	
	The latter can be written in the form:
	
	The vectors $\vec{e}_i\otimes\vec{j}$ generates a basis of $\mathcal{E}^9$ in the case of our example with is same the "\NewTerm{tensor associated basis}\index{tensor associated basis}".
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider:
	
	We then have for example:
	
	That is to say:
	
	\end{tcolorbox}
	It follows that as element of a space $\mathcal{E}^n\otimes \mathcal{E}^n$, a tensor $\vec{U}$ is a vector of the general form:
	
	Let us study its properties vis-à-vis a base change of $\mathcal{E}^n$ such that:
	
	During such a base change, the base $(\vec{e}_i\otimes\vec{e}_j)$ associated to $\vec{e}_i$  becomes another base $(\vec{e}_k^{\prime}\otimes\vec{e}_l^{\prime})$ associated to $\vec{e}_k^{\prime}$, that is:
	
	It follows that the tensor product $\vec{U}$ has for components in the new basis:
	
	We have the following properties for the tensor product given:
	
	\begin{enumerate}
		\item[P1.] Right/Left distribituvity relatively to the addition of vectors:
		
		The proof of these relation is simple deduce from the definition of the tensor product. Indeed, we have for example:
		
		
		\item[P2.] Associativity with multiplication by a scalar:
		
		Indeed, we have:
		
	
		\item[P3.] When we choose a base in each of the vector spaces $(\vec{e}_i)$ for $\mathcal{E}^n$, $(\vec{f}_i)$ to for $\mathcal{F}^m$, the $n\cdot m$ elements of $G_{nm}$ that we denote by $\vec{e}_i \otimes\vec{f}_i$ also form a basis of $G_{nm}$.
		\begin{dem}
			Already made in the particular example we used earlier above.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In practice, we often have to use tensor formed from vectors belonging to the same vector spaces $\mathcal{E}^n$.
	\end{tcolorbox}
	We can of course generalize the tensor product to any number of vectors. Gradually, given the property P1, we can consider $p$ vectors $\vec{x}_1,\vec{x}_2,\ldots,\vec{x}_p$ each belonging to different vector spaces $\mathcal{E}^{n_1},\mathcal{E}^{n_2},\ldots,\mathcal{E}^{n_p}$. If we have:
	
	we can form the tensor product:
	
with $i_1=\{1,\ldots,n_1\},i_2=\{1,\ldots,n_2\},\ldots,i_p=\{1,\ldots,n_p\}$.

	We build thus tensor products of order $p$ belonging to the vector space $\mathcal{E}^{n_1}\otimes\mathcal{E}^{n_2}\otimes\ldots \otimes\mathcal{E}^{n_p}$, space that has a product structure tensor. The elements of this space are by definition tensor of order $p$.

	In order to unify the classification, the elementary vector spaces, which can not be fitted with a tensor product structure can be regarded as having components of a tensor of order $1$. In general, we name these elements "\NewTerm{vectors}\index{vector}", reserving the term "\NewTerm{tensor}\index{tensor}" to elements of tensor spaces of order equal or greater than $2$!
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	It is of usage to name "\NewTerm{tensor of order zero}\index{tensor of order zero}" scalar quantities. It is also rare to meet tensor of order tensor greater than $2$.
	\end{tcolorbox}
	It is quite obvious and we will not do the proof  that we absolutely can redefine all the concepts (base, decomposition on a base, reciprocal basis, dot product, tensor product) that we have seen so far considering tensor of order $1$ as a vector (we should therefore rewrite everything that was already written above... which is useless in our point of view).

	It is also quite possible to repeat all these definitions for higher order tensor and thus generalize the concept of space tensor for all dimensions.

	From these considerations, we can state the "\NewTerm{tensoriality criterion}\index{tensoriality criterion}":
	
	So that the elements of a sequence of $n^p$, relative to a base of a vector space $\mathcal{E}_{(p)}^n$, can be considered as the components of a tensor, it is necessary and sufficient that these quantities to be linked together, in two different bases of $\mathcal{E}_{(p)}^n$, by the relations of transformation of the components.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	A vector can be represented in any base by a sequence of $n$ components. However, we can not conclude that any sequence of $n$ numbers is a vector. Indeed, when we put ourselves in another base of space, the components must also change to represent the same object, then we say that the vector is an intrinsic object (whose existence does not depend on the choice of the base). It remains then to know that a vector is a tensor of order $1$.	
	\end{tcolorbox}

	\subsubsection{Linear combination of tensors}
	We can form other tensor by combining together the components of various tensor products defined using vectors of the same vector space. For example, let us consider the contravariant components of the tensor products of the vectors $\vec{x},\vec{y}$ and $\vec{w},\vec{z}$ :
	
	Let us form the following quantities:
	
	The $n^2$ quantities $t^{ij}$ also satisfied the general formulas for basis change. We have indeed by substituting the relations of transformation of the  contravariant components of a tensor product in the previous expression:
	
	The $n^2$ quantities $t^{ij}$, satisfying the relations of basis change also constitutes components of a tensor of order two.

	\subsubsection{Contraction of indices}
	Let us consider the mixed tensor product of two vectors $\vec{x}$ and $\vec{y}$ of respective contravariant $x^i$ and covariant $y_j$ components . The mixed components of the tensor product $\vec{V}$ of these two vectors are:
	
	Let us perform the addition of the various components of the tensor $\vec{V}$ such as $i=j$, ie:
	
	We thus get the expression of the dot product of vectors $\vec{x}$ and $\vec{y}$. The quantity $v$ is a scalar (tensor of order zero). Such an addition on different variance indices constitutes, by definition, the operation of "\NewTerm{contraction of indices}\index{contraction of indices}" of the tensor $\vec{V}$. This operation allowed us to move from a tensor of order two to a tensor of order zero. The tensor $\vec{V}$ has been amputated and of a covariance and a contravariance.
	
	Let us also take the example of a tensor $\vec{U}$ whose mixed components are one time covariant and one two times contravariant $u_k^{ij}$ (caution ... it is not a three-dimensional matrix but simply an indication that the components of this tensor are expressed from three other variables!!!). Let us consider some of its components such as $k=j$, that is the components $u_j^{ij}$ and let us perform the addition of the latter. We then get:
	
	These new quantities $v^i$ form the components of a tensor $\vec{V}$ of order one (thus a vector!) and constitute what we name then the "\NewTerm{contracted components}\index{contracted components}" of the tensor $\vec{U}$ and of course meet the basis relations change (on request we can prove it but you have to know that it is similar to the one we made for vectors). So we have indeed change form a tensor of order three to a tensor of order one!
	
	So we can see that the underlying idea of the contraction is to allow us to facilitate the resolution of a purely mathematical problem and depending on the situation it may be good  to raise or reduce the order of a tensor. This is often a choice that is made by trial and error based on a specific context or that naturally arises from a purely mathematical or mathematical-physical development (as we will see examples further below).

	\paragraph{Raising and lowering indices}\mbox{}\\\\
	If we start with a contravariant or covariant components tensor , we can lower / raise one or more indices by multiplication (if repeated) by $g_{ij}$ or $g^{ij}$ (unitary diagonal and positive signature: of canonical type) to get mixed components on which we can then perform contraction operations.
	
	Let us consider an Euclidean tensor $\vec{U}$ of contravariant components $u^{i_1i_2i_3\ldots i_p}$

	If we want to perform a contraction on this tensor, we will first need to transform it into a mixed tensor. This transformation we be done using a fundamental tensor.

	Let us write $\vec{U}$ in mixed components by lowering at the covariant position the index $i_1$ by example (it is therefore equivalent to express this in contravariant component in covariant component). So:
	
	We see well that in this case to take down a contravariant index in a tensor using a fundamental tensor, we must first go search in the covariants components of the fundamental tensor the one who is itself in contravariant position in the original tensor and replace its position (but this time in covariance) by the other index of the fundamental tensor (it is the same idea when it we desire to operate a contraction on a covariant tensor ).
	
	Indeed, let us recall that we have proved that:
	
	Also remember that raising and then lowering the same index (or conversely) are inverse operations, which is reflected in the covariant and contravariant metric tensors being inverse to each other:
	
	Now that we got a mixed tensor components, we can very well getting contract the indices. Let us choose for example the index $i_2$ and let us perform the contraction with the index $j_1$, let us put $i_2=j_1=k$ (we are then concerned only to some specific terms), then just writing the whole process from the beginning:
	
	So we get after lowering the index and one contraction, a tensor of order $p-2$.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Let us see a first example of raising down and contracting the covariant $4$-position first order tensor given by (\SeeChapter{see section Special Relativity page \pageref{four-vector displacement}}):
	
	in components:
	
	(where $x_j$ are the usual Cartesian coordinates) and the Minkowski metric tensor with signature $(-+++)$ given by (\SeeChapter{see section Special Relativity page \pageref{minkowski metric}}):
	
	in components:
	In components:
	
	To raise the index, multiply by the tensor and contract:
	
	Then for $\lambda = 0$:
	
	and for $\lambda = j = 1, 2, 3$:
	
	So the index-raised contravariant $4$-position is:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	E2. For a second order tensor example let us consider the contravariant electromagnetic tensor in the $(+---)$ signature is given by (\SeeChapter{see section Electrodynamics page \pageref{electromagnetic tensor}}):
	
	in components:
	
	To obtain the covariant tensor $F_{\alpha\beta}$, we multiply by the metric tensor and contract:
	
	and since $F^{00} = 0$ and $F^{0i}=-F^{i0}$, this reduces to:
	
	Now for $\alpha = 0, \beta = k = 1, 2, 3$:
	
	and by antisymmetry, for $\alpha = k = 1, 2, 3$, $\beta = 0$:
	
	then finally for $\alpha = k = 1, 2, 3$, $\beta = \ell = 1, 2, 3$:
	
	The (covariant) lower indexed tensor is then:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	E3. We will also see further below an example where we contract a tensor of order $1$ (one of the contravariant components of the vectors of the spherical base) having already a lowered index:
	
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} In the relation:
	
	the equality is an abusive notation that we can found in some books (because strictly speaking we should do the calculation in two steps).\\
	
	\textbf{R2.} As a result of the symmetry of the quantities $g_{ij}$ (the dot product is commutative), this latter tensor is identical to what we would get to the position by lowering to the covariant position the index $i_2$, and then by doing the contraction of the index $i_1$ with the index $j_2$.\\

	Let see this:\\

	The symmetry $g_{ij}=g_{ji}$ takes the form (this may seem confusing but let us remember that the number of the component $i$ indicates the place of this component):
	
	Therefore it comes:
	
	and putting $i_1=j_2=k$:
	
	\end{tcolorbox}
	In general, the contraction of a tensor allows to form a tensor of order $p-2$ from a tensor of order $p$. We can of course repeat the operation of contraction. Thus, an even tensor, $2p$, will become a scalar after $p$ contractions and an odd order tensor $2p+1$, will become a vector.

	We can extend after this definition of the contraction of indices, the tensoriality criterion. We have seen until now two ways to recognize the character of a tensor of a sequence of quantities:
	\begin{itemize}
		\item The first is to show that these quantities are formed by the tensor product of component vectors or by a sum of tensor products;

		\item The second is to study how these quantities are converted during a basis change and to check the conformity of the relations of transformation;

		\item The third and new one that brings to put that for a set of $n^{p+1}$ quantities, having $p$ upper and $q$ lower indexes to be a tensor, it is necessary and sufficient that their product fully contracted by the contravariant components of any $p$ vectors and the covariates components of any $q$ vectors, to be a quantity (the norm in fact...) that remains invariant under basis change.
	\end{itemize}
	
	\subsection{Special Tensors}
	We may face in theoretical physics and engineering with tensors that have interesting properties. To avoid redundant work in each case, we will list here and proved the various existing properties used in this book in other sections and discuss their possible implications briefly (the detailed analysis being reserved for their application in the other same sections of the book).
	
	\subsubsection{Symmetric Tensor}\label{symmetric tensor}
	Consider a tensor $\vec{T}$ of order two of contravariant components $T^{ij}$. Let us suppose that, following a base $(\vec{e}_i)$, all these components satisfy the relations:
	
	On another base $(\vec{e}_j^{\prime})$, related to the previous by the known transformation relations, the new components of ${T'}^{lm}$ satisfy the relation:
	
	We see that the property $T^{ij}=T^{ji}$ is therefore an intrinsic characteristic of the tensor $\vec{T}$, independent of the base! We then say that the tensor is a "\NewTerm{symmetric tensor}\index{symmetric tensor}" (we will come back again on this concept a little further below) also named "\NewTerm{totally invariant tensor}\index{totally invariant tensor}" (implicitly meaning: by base change).

	The symmetry property is also true for the covariant components of a symmetric tensor since we have:
	
	Conversely, the symmetry of the covariant components implies that of the contravariant components.

	For higher order tensor, the symmetry may be partial, being valid only for two covariant indices or two contravariant indices. Thus, a fourth order tensor, of mixed components $T_l^{ijk}$, may also be symmetrical in $i$ and $j$, for example, given:
	
	We check, as above, that such a property is intrinsic.
	
	A tensor is said to be "\NewTerm{completely symmetrical tensor}\index{completely symmetrical tensor}" if any transposition of two indices with the same variance, changes the corresponding component into itself. For example, for a tensor of order three $T^{ijk}$, completely symmetric, we have the following components that are equal:	
	
	There are many examples of symmetric tensors. Some include:
	\begin{itemize}
		\item the metric tensor $g_{\mu \nu }$ (\SeeChapter{see section General Relativity page \pageref{metric tensor}}) 
		\item the Einstein tensor $G_{\mu \nu }$ (see further below) 
		\item the Ricci tensor $R_{\mu \nu }$ (see also further below).
		\item the stress and strain tensor for fluids or solids  $\sigma_{ij}$ (\SeeChapter{see section Continuum Mechanics page \pageref{cauchy stress tensor}})
		\item  the Lorentz boost tensor $\Gamma_\nu^\mu$ (\SeeChapter{see section Special Relativity page \pageref{lorentz boost tensor}}) 
		\item ...
	\end{itemize}
	We can also (very interesting curiosity) obtain a geometric representation of the values of the components of a symmetric tensor of order two!! 
	
	For this let us, consider in the ordinary geometric  space coordinates $x^i$, the following equation:
	
	where, for recall, $x^ix^j$ can be seen as a tensor with $i,j=1,2,3$ and where the $a_{ij}$ are real given coefficients. Let us suppose that coefficients are such that:
	
	The above equation is then:
	
	Here we fall back on the equation of a surface of the second degree of a quadratic similar to of the plan that we saw in the section of Analytical Geometry. We know by extension in to three dimensions that these surfaces are ellipsoids or hyperboloids, depending to the values of the quantities $a_{ij}$.
	
	Let us study how the quantities $a_{ij}$ transform  when we make a change of coordinates such as:
	
	The equation of the quadric is written in this new coordinate system:
	
	Hence the expression of the coefficients in the new system of axes:
	
	The coefficients $a_{ij}$ therefore transform as the covariant components of a tensor of order two. Conversely, if the quantities $a_{ij}$ are the components of a symmetric tensor, these components define the coefficients of a quadric!! There is therefore a certain equivalence between a symmetric tensor and the coefficients of a quadratic...!!! We say then that the equation of the quadric is the "\NewTerm{quadric representation of a symmetric tensor}\index{quadric representation of a symmetric tensor}" or "\NewTerm{representation surface}".
	
	So the representation surface (or representation quadric) is a geometrical representation of a second rank symmetric tensor and is useful for giving us a visual image of the tensor as well as being useful for example in calculating magnitudes of material properties described by second rank symmetric tensors!!
	
	We know from our study of quadrics in the section of Analytical Geometry (by extending it to the three-dimensional case) that we can always find a coordinate system relative to which the equation of a quadratic takes a simpler form:
	
	In this case, the basis vectors are supported by the main axes of the quadric. In this coordinate system, the components of the tensor equation reduce to:
	
	and $a_{ij}=0$ for the other components. The quantity $b_i$ equation are named the "\NewTerm{principal values}\index{principal values}" of the tensor $a_{ij}$.

	If the quantities $b_1,b_2,b_3$ are positive, the surface is an ellipsoid, if two quantities are strictly positive and the third strictly negative, we have a one sheet hyperboloid, if two quantities are strictly negative and the third positive, we have two sheets hyperboloid (\SeeChapter{see section Analytical Geometry page \pageref{two sheets hyperboloid}}).
	
	Comparing the expression of the quadric obtained previously with the classic equation:
	
	where $a$, $b$, $c$ are the semi-axes of an ellipsoid shows that we have:
	
	Below we can see a screen shot of an interactive tool of Cambridge University to play with the ellipcity (only!) representation quadric of a rank two symmetric tensor:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/algebra/symmetric_tensor_represntation_quadric_cambridge.jpg}
		\caption[Visual link between a tensor and its representative surface]{Visual link between a tensor and its representative surface \\(source: \href{http://www.doitpoms.ac.uk/tlplib/tensors/representation.php}{http://www.doitpoms.ac.uk})}
	\end{figure}
	
	\subsubsection{Antisymmetric Tensor}
	When the contravariant or covariant components of a tensor of order two, satisfy the property:
	
	we the say that the tensor is a "\NewTerm{antisymmetric tensor}\index{antisymmetric tensor}\label{antisymmetric tensor}".  In other words a tensor is antisymmetric on (or with respect to) an index subset if it alternates sign ($+/-$) when any two indices of the subset are interchanged.
	
	It follows from this definition that an antisymmetric tensor must obviously satisfy the fact that its diagonal components are all zero, such as for example for a rank $2$ tensor:
	
	
	A well known antisymmetric tensor is the electromagnetic tensor $F_{\mu \nu }$ (\SeeChapter{see section Electrodynamics page \pageref{electromagnetic tensor}}).
	
	For example a covariant tensor of order three $T_{ijk}$ will be say to be symmetric in $i$ and $k$ if for all values that can take the indices, we have:
	
	Or the fourth order covariant tensor $T_{ijkl}$ will be said in antisymmetric on $i$ and $l$ if for all values that can take the indices, we have:
	 
	
	A tensor will "\NewTerm{totally antisymmetric}\index{totally antisymmetric tensor}" if any transposition of index of same variance (covariant/contravariant) changes the corresponding component into its opposite.

	\label{decomposition square matrix symmetric and antisymmetric} A tensor $T_{ij}$ can be put in the form of a sum of a symmetric tensor and an antisymmetric tensor. Indeed, we have:
	
	The first term of the sum above is a symmetric tensor and the second, an antisymmetric tensor.
	\begin{dem}
	Consider first that $T_{ij}$ is symmetric, then we have:
	
	So this proves that the left term is indeed symmetric for this special case.
	
	Consider secondly that $T_{ij}$ is symmetric, then we have:
	
	So this proves that the left term is indeed antisymmetric for this special case.

	Now if $T_{ij}$ is neither symmetric or antisymmetric we have:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	A tensor $T_l^{ijk}$ will be "\NewTerm{partially antisymmetric}\index{partially antisymmetric tensor} if for example we have:
	
	That is to say antisymmetric only for a subset of indices.
	Let s now consider two vectors $\vec{x}=x^i\vec{e}_i$ and $\vec{y}=y^j\vec{e}_j$ of a vector space $\mathcal{E}^n$. Let us form the following antisymmetric quantities (we can see in it two tensor products):
	
	where we see immediately that the components $T^{ij}$ are those of an antisymmetric tensor $\vec{T}$ by construction as:
	
	The decomposition of the vector $\vec{T}$ in the base $\vec{e}_i\vec{e}_j$ is:
	
	The tensor $\vec{x}\otimes\vec{y}$ (written as it in analogy to the vectors cross product for $n=3$ ) is named the "\NewTerm{outer product}\index{outer product}" of the vectors $\vec{x}$ and $\vec{y}$. We say that this tensor is a "\NewTerm{bivector}\index{bivector}".
	
	The outer product is an antisymmetric tensor which satisfies the following properties:
	\begin{enumerate}
		\item[P1.] Anticommutativity:
		
		the result is:
		

		\item[P2.] Left and right distributivity for the vector addition:
		

		\item[P3.] Associativity for the multiplication by a scalar:
		

		\item[P4.] Outer products:
		
	\end{enumerate}
	constitute a base for all bivectors.
	\begin{dem}
	An antisymmetric tensor $\vec{T}$ of order two, element of $\mathcal{E}_{(2)}^2$ can, as we have proved it earlier, be written as:
	
	Exchanging, in the last sum of the above relation, the name of the indices and considering that $T^{ij}=-T^{ji}$, we get:
	
	The elements:
	
	are linearly independent vectors since the vectors $\vec{e}_i\otimes\vec{e}_j$ also are it. These elements constitute a base on which the antisymmetric tensor can be decomposed.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	The number of distinguishable vectors $\vec{e}_i\otimes\vec{e}_j-\vec{e}_j\otimes\vec{e}_i$ is equal to the number of combinations of vectors taken two by two and distinguishable among $n$ such that (\SeeChapter{see section Probabilities page \pageref{choice function}}):
	
	Indeed among the $n^2$ components, $n$ components are equal to zero and $n(n-1)$ other components have opposed values to two by two. So we can consider that half of the latter is sufficient to characterize the tensor.

	In the context of the outer tensor product where we have:
	
	the number of distinguishable components is also equal to $n(n-1)/2$ and they are named "\NewTerm{strict components}\index{strict components}".
	
	We notice that for $n=3$, the strict number of components of the outer product of two vectors is also equal to three. This allows to form, with the components of the bivector, the components of a cross product $\vec{z}$.

	Thus, a cross product therefore exists only for a subspace of bivectors whose number of dimensions is equal to $3$ and the pre-images that are antisymmetric tensors.

	If all these conditions are satisfied, we say that the vector 
$\vec{z}$ is the "\NewTerm{adjoint tensor}\index{adjoint tensor}" of tensor $\vec{T}$.

	\subsubsection{Fundamental Tensor}
	We saw at the beginning of our study of Tensor Calculus the definition of the components of the fundamental covariant tensor $g_{ij}$, that is for recall:
	
	These quantities are involved, as we know it (see previous topics), in the expression of the dot product of two vectors $\vec{x}$ and $\vec{y}$ of contravariant components $x^i$ and $y^i$, given by the relation (for recall):
	
	Let us use the general test of tensoriality to highlight the tensor character of the $g_{ij}$. The previous expression is a product fully contracted of the quantities $g_{ij}$ with the contravariant components  $x^iy^i$ of an arbitrary tensor. As the dot product is an invariant quantity (in this case a scalar) with respect to the base changes, it follows that the $n^2$ quantities $g_{ij}$ are the covariant components of a tensor.

	This tensor is also symmetrical as a result of the symmetry property of the dot product of the basis vectors such that:
	
	We have the same for the contravariant components of the fundamental tensor:
	
	If we denote by $g_j^i$ the mixed components of a fundamental tensor to himself:
	
	obviously with the canonical basis:	
	
	
	\subsection{Curvilinear Coordinates}
	The conventional concepts of coordinate system can be generalized as we know to any specific $n$-dimensional punctual space (\SeeChapter{see section Principia page \pageref{point spaces}}). We name "\NewTerm{coordinate system}\index{coordinate system}" in $\mathcal{E}^n$, any mode of definition of a point $M$ in the considered system.
	
	\textbf{Definitions (\#\mydef):} 
	\begin{enumerate}
		\item[D1.] For a given coordinates system (Cartesian, spherical, cylindrical, polar ...) system we name "\NewTerm{coordinate line}\index{coordinate line}" the "place" of the points $M$ when a only single coordinate of $M$ varies, the other being keep as constant.

		\item[D2.] A "\NewTerm{curvilinear coordinates}\index{curvilinear coordinates}\label{curvilinear coordinates tensor calculus}" are a coordinate system for Euclidean space in which the coordinate lines may be curved. These coordinates may be derived from a set of Cartesian coordinates by using a transformation that is locally invertible (a one-to-one map) at each point (\SeeChapter{see section Vector Calculus page \pageref{system of coordinates}}).
	\end{enumerate}
	 This means that one can convert a point given in a Cartesian coordinate system to its curvilinear coordinates and back. The purpose as we already know is that depending on the application, a curvilinear coordinate system may be simpler to use than the Cartesian coordinate system. For instance, a physical problem with spherical symmetry defined (for example, motion of particles under the influence of central forces) is usually easier to solve in spherical polar coordinates than in Cartesian coordinates.
	 
	 Well-known examples of curvilinear coordinate systems are as we know in three-dimensional Euclidean space the Cartesian, cylindrical and spherical polar coordinates.
	 
	 Let us first study the generalization of a coordinate system relative to a fixed reference frame (we urge the reader to read first the subsection about Coordinate Systems in the section of Vector Calculus and the subsection of Analytical Mechanis in the section Principia).
	 
	 Let us consider a punctual space $\mathcal{E}^n$ and $(\vec{e}_i)$ a reference frame of that space. Given $x_i$ the rectilinear coordinates of a point $M$ of $\mathcal{E}^n$ relatively to this reference fame. Any curvilinear coordinate system $u^k$ (with $k=1,2,\ldots,n$ is obtained by giving $n$ arbitrary functions $f^i$ of parameters $u^k$, such that:
	 
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[x=(10:4cm),y=(90:4cm),z=(225:4cm),>=Triangle,scale=1.5]
		\coordinate (O) at (0,0,0); 
		\draw [->] (O) -- (1,0,0) node [at end, right] {$x^2$ axis};
		\draw [->] (O) -- (0,1,0) node [at end, above] {$x^3$ axis};
		\draw [->] (O) -- (0,0,1) node [at end, left]  {$x^1$ axis};
		
		\draw [draw=blue, -Circle] (O) to [bend left=8] 
		  coordinate [pos=7/8] (q2n) 
		  (1,-1/4,0) coordinate (q2) node [right] {$u^2$};
		\draw [draw=blue, -Circle] (O) to [bend right=8] 
		  coordinate [pos=7/8] (q3n) 
		  (0,1,1/2) coordinate (q3) node [left] {$u^3$};
		\draw [draw=blue, -Circle] (O) to [bend right=8] 
		  coordinate [pos=7/8] (q1n) 
		  (1/4,0,1) coordinate (q1) node [right] {$u^1$};
		
		\begin{pgfonlayer}{background}
		\begin{scope}
		\clip (O) to [bend left=8] (q2) -- (1,1,0) -- (q3n) to [bend right=8] (O);
		\shade [left color=green, right color=green!15!white, shading angle=135]
		  (O) to [bend left] (q3n) to [bend left=16] (3/4,1/2,0) to [bend left=16] (q2n) -- cycle;
		\end{scope}
		
		\begin{scope}
		\clip (O) to [bend left=8] (q2) -- (1,0,1) -- (q1) to [bend left=8] (O);
		\shade [left color=red, right color=red!15!white, shading angle=45]
		  (O) to [bend right] (q1n) to [bend left=16] (1,0,1) to [bend left=16] 
		  (q2n) to [bend right] (O);
		\end{scope}
		
		\begin{scope}
		\clip (O) to [bend right=8] (q1) -- (0,1,1) -- (q3) to [bend left=8] (O);
		\shade [left color=cyan, right color=cyan!15!white, shading angle=225] 
		  (O) -- (q1n) to [bend right=16] (0,1,1) to [bend left=16] (q3n) 
		to [bend left] (O);
		\end{scope}
		\end{pgfonlayer}
		
		\node at (1/3,1/3,0) {$q_1=\mbox{const}$};
		\node at (0,1/2,1/2) {$q_2=\mbox{const}$};
		\node at (1/2,0,1/3) {$q_3=\mbox{const}$};
		\end{tikzpicture}
		\caption{Coordinate surfaces, coordinate lines, and coordinate axes of general curvilinear coordinates}
	\end{figure}
	We will assume thereafter that these $n$ functions satisfy the following three properties:
	\begin{itemize}
		\item[P1.] They must be of class at least $\mathcal{C}^2$ (differentiable at least twice for the needs of physics: speed and acceleration). This assumption implies, that at any point where it is satisfied, that we have the permutation of derivations (with respect to the two partial derivaties as seen in the section of Differential and Integral Calculus):
		

		\item[P2.] These functions are such that we can solve the system of $n$ equations of coordinate system change relatively to the variables $u^k$ and express them in function of the parameters $x^i$, thus:
		
		still with $k=1,2, \ldots,n$.

		\item[P3.] When the variables $u^k$ vary in a domain $\Delta$, the variables $x^i$ vary in a domain $\Delta'$ (think to cartesian$\leftrightarrow$ spherical coordinates where in cartesian the variable range is infinite when in spherical both of them are limited a typical $2\pi$ width range ). 

		\item[P4.] The Jacobian of the functions $x^i=f^i(u^1,u^2,\ldots,u^n)$, defined by (\SeeChapter{see section Differential and Integral Calculus page \pageref{jacobian}}):
		
		will be assumed different from zero in the domain $\Delta$ (and also the Jacobian $\mathrm{D}(\partial_i u^k)$ of the functions $u^k=g^k(x^1,x^2,\ldots,x^n)$) and is the inverse of the Jacobian of $u^k=g^k(x^1,x^2,\ldots,x^n)$. If the jacobians exist, they are not zero as a result primarily of the second property above and implicitly the first.
	\end{itemize}
	As we have already mention if we fix $(n-1)$ parameters $u^k$ by varying only one parameter, $u^i$ for example, we get the coordinates $x_{(1)}^i$ of a set of points $M$ that constitute a "coordinate line line". In general, the coordinate lines are not straight but curved as we know. These coordinates $u^k$ are named for this reasons the "curvilinear coordinates". On a point $M$ of $\mathcal{E}^n$ intersect elsewhere $n$ coordinate lines (see figure above).
	
	We proved in the section of Analytical Mechanics, during our study of punctual spaces (see page \pageref{point spaces}), that partial derivatives of a vector $\overrightarrow{\text{O}M}$ are independent of the point O (origin) of a given reference frame. If $\mathcal{E}^n$ is assimilated to a system of curvilinear coordinate, we write:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	A famous example of curvilinear coordinates, where each $u^k$ is a uniform function of the rectilinear coordinates $x^k$, the being moreover $u^k$  continuous functions at the current point $M$, is that of the spherical coordinates where we have (\SeeChapter{see section Vector Calculus page \pageref{spherical coordinates}}):
	
	and where for recall:
	
	Let us also recall that during our study of the system of spherical coordinates in the section of Vector Calculus we obtained after normalization of the basis vector:
	
	Therefore:
	
	with:
	
	\end{tcolorbox}
	In a non-Euclidean space, we can not define a unique valid basis over the whole space. Thus, we construct a base at each point separately and for this purpose we use the curvilinear coordinates such that at each point $M$, the base vectors $\vec{e}_k$ are tangent to the corresponding coordinate line equation via the relation given above:
	
	Given now $u^1, u^2, \ldots, u^n$ the curvilinear coordinates of the point $M$ with respect to a Cartesian basis $(\vec{e}_i^0)$. In this reference frame, we obviously have:
	
	where the Cartesian coordinates are functions $x^i=x^i(u^1,u^2,\ldots,u^n)$.
	
	The vector $\vec{e}_k$ therefore also be expressed by:
	
	The reader can check with the example of spherical coordinates (by looking to the explicit version of the corresponding vectors in the section of Vector Calculus) that this relation is correct but at the condition that we work with the non-normalized version of the orthogonal vectors $\vec{e}_r$, $\vec{e}_\theta$, $\vec{e}_\phi$!
	
	From the components $\partial_k x^i$ of the vector $\vec{e}_k$, we can form a determinant $\det(\partial_kx^i)$ which is precisely the Jacobian of the of the functions $x^i$ we have defined previously. Since this determinant is different from zero (at least imposed as such), it follows that the $n$ vectors $\vec{e}_k$ (as functions) are linearly independent (we have proved in the section of Differential and Integral Calculus that this Jacobian is in absolute value equal to $r^2\sin(\theta)$ for the spherical coordinates).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Let us recall that in the section of Differential and Integral Calculus we have proved that the determinant of the Jacobian appears when calculating the surface of a parallelogram in a non-euclidean space. Obviously is the determinant is zero, then the surface is zero as it means that the vector basis of the parallelogram are all collinear (linearly dependents). Hence the fact that if the determinant is not null then some of the vectors (or all) are independent!
	\end{tcolorbox}
	These $n$ vectors, defined by the relation:
	
	are named the "\NewTerm{natural basis}\index{natural basis}" at the point $M$ of the vector space $\mathcal{E}^n$. They are collinear to the tangents of the $n$ coordinated lines which intersect at the point $M$ where they are defined.
	
	We will not insist on the quite obvious fact that at each system of curvilinear coordinates there is an associated natural basis whose base is expressed by these same coordinates (\SeeChapter{see section Vector Calculus page \pageref{system of coordinates}}).
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	In spherical coordinates, the vectors of the natural basis are those that we obtained in our study of the spherical coordinate system in the section of Vector Calculus and that are orthogonal but not orthonormal.
	\end{tcolorbox}
	
	Let us associate at the point $M$ of $\mathcal{E}^n$ a reference frame formed by the point $M$ and by the vectors of the natural basis\footnote{For recall that natural basis in an Euclidean space is the set of unit vectors pointing in the direction of the axes of a Cartesian coordinate system}. This reference frame is named the "\NewTerm{natural reference frame}\index{natural reference frame}" on $M$ of the coordinate system $u^k$. It will be denoted by:
	
	The differential of the vector $\overrightarrow{OM}$ is then expressed as:
	
	The quantities $\mathrm{d}u^k$ are (obviously) the contravariant components of the vector $\mathrm{d}\vec{M}$ in the natural reference frame $(M,\vec{e}_k)$ of the coordinate system $u^k$.
	
	Let us now consider any two curvilinear coordinate systems $u^i$ and $u^k$ (thinks for example to the spherical and cylindrical coordinate systems), linked between them by the relations:
	
	where the functions $u^i=u^i({u'}^1,{u'}^2,\ldots,{u'}^n)$ are assumed as we already know to be several times continuously differentiable with respect to the ${u'}^k$ and same for the functions ${u'}^k={u'}^k(u^1,u^2,\ldots,u^n)$ with respect the coordinates $u^i$. When we move from one coordinate system to another, we say that we make a "\NewTerm{change of curvilinear coordinates}"\index{change of curvilinear coordinates}.
	
	We saw that in the sections of Differential Geometry and General Relativity that the squared distance $\mathrm{d}s$ between two points $M$ and $M'$ infinitely close was given by the relation:
	
	where the $\mathrm{d}x^i$ are the components of the vector $\mathrm{d}\vec{M}=\overrightarrow{MM}$, assimilated to a fixed reference frame of a punctual space $\mathcal{E}^n$. When this space is assimilated to a system of curvilinear coordinates $u^i$, we have seen that the relation:
	
	shows that the vector $\mathrm{d}\vec{M}$ has for curvilinear contravariant components the quantities $\mathrm{d}u^i$ with respect to the natural base $(M,\vec{e}_i$. The square of the distance $\mathrm{d}s^2$ (ie the "line-elements") is then written in the natural reference frame:
	
	Where the quantities $g_{ij}=\vec{e}_i\circ\vec{e}_j$ are the components of the "fundamental tensor" or of "metric tensor" defined using a natural base. The previous expression is named the  "\NewTerm{linear element of the point space}" $\mathcal{E}^n$ or sometimes the "\NewTerm{metric}" of this space.
	
	The vectors $\vec{e}_i$ of the natural reference frame generally vary from one point to another. This is the case, for example, of the spherical coordinates whose quantities $g_{ij}$ (we show it afterwards with a detailed example) are variable since depending on the parameters $r$, $\theta$ or $\phi$!
	
	A curve $\Gamma$ of $\mathcal{E}^n$ can be defined by the data of the curvilinear coordinates $u^i(\alpha)$ of the locus of the points $M(\alpha)$ as a function of a parameter $\alpha$ (\SeeChapter{see section Differential Geometry page \pageref{parametric curves}}). The elementary distance $\mathrm{d}s$ on this curve $\Gamma$ is then written:
	
	If we consider a tangent vector $\vec{v}$ of $\Gamma$ and denote its components $v^{i}$ on a basis $\vec{e}_{i}$. On another basis $\vec{e}_{i}^{\prime}$ we denoted the components $v^{\prime i}$, so:
	
	in which:
	
	If we express the new components in terms of the old ones, then:
	
	This is the explicit form of a transformation named the "\NewTerm{contravariant transformation}\index{contravariant transformation}\label{contravariant transformation}". In order to distinguish them such vectors from the covariant (tangent) vectors, the index is placed on top.
	
	A famous example of a contravariant transformation is given by a differential form $\mathrm{d}f$. For $f$ as a function of coordinates $x^{i}$, $\mathrm{d}f$ can be expressed in terms of $\mathrm{d}x^{i}$ (ie the total derivative). The differentials $\mathrm{d}x^{i}$ transform according to the contravariant rule since:
	
	
	\subsubsection{Natural basis in spherical coordinates (curvilinear basis in spherical coordinates)}
	Let us determine the natural basis of the vector space $E^3$ associated with the point space $\mathcal{E}^3$ of ordinary geometry, in spherical coordinates. Let us write the expression of the vectors $\overrightarrow{\text{O}M}$ in a fixed cartesian basis $(\vec{e}_i^{\,0})$ which is by definition (see the section Vector Calculus for more details):
	
	The vectors of the natural base are given by:
	
	Therefore we have:
	
	The derivative of $\overrightarrow{\text{O}M}$ with respect to $\theta$ gives the vector $\vec{e}_2$:
	
	The derivative with respect to $\varphi$ gives the vector $\vec{e}_3$:
	
	These three vectors are orthogonal to each other as we can easily verify it by performing the dot products $\vec{e}_i\circ\vec{e}_j$. When this is the case, we say that the coordinates are "\NewTerm{orthogonal curvilinear coordinates}\index{orthogonal curvilinear coordinates}" (\SeeChapter{see section Vector Caluculus page \pageref{orthogonal curvilinear coordinates}}).

	We thus find the same result as in the section of Vector Calculus! These vectors, however, are not all normalized (they're norm is not equal to $1$) since we have:
	
	The natural basis in spherical coordinates is thus formed by vectors that are variable in direction and in modulus at each point of $M$. The quantities $g_{ij}$ constitute an example of a metric tensor attached to each of the points $M$ of the space $\mathcal{E}^3$.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/algebra/spherical_natural_basis.jpg}
		\caption[Coordinate surfaces, coordinate lines, and coordinate axes of spherical coordinates]{Coordinate surfaces, coordinate lines, and coordinate axes of spherical coordinates (source: Wikipedia)}
	\end{figure}
	The linear element of the surface is then given by (the details of the calculations can be found in the section of General Relativity):
	
	
	\subsubsection{Natural basis in polar coordinates (curvilinear basis in polar coordinates)}
	Let us determine the natural basis of the vector space $E^2$ associated with the point space $\mathcal{E}^2$ of ordinary geometry, in polar coordinates. Let us write the expression of the vectors $\overrightarrow{\text{O}M}$ in a fixed cartesian basis $(\vec{e}_i^{\,0})$ which is by definition (see the section Vector Calculus for more details):
	
	The vectors of the natural basis are given by:
	
	We have:
	
	The derivative of $\overrightarrow{\text{O}M}$ with respect to $\phi$ gives the vector $\vec{e}_2$:
	
	These two vectors are orthogonal to each other as we can easily verify by performing the dot products $\vec{e}_1\circ\vec{e}_2$. We thus find the same result as in the section of Vector Calculus.
	
	We then have:
	
	The linear element of the plane is then given by (the details of the calculations can be found in the section of General Relativity):
	
	
	\subsubsection{Natural basis in cylindrical coordinates (cylindrical basis in polar coordinates)}
	Let us determine the natural basis of the vector space $E^3$ associated with the point space $\mathcal{E}^3$ of ordinary geometry, in cylindrical coordinates. Let us write the expression of the vectors $\overrightarrow{\text{O}M}$ in a fixed cartesian basis $(\vec{e}_i^{\,0})$ which is by definition (see the section Vector Calculus for more details):
	
	The vectors of the natural basis are given by:
	
	We have:
	
	The derivative of $\overrightarrow{\text{O}M}$ with respect to $\varphi$ gives the vector $\vec{e}_2$:
	
	and finally:
	
	These three vectors are orthogonal to each other as we can easily verify by performing the dot products $\vec{e}_i\circ\vec{e}_j$. We thus find the same result as in the section of Vector Calculus.
	
	The linear element of the surface is then given by (the details of the calculations can be found in the section of General Relativity):
	
	Notice now that:
	
	expresses something about the intrinsic metrical relations of the space, but it does so in terms of a specific coordinate system. If we considered the metrical relations at the same point in terms of a different system of coordinates (such as changing from Cartesian to polar coordinates), the coefficients $g_{\mu\nu}$ would be different. Fortunately there is a simple way of converting the $g_{\mu\nu}$ from one system of coordinates to another, based on the fact that they describe a purely localistic relation among differential quantities. Suppose we are given the metrical coefficients $g_{\mu\nu}$ for the coordinates $x^\alpha$, and we are also given another system of coordinates $y^\alpha$ that are defined in terms of the $x^\alpha$ by some arbitrary continuous functions:
	
	Assuming the Jacobian of this transformation isn't zero, we know that it's invertible, and so we can just as well express the original coordinates as continuous functions (at this point) of the new coordinates:
	
	Now we can evaluate the total derivatives of the original coordinates in terms of the new coordinates. For example, $\mathrm{d}x^0$ can be written as:
	
	and similarly for the $\mathrm{d}x^1, \mathrm{d}x^2$, and $\mathrm{d}x^3$. The product of any two of these differentials, $\mathrm{d}x^\mu$ and $\mathrm{d}x^\nu$, is of the form:
	
	(remembering the summation convention giving $16$ components). Substituting these expressions for the products of $x$ differentials in the metric formula:
	
	gives:
	
	The first three factors on the right hand side obviously represent the coefficient of $\mathrm{d}y^\alpha\mathrm{d}y^\beta$ in the metric formula with respect to the $y$ coordinates, so we've shown that the array of metric coefficients transforms from the $x$ to the $y$ coordinate system according to the relation:
	
	Also often denoted:
	
	and more rarely (sadly) denoted:
	
	Notice that each component of the new metric array is a linear combination of the old metric components, and the coefficients are the partials of the old coordinates with respect to the new. Arrays  that transform to each other in the way:
	
	 are named "\NewTerm{(second order) covariant\footnote{Keep in mind that when a tensor expressed in terms of Cartesian coordinates, there is no distinction between covariant and contravariant components} tensors}".
	 
	 When we speak of an array being transformed from one system of coordinates to another, it's clear that the array must have a definite meaning independent of the system of coordinates. We could, for example, have an array of scalar quantities, whose values are the same at a given point, regardless of the coordinate system. However, the components of the array might still be required to change for different systems.
	 
	 Now we define arrays that transform to each other in the way:
	
	 as "\NewTerm{(second order) contravariant tensors}".
	
	We will need this important result for our study of the Stress-Energy tensor in the section of General Relativity.
	
	Now the reader must keep in mind that in General Relativity, we don't equip space-time itself with a vector space structure, which means that the "four-position" is not a four-vector! As a result, the object $x_{\mu}$ whose components are given by $x_{\mu}=g_{\mu \nu} x^{\nu}$ as proved earlier is not a covector! One can show this by its transformation properties.
	
	If we change the coordinate systems from $x$ to $y$, then:
	
	and so:
	
	If this object was a covector, it would transform as proven earlier using the covariant transformation:
	
	Comparing this to what we found above, this is only true if:
	
	i.e. if the coordinate transformation is linear. Since coordinate transformations are, in general, not linear, then this equality does not hold, and $x_{\nu}$ are not the components of a covector.
	
	Finally another obvious identity that you can also sometimes found in some rare textbooks is (I never see a practical application of that latter):
	
	
	\pagebreak
	\subsection{Christoffel symbols}
	The study of tensor fields constitutes, for the physicist, the essential element of the tensorial analysis. The generic tensor $\vec{U}$ of this field is a function of the point $M$ and we will denote it simply by:
	
If the tensor $\vec{U}$ is a function only of $M$, the field considered is named a "\NewTerm{fixed field}". If $\vec{U}$ is moreover a function of one or more equation parameters other than the coordinates of $M$, then we say that this is a "\NewTerm{variable field}" and we note it:
	
	The different algebraic operations on the tensors $\vec{U}(M)$ associated with a same point $M$ do not generates any particular difficulty. The derivative of $\vec{U}(M)$ with respect to a parameter $\alpha$ leads to the use of the classical results relating to the derivation of the vectors.
	
	However, a difficulty appears when we try to calculate the derivative of a tensor $\vec{U}(M)$ with respect to the curvilinear coordinates. Indeed, the components of the tensor are defined at each point $M$ with respect to a natural coordinate system which varies from one point to another.

	Consequently, the calculation of the elementary variation, named "\NewTerm{elementary transport}":
	
	When we pass from a point $M$ to an infinitely neighboring point $M'$ this can be done in physics only if we use to the same basis. In order to compare the tensors $\vec{U}(M')$ and $\vec{U}(M)$ each other, we are led to study how a natural coordinate system for a given coordinate system varies when we pass from a point $M$ to an infinitely close point $M'$.
	
	For a system of curvilinear coordinates $u^i$ given a punctual space $\mathcal{E}^n$, a fundamental problem of tensor analysis consists in determining, with respect to the natural reference point $(M,\vec{e}_k)$ at the point $M$, the natural reference point $(M',\vec{e}_k^{\prime})$ at the infinitely close point $M'$. We then say that we are looking for an "\NewTerm{affine connection}\index{affine connection}".
	
	On the one hand, the point $M'$ will be perfectly defined with respect to $M$ if we determine the vector $\mathrm{d}\vec{M}$ such as $\overrightarrow{MM'}=\mathrm{d}\vec{M}$. For curvilinear coordinates $u^k$, the decomposition of an elementary vector $\mathrm{d}\vec{M}$ is given by the relation that we have previously proved:
	
	the quantities $\mathrm{d}u^k$ being for recall the contravariant components of the vector $\mathrm{d}\vec{M}$ on the natural basis $(\vec{e}_k)$.
	
	And now to make things physically comparable, we must guarantee that vectors of the both basis are also parallel! So the idea is that the derivative we are looking for allows one to transport vectors of a manifold (surface) along curves so that they stay parallel with respect to the connection (derivative). Such an idea is named in physics "\NewTerm{parallel transport}\index{parallel transport}".

	Therefore, the idea is to determine the vector $\vec{e}_k^{\prime}$ with elementary variations $\mathrm{d}\vec{e}_k$ of the vector $\vec{e}_k$ relatively to the natural reference frame $(M,\vec{e}_k)$, when we go from $M$ to $M'$. We then have:
	
	The computation of the vectors $\mathrm{d}\vec{e}_k$ then remains the main problem to solve. We will first consider an example of this type of calculation in spherical coordinates for pedagogical purposes as it can help to understand what will follows.

For this, let us return the expression of the vectors $\vec{e}_k$ of the natural base in spherical coordinates, that is:
	
	Since the basis vectors $\vec{i}$,$\vec{j}$,$\vec{k}$ of the fixed Cartesian reference being constant in modulus and in direction, the differential of the vector $\vec{e}_1$ is written:
	
	We notice that the terms in parentheses represent respectively the vectors $\vec{e}_2/r$ and $\vec{e}_3/r$, hence:
	
	We also compute, by differentiating the vector $\vec{e}_2$:
	
	with:
	
	we have:
	
	So finally:
	
	And:
	
	After a few elementary and very tricky algebraic operations (...), we get:
	
	The differentials $\mathrm{d}\vec{e}_k$ are thus decomposed on the natural basis $(\vec{e}_k)$. If we denote by $\omega_i^k$ the contravariant components of the vector $\mathrm{d}\vec{e}_1$, that latter is written:
	
	The components $\omega_i^k$ of the vector $\mathrm{d}\vec{e}_i$ are differential forms (linear combinations of differentials). We have, for example:
	
	If we denote by in a general way by $u^i$ the spherical parameters, we have:
	
	The coordinate differentials are then denoted:
	
	and the components $\omega_i^j$ are then written in a general way:
	
	Where the quantities $\Gamma_{ki}^j$ are functions of $r$,$\theta$,$\phi$ that will be explicitly obtained by identifying each component $\omega_i^j$. For example, the component $\omega_3^3$ is written with the notation of the previous relation:
	
	Identifying the coefficients of the differentials, it comes:
	
	By doing the same with the $9$ components $\omega_i^j$, we get the $27$ (...) terms for which the detailed calculations for the $3\cdot 9=27$ are given much further below in the text. For any curvilinear coordinate system, these quantities $\Gamma_{ki}^j$ are named "\NewTerm{Christoffel symbols of the second kind}\index{Christoffel symbols of the second kind}\label{Christoffel symbols of the second kind}" or "\NewTerm{Euclidean functions of affine connection}\index{Euclidean functions of affine connection}".
	
	Thus, for a punctual space $\mathcal{E}^3$ and a system of any curvilinear coordinates $u^j$, the differential $\mathrm{}\vec{e}_i=\omega_i^k\vec{e}_k$ of the vectors $\vec{e}_i$ of the natural basis is written on this basis
	 
	where $\Gamma_{ji}^k$ in this context is often named "\NewTerm{Levi-Civita connection}\index{Levi-Civita connection}".
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Therefore this also means that given a vector $\vec{v}$ such that:
	
	Thus:
	
	But as:
	
	we have then:
	
	\end{tcolorbox}
	We have just seen, on the example of the spherical coordinates, that a direct calculation makes it possible, by identification, to obtain explicitly the quantities $\Gamma_{ki}^j$. We shall see that we can also obtain the expression of these quantities as a function of the components of $g_{ij}$.
	
	The calculation of the quantities $\Gamma_{ki}^j$ as a function of the $g_{ij}$ will lead us to introduce other Christoffel symbols. For this, let us write the covariant components, denoted $\omega_{ji}$, of the differentials $\mathrm{d}\vec{e}_i$, thus (it's kind a definition):
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	With our example above:
	and:
	
	We get (most dot products are orthogonal vectors, therefore they vanish):
	
	\end{tcolorbox}
	The covariant components are also a linear combinations of the  differentials $\mathrm{d}u^k$ that we can write in the form:
	
	The quantities $\Gamma_{kji}$ are named the "\NewTerm{Christoffel symbols of the first kind}\label{christoffel symbols of the first kind}".

	We see quite clearly by going through the definitions and examples of the Christoffel symbols above again that:
\begin{enumerate}
	\item For the Christoffel symbols of the second kind, they are symmetrical with respect to their lower indices and therefore if the metric is symmetric, we have:
	
	
	\item For the Christoffel symbols of the first kind, they are also symmetrical with respect to their extremal indices and therefore if the metric is symmetric, we have:
	
	\end{enumerate}
	Indeed (following the request of a reader), since we have:
	
	it then comes:
	
	and by swapping the indices $i$ and$ j$:
	
	The term-to-term identification of the development on a concrete case of the last two relations will give (necessarily) the equality:
	
	that we wanted to prove.

	Since the covariant components are related to the contravariant components by the relations (contraction of indices):
	
	We get the expression linking the Christoffel symbols of each kind:
	
	Conversely:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Various notations are used to represent the Christoffel symbols. The most common are:
	\begin{itemize}
		\item Christoffel symbol of the first kind:
		

		\item Christoffel symbol of the second kind:
		
	\end{itemize}
	\end{tcolorbox}
	Let us consider now a punctual space $\mathcal{E}^n$ and given a linear element $\mathrm{d}s^2$ of this space:
	
	Starting from:
	
	we get by differentiation:
	
	By injecting in it the expression of the differential $\mathrm{d}\vec{e}_j=\omega_j^l\vec{e}_l$ this gives us:
	
	where the term represents $\omega_j^l$ represents the mixed component of the vector $\mathrm{d}\vec{e}_j$. We can make this component covariant by multiplying it by the metric tensor $g_{il}$ so as to form an $\omega_{ij}$ quantity which can in turn be expressed by means of the Christoffel symbols as follows as we already know:
	
	Substituting the relation $\Gamma_{kji}=g_{jl}\Gamma_{ki}^l$ in the preceding expression (the indices used in this relation are not those of the expression in question, but mutatis mutandis it is equivalent), we then get:
	
	The differential $\mathrm{d}g_{ij}$ is then written:
	
	On the other hand, the differential of the function $g_{ij}$ is then written:
	
	hence by identifying the coefficients of the differentials $\mathrm{d}u^k$ in these two last expressions (much further below in this section there is a detailed example of all the relations which will follow with several coordinate systems):
	
	Relation that the reader can (if he doubt about its veracity) check with the detailed practical examples which are given much further below.

	As we have (in the case of a symmetric metric):
	
	where it is strongly recommended to the reader to remember that the permutation of the indices respecting this last relation generally only  works on the extreme indices!

	We can therefore write the prior-previous relation as:
	
	and by performing a circular permutation on the indices (hence it is not a permutation of the extreme indices!), we get:
	
	By making the sum:
	
	and by subtracting:
	
	Simplifying it comes:
	
	therefore:
	
	It is the expression of the Christoffel symbols of the first kind as a function of the partial derivatives of the components $g_{ij}$ of the fundamental tensor named "\NewTerm{first Christoffel identity}\index{first Christoffel identity}\label{first Christoffel identity}". We thus understand why in a locally inertial frame (of the Minkowski type) the Christoffel symbols are all zero (given that the metric is constant).
	
	We get those of the second kind from the following relation (by definition) named "\NewTerm{fundamental theorem of Riemannian geometry}\index{fundamental theorem of Riemannian geometry}\label{fundamental theorem of Riemannian geometry}" or "\NewTerm{Levi-Civita connection}\index{Levi-Civita connection}" or "\NewTerm{first Christoffel identity}\index{first Christoffel identity}":
	
	The last two expressions listed above allow the actual calculation of the Christoffel symbols for a given metric (hence an enormous gain in calculations). When the quantities $g_{ij}$ are given a priori, we can study the properties of the punctual space defined by the data of this metric, which is the case of the Riemann spaces we will see later.
	
	For our study of Cosmology we will need to calculate:
	
	For this purpose, notice that:
	
	Therefore:
	

	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We propose to calculate the Christoffel symbols of the second kind $\Gamma_{kj}^i$ corresponding to the polar coordinate system in the plane  (it will be sufficiently long...) that we will write this time (at the opposite of the section of Vector Calculus) in index notation:
	
	We will calculate the Christoffel symbols of the second kind from our last relation:
	
	Let us determine the components of the metric. By the way, they are the same as those we had calculated for the cylindrical coordinates above with the obvious difference that $g_{33}$ does not exist. Therefore, we have:
	
	Let us then calculate the $g_{ji}$. In this example it is rather trivial, it is enough to apply the relation demonstrated at the beginning of this section:
	
	Or by dealing as with standard matrices (\SeeChapter{see section Linear Algebra page \pageref{some matrix inverse}}):
	
	We then have immediately:
	
	Now let us develop the Christoffel symbols writing for these coordinates:
	\end{tcolorbox}
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	Hence due to the properties of symmetry:
	
	In the same way:
	
	\end{tcolorbox}
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	To sum up:
	
	\end{tcolorbox}
	
	\subsection{Ricci Theorem}
	\begin{tcolorbox}[colback=red!5,borderline={1mm}{2mm}{red!5},arc=0mm,boxrule=0pt]
	\bcbombe Before reading what follows ... We want to remind the reader that the writing of this section is not finished (as most chapters in the book)! Thus, we still have to illustrate the abstract notions that will follow with concrete practical examples but that are example outside of the special case of General Relativity!
	\end{tcolorbox}
	This being said, we have seen in the section of General Relativity that geodesics are the shortest distances between two points in any type of space. What will interest us now is to study the variations of a vector during such a displacement. Let us first recall that the geodesics equation\index{geodesic equation} for any curvilinear coordinate system $y^i$ of the punctual space $\mathcal{E}^n$ (\SeeChapter{see section Principles page \pageref{point spaces}}) is given by (\SeeChapter{see section General Relativity page \pageref{geodesic equation}}):
	
	Let us consider now a vector $v$ of $\mathcal{E}^n$ of covariant components $v_i$ and let us form the dot product of the vector $\vec{v}$ and $\vec{n}=\mathrm{d}y/\mathrm{d}s$ (the latter vector, denoted here abusively with the indices, gives the components tangent to the geodesic on which the first vector flows), we then have the following quantity:
	
	When moving along the geodesic from a point $M$ to an infinitely near point $M'$, the scalar undergoes the variation:
	
	and as:
	
	Hence:
	
	Let us replace in this last expression, on the one hand, the differential of $\mathrm{d}v_K$ by its exact total differential that we rewrite a bit:
	
	and on the other hand, the second derivative $\mathrm{d}^2y^i/\mathrm{d}s$ by its expression taken from the equation of geodesics. We are getting:
	
	which can also be written:
	
	Where we have put:
	
	which are by definition the absolute differentials of the covariant components of the vector $\vec{v}$. We also define the "\NewTerm{covariant derivative for $1$-forms}\index{covariant derivative}\label{covariant derivative}" (also named "\NewTerm{connection}\index{connection}" or "\NewTerm{affine connection}\index{affine connection}") by the relation:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In ancient or American textbooks this is often written in the form (which we will not use in this book):
	
	making use of the "$;$" to denote the covariant derivative and the "$,$" for the partial differential.
	\end{tcolorbox}
	Since the derivative of the product of two functions is the sum of the partial derivatives, then we also have:
	
	If we put $t_br_c\equiv \nabla_j v_i$ then we have (result which we will use after having proved the Ricci theorem to further determine the Einstein tensor necessary in the section of General Relativity):
	
	In curvilinear coordinates, in order for the differential of a vector to be a vector, the two vectors of which we take the difference must be in the same point of space. In other words, one of the two infinitely close vectors must be transported in one way or another to the point where the second is located, and only after making the difference between the two vectors which are now in one and only one point of the same punctual space. The "\NewTerm{parallel transport}\index{parallel transport}" operation must be defined in such a way that in cartesian coordinates (for the small example...), the difference of the components coincides with the ordinary difference $\mathrm{d}v_k$. 
	
	Thus, we have indeed in Cartesian coordinates:
	
	since in this system: $\Gamma_{kj}^i=0$.
	
	Thus, in curvilinear coordinates, the difference of the components of the two vectors after the transport of one of them to the point where the other is denoted $\delta v_k$ such that we have:
	
	This brings us by identification to write:
	
	But also to write the principle of least action (variational principle) in the tensorial form:
	
	Let us consider now a tensor of order two, product of two tensors of order one such that (as we have seen in our study of tensor compositions):
	
	Therefore:
	
	hence (we take out the last two equalities just for aesthetics!):
	
	A parallel transport is therefore an operation that takes a tangent vector and moves it along a path in space without turning it (relative to the space) or changing its length. In flat space we can say that the transported vector is parallel to the original vector at every point along the path. In curved space we cannot say such a thing. Let's use the spherical surface of Earth to show this! We start at the equator at longitude $0^\circ$ holding an arrow pointing north:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/algebra/parallel_curvature.jpg}
		\caption{Parallel transport illustration on a sphere}
	\end{figure}
	We go along longitude $0^\circ$ up to the North Pole, keeping our arrow parallel to the ground and pointing forward all the time. When we get to the North Pole our arrow is pointing in the direction of longitude $180^\circ$. Now we go south along longitude $90^\circ$ east keeping our arrow perpendicular to our path as it was at the North Pole. When we get to the equator our arrow is pointing to the east. Finally, we go west along the equator until we get back to our starting point. We keep the arrow pointing backwards all the way. Though we did not turn the arrow all the way, we are now at the starting point with our arrow turned $90^\circ$ relative to its original position. We surely can't say that it is now parallel to the original position. This means that the term "direction" cannot be defined globally in a curved space. We can only compare the direction of two vectors if they are at the same point.

	The fact that parallel transport along a closed loop changes the direction of a vector in a curved space but not in a flat space may lead to the idea of using it as a way to measure curvature. It turns out that if we choose a loop that is small enough around a point in a curved space, the amount of change in the direction of a vector that is parallel transported along it is proportional to the area enclosed by the loop. So, the ratio between the area of the loop and the amount of change in the direction of the vector (whatever way we chose to measure it) can be used as a measure to the curvature of the surface that includes the loop. Actually we define curvature by the value of this ratio.
	
	The previous relation leads us to be able to write the metric in its variational form named "\NewTerm{Ricci identity}\index{Ricci identity}":
	
	But we also have since $g_{ij}=\vec{e}_i\circ\vec{e}_j$:
	
	hence the identity:
	
	With the both relations:
	
	and the absolute differential (which is simply generalized for a tensor of order two):
	
	we get:
	
	Now, let us recall that we have by definition:
	
	Hence finally:
	
	The absolute differential on a geodesic in the approximation of an infinitesimal transport of the fundamental tensor is therefore (as we could expect) zero. This is the "\NewTerm{Ricci theorem}\index{Ricci theorem}". Some theoretical physicists then say that \og \textit{the covariant derivative kills the metric} \fg{} in the sense that the metric does not change on a space differential.

	Finally, we also see that for a tensor of order two (the metric in particular) we have:
	
	We can therefore write the absolute differential which in this particular case is zero:
	
	And therefore the "\NewTerm{covariant derivative of a $(0,2)$-form}\index{covariant derivative}" of the metric is null:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The reader will have to remember for the definition of the Einstein tensor that:
	
	and that this is another way of expressing that an infinitesimal variation on a geodesic according to the principle of least action kills the metric. We will therefore work from now on (as before) with nonlinear differential equations that must be integrated to find the behavior of matter in a given space.
	\end{tcolorbox}
	In general, if $T$ is an $(r,s)$ tensor field (we will need that during our study of General Relativity), then $\nabla T$ is an $(r,s+1)$-tensor field given by:
	
	where the latter expression is intrinsically defined by the equation:
	
	In particular, if $T$ is a $(1,1)$-tensor, then $(\nabla_X T)(\eta,Y)$ is intrinsically defined by the equation:
	
	and therefore the correct expression for $(\nabla_X T)(\eta,Y)$ is:
	
	This will help us to derive a special case of:
	
	for the case of a $(1,1)$-tensor (as we will have during our Study of General Relativity).

	So, by the definition above in the case of a $(1,1)$-tensor we have $T^{\mu}_{\phantom{\mu}\nu\,;\rho}:=\nabla_{e_\rho}T(f^\mu,e_\nu)$, where we are using $f^\mu$ and $e_v$ for $\mathrm{d}x^\mu$ and $\partial/\partial x^/nu$.

	Hence we have (it's very not obvious so maybe in the future we will detail the process more or found another approach):
	
	The first and third terms are easily evaluated:
	
	For the second term, since $f^\mu$ is a $(0,1)$-tensor field, $\nabla f^\mu$ is a $(0,2)$-tensor field and hence $\nabla_{e_\rho}f^\mu$, for fixed $e_\rho$, is a $(0,1)$-tensor field. We evaluate:
	
	Therefore $\nabla_{e_\rho}f^\mu =  -\Gamma_{\sigma\rho}^\mu f^\sigma$ and hence:
	
	Thus, we have:
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let's try to verify this by calculating one component of the covariant differentiation in the spherical coordinates.\\
	
	We recall from our article that the diagonal metric of the $2$-sphere surface $\mathcal{S}^2$ expression is (\SeeChapter{see section Differential Geometry \pageref{metric two sphere}}):
	
	If we were to calculate the component $g_{\phi\phi;\theta}$, we should then write:
	
	But $g_{ij}\neq 0$ only if $i=j$, based on the above expressions, so we can simplify this relation:
	
	As (\SeeChapter{see section Differential and Integral Calculus page \pageref{usual derivatives}}):
	
	 Further below (see page \pageref{Christoffel symbol 2-sphere}) we will prove that for $\mathcal{S}^2$ we have:
	
	So that:
	
	Finally, we confirm that this component of the covariant derivative with respect to $\theta$ equals also zero in a polar coordinates system, as expected:
	
	\end{tcolorbox}
	Let us now determine an expression which will be very useful in General Relativity when we will determine the Einstein field equations (another way of expressing that the covariant derivative of the metric is null):

	Let us perform the contracted multiplication of:
	
	by $g^{ij}$, it then comes by using the relation $g^{ij}g_{jl}=\delta_i^l$ (which we had proved much earlier above) that:
	
	hence the relation:
	
	The quantities $\Gamma_{jh}^j$ and $\Gamma_{ih}^h$ representing the same sums, we then have:
	
	\begin{theorem}
	Let us consider now the determinant $g$ of the quantities $g_{ij}$. The derivation of the determinant gives us:
	
	\end{theorem}
	\begin{dem}
	Given any variable that we choose here as being the time $t$ only to simplify the notations of the calculations that will follow. When the main part of the development is completed, the result can be adapted to any other variable! We will write for what will follow $g_j$ the elements of the $j$-th column of $g_{ij}$.

	For the following developments, we define the notations:
	
	The rule of derivation of a functional determinant is given for recall (\SeeChapter{see section Linear Algebra page \pageref{derivative of a determinant}}) by:
	
	By considering the first determinant, by using the minors (\SeeChapter{see section Linear Algebra page \pageref{minor}}) for the development of its first column:
	
	For the $j$-th determinant, it comes:
	
	Thus:
	
	Now, we have demonstrated much earlier that the metric tensor is its own inverse. Therefore:
	
	This allows us to write:
	
	and therefore:
	
	what is also written (following the conventions defined at the beginning of the proof):
	
	where the reader must therefore be careful not to read badly, typically thinking that the derivative $\mathrm{d}_t$ in the right-hand term derives everything ($g_{ij}g^{ij}$)... while it only derives $g_{ij}$.
	
	However, we can adopt another variable. Let $h$ be this other variable:
	
	Either by rearranging:
	
	This is what we wanted to prove.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Now by combining:
	
	Demonstrated earlier above and the result we have just proved:
	
	it comes:
	
	Therefore we have:
	
	Let us prove that it is possible to derive this last relation from the following important equality:
	
	Indeed:
	
	This relation does not mean much until we will make a more explicit use of it in our study of General Relativity (\SeeChapter{see section General Relativity page \pageref{general relativity}}).

	Let us now determine the second covariant derivative of the metric tensor. Let us remember before we go further (because it is important) that we had obtained:
	
	
	\subsection{Riemann-Christoffel symbols}\label{Riemann-Christoffel symbols}
	\begin{tcolorbox}[colback=red!5,borderline={1mm}{2mm}{red!5},arc=0mm,boxrule=0pt]
	\bcbombe Before reading what follows ... We want to remind the reader \underline{again} that the writing of this section is not finished (as most chapters in the book)! Thus, we still have to illustrate the abstract notions that will follow with concrete practical examples but that are example outside of the special case of General Relativity!
	\end{tcolorbox}
	
	Let us recall that we have demonstrated earlier above that:
	
	This relation means, exception of an interpretation error from the writer of those linear as sometimes interpreting Tensor Calculus result is a pain in the a.., that the covariant derivative of a tensor of order two - such as the metric - on a geodesic path in two perpendicular directions (the second covariant derivative making it possible to the "perpendicular geodesic" between the two geodesics infinitely close to the first covariant derivative). We know already that such a shift is "parallel transport".
	
	By substituting in it:
	
	We then have:
	
	Let us now switch the indices $j$ and $k$ in the previous expression to have a differential with respect to another path:
	
	Assuming that the components satisfy the classical properties $\partial_{kj}v_i=\partial_{jk}v_i$, we get by subtraction of the two previous expressions:
	
	And since we have proved that in the case of a symmetric metric we have:
	
	We have therefore:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The fact of having in the case of a symmetric metric $\Gamma_{jk}^r=\Gamma_{kj}^r$ and which vanishes in the preceding relation, leads many practitioners to define what we name the "\NewTerm{torsion tensor}\index{torsion tensor}\label{torsion tensor}" or "\NewTerm{torsor}" (but in reality it is a particular case of a more general relation which belongs to the domain of differential geometry). Thus, we define the torsion tensor as:
	
	and in the case of a symmetric metric (Euclidean space), the torsion is zero by extension as we have already seen it! In fact, the Einstein field equations which we shall prove later implicitly implies a symmetric metric with zero torsion. However, it is possible to proved that a non-symmetric tensor can always be decomposed into a symmetric and non-symmetric tensor (this is trivial because it is like separating a complete matrix in the sum of a matrix having a diagonal of non-zero component that are and another matrix having the diagonal zero).
	\end{tcolorbox}
	It then remains:
	
	Since parallel transport takes place on infinitely close geodesic paths, we take the limit:
	
	Which mainly underlies the fact that the velocity field is almost equal in two infinitely close parallel points.

	It then remains:
	
	This relation expresses the fact that, like gravity, the curvature of space-time causes a mutual acceleration between the geodesics! Moreover, it is easy to see that the mutual acceleration between the geodesics is zero if the Riemann-Christoffel tensors are null (typically in Cartesian coordinates, in extenso it means for a flat time-space). This is exactly what we expect of gravity: if we do not observe any acceleration, the curvature (we shall now define what it is) is zero and if the curvature is zero, we observe no acceleration. Morale of the story: the gravity is curvature and the curvature is gravity !!

	We see that the quantity in parentheses is a tensor of order four that we will write on this site as following (because there are several traditions in the way to write it...):
	
	and which summarizes itsel the parallel transport and the fact that gravity and geometry of space are linked together. Obviously, if the metric is of Minkowski type (or tends to a metric of Minkowski under certain conditions), then $R_{i,jk}^l$ is zero! Very rare authors write this last equality in the (unhappy ....) form:
	
	The tensor $R_{i,jk}^l$ is named the "\NewTerm{Riemann-Christoffel tensor}\index{Riemann-Christoffel tensor}" or "\NewTerm{Riemannian space tensor}\index{Riemannian space tensor}". The curvature of a Riemannian space can also be characterized using this tensor.

	If we multiply the tensor $R_{i,rs}^k$ by $g_{jk}$, then we have the covariant components of this tensor such that:
	
	and given the following relations that we proved earlier above:
	
	Therefore we get:
	
	and let us replace the quantities $g_{jk}\partial_r\Gamma_{is}^k$ by $\partial_r\left(g_{jk}\Gamma_{is}^k\right)-\Gamma_{is}^k\partial_r g_{jk}$. We then get:
	
	We also proved earlier before that:
	
	Hence:
	
	and as:
	
	we get:
	
	and we also proved that:
	
	Hence:
	
	And by reporting them into the prior-previous relation, we get:
	
	Finally, we get for the covariant expression of the Riemann-Christoffel tensor:
	
	It should be noticed that the Riemann-Christoffel tensor is therefore antisymmetric:
	
	and that in the parenthesis of the prior-previous relation we have only double partial derivatives, while outside of the parenthesis the Christoffel symbols contain only simple partial derivatives!

	Finally, the permutation of the indices $ij$ and $rs$ as a block gives us as a consequence of the symmetry of the $g_{ij}$ and by inverting their derivation order:
	
	Let us now perform a circular permutation on the indices $j$, $r$, $s$ in the expression (obtained just above)
	
	then we get:
	
	and we get (it is very simple to control by summing the three lines above):
	
	The previous identity is named the "\NewTerm{first Bianchi identity}\index{first Bianchi identity}\label{first bianchi identity}" or also "\NewTerm{Bianchi algebraic identity}\index{Bianchi algebraic identity}" and it highlights the cyclicity property of the  Riemann-Christoffeltensor. In reality, we should not use the word "identity" since it is verified only (at least to my knowledge) in the case of a symmetric metric tensor (otherwise, the torsion is not zero for recall!).

	The reader will observe that it is immediate that this last relation is satisfied in the case of the Minkowski metric, since if at all points the partial derivative of the metric is zero we have:
	
	And we will see in the sectin of General Relativity that this first identity will serve as a basis for the construction of the Schwarzschild metric.

	If the metric is of the Minkowski type (we change the notations of the indices to be more in conformity with the usual writings in general relativity) then it is immediate that we also have:
	
	But in the case where the metric is not of the Minkowski type, this latter relation can be satisfied and has an interest only if and only if the chosen metric is decomposable in Taylor series whose first partial derivatives are zero at $0$ (see the section of Differential Geometry for such Taylor series!).

	This relation is valid only in the case of a "\NewTerm{locally inertial frame (LIF)}\index{locally inertial frame}" in which all Christoffel symbols cancel each other but not their derivatives.

	By extension:
	
	Let us recall that implicitly, this relation, named "\NewTerm{Bianchi second identity}\index{Bianchi second identity}\label{Bianchi second identity}" or "\NewTerm{Bianchi differential identity}\index{Bianchi differential identity}", always expresses simply (if one may say ...) the fact that gravity and geometry of space are linked together.

	Following a reader request let us detail much more how to derivate this identity!

	The identity is easiest to derive at the origin of a locally inertial frame (LIF) as already mention, where the first derivatives of the metric tensor, and thus the Christoffel symbols, are all zero. At this point, we have
	
	If the Christoffel symbols are all zero, then the covariant derivative becomes the ordinary derivative
	
	Therefore, we get, at the origin of a LIF:
	
	By cyclically permuting the index of the derivative with the last two indices of the tensor, we get
	
	By adding up all linear with the covariant derivative and using the commutativity of partial derivatives, we see that the terms cancel in pairs, so we get
	
	As usual we can use the argument that since we can set up a LIF with its origin at any non-singular point in spacetime, this equation is true everywhere and since the covariant derivative is a tensor, this is a tensor equation and is thus valid in all coordinate systems.
	
	
	\subsection{Ricci curvature (Ricci tensor)}\label{Ricci tensor}
	Before we can see the consequences of second Bianchi's identity, we need to define the "\NewTerm{Ricci tensor}\index{Ricci tensor}":
	
	which is therefore simply the contraction of the first and third indices of the Riemann-Christoffel tensor which we have given above:
	
	in other words it is just a more condensed notation ... and then the letters for the upper or lower indices as well as the presence of the comma are at the free choice of the writer (according to the mood and especially if the context makes it possible to avoid any confusion).

	For example with the Riemann-Christoffel tensor we have just given, the Ricci tensor could be written in the following two ways (we keep the indices with latin letters):
	
	The Ricci tensor can be taken as the trace of the Riemann tensor, hence it is of lower rank, and has fewer components. If you have a small geodesic ball in free fall, then (ignoring shear and vorticity) the Ricci tensor tells you the rate at which the volume of that ball begins to change, whereas the Riemann tensor contains information not only about its volume, but also about its shape. 
	
	Other contractions of other indices may also be possible but because $R_{\alpha\beta,\mu\nu}$ is antisymmetric on $\alpha,\beta$ and $\mu,\nu$ then the contraction on these indices are equivalent to have $\pm R_{\alpha\beta}$.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	There is no widely accepted convention for the sign of the Riemann
curvature tensor, or the Ricci tensor, so check the sign conventions of whatever book you are reading!
	\end{tcolorbox}
	Similarly, we define the "\NewTerm{Ricci scalar}\index{Ricci scalar}\label{ricci scalar}" (also sometimes named "\NewTerm{Riemann scalar}\index{Riemann scalar}") by the relation:
	
	which has the following properties:
	\begin{itemize}
		\item If the space is flat, the Ricci scalar is zero
	
		\item If space is curved like a sphere, the Ricci scalar is positive
	
		\item If the space is curved like a horse saddle, the Ricci scalar is negative
	\end{itemize}
	Either explicitly (by changing the notation for the indices in order to insist on the fact that it has no impact!):
	
	The Ricci scalar is the trace of the Ricci tensor, and it is a measure of scalar curvature. It can be taken as a way to quantify how the volume of a small geodesic ball (or alternatively its surface area) is different from that of a reference ball in flat space.

	We will have concrete practical examples in the section of General Relativity for the first two cases, but let us look at simplified examples for the first two cases (we will not, however, prove the reciprocal).
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Let us start with the metric of flat space (without the temporal component). We have (\SeeChapter{see section General Relativity page \pageref{metric flat space}}):
	
	By taking the definition of Ricci's scalar in explicit form:
	
	It is immediate that R is zero since the partial derivatives will all be zero. So a flat space has a null Ricci scalar.\\
	
	E2. Let us now look at the metric of the plane expressed in spherical coordinates (without the temporal component). We have (\SeeChapter{see section General Relativity page \pageref{metric spherical space}}):
	
	and:
	
	with:
	
	We know that to compute the Ricci scalar (or Ricci curvature), we must therefore compute the contraction of the Riemann-Christoffel tensor (that is, the Ricci tensor) which itself depends on the Christoffel symbols of the second kind which themselves depend on the symbols of Christoffel of the first kind (argh!). We should therefore begin with the lowest level, that is to say by determining all the Christoffel symbols of the first kind given for recall by:
	
	
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	So we have an $3^3$, that is $27$ possible Christoffel symbols of the first kind! Even if some symbols are equal (we have proved it!), We will still calculate everything.\\

	Let's start with joy and good humor ...:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	Let us now calculate all the symbols of Christoffel symbols of the second in the details:
	
	Again, as the metric tensor is diagonal, this will simplify the calculations!\\

	We have then:
	
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	Let us now calculate the $9$ components of the Ricci tensor in details according to:
	
	We then have (we calculate them all, even if we know that subsequently those which do not have $\alpha=\beta$ will be useless by the fact that the metric is diagonal):
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	Let us now calculate the Ricci scalar:
	
	we then have:
	
	The Ricci scalar is therefore also zero. This result may be surprising, but in reality it is logical since we have only calculated the scalar curvature of a flat space expressed in spherical coordinates.
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	E3. \label{Christoffel symbol 2-sphere}Let us now impose ourselves the diagonal metric of the 2-sphere surface $\mathcal{S}^2$ (without the temporal component). We have then in accordance with what we have seen in the section of Differential Geometry page \pageref{metric two sphere}:
	
	and:
	
	Therefore (\SeeChapter{see section Differential geometry page \pageref{metric two sphere}}):
	
	often written as:
	
	where $r$ is a constant!\\

	We shall therefore begin with the lowest level, that is, by determining all the Christoffel symbols of the first kind given for recall by:
	
	We have therefore $2^3$, that is, $8$ possible Christoffel symbols of the first kind! Even if some symbols are equal (we have already proved it!), we will still calculate everything:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	Let us now calculate all the Christoffel symbols of the second kind in the details:
	
	Again, as the metric tensor is diagonal, this will simplify the calculations!\\

	We have then:
	
	Let us now calculate the $4$ components of the Ricci tensor in the details according to:
	
	We then have (we calculate them all, even if we know that subsequently those which do not have $\alpha=\beta$ will be useless by the fact that the metric is diagonal):
	
	\end{tcolorbox}
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	Let us now calculate the Ricci scalar :
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Notice that $R_{\alpha\beta}=1/r^2 g^{\alpha\beta}$ and for information we can also prove that (its aesthetics but so far it has no practical application in this book):
	
	\end{tcolorbox}
	We then have:
	
	We notice that:
	\begin{enumerate}
		\item The Ricci scalar is a constant. This means that the hypersurface has a constant curvature at all points of the surface (we know that the sphere by symmetry has a constant curvature at all points). It thus possesses a form of symmetry, with respect to its curvature. We are then dealing with a "\NewTerm{maximally symmetrical variety}".

		\item This scalar is positive which describes a domed space (ball, sphere) and if $r\rightarrow +\infty$ the curvature is zero!
	\end{enumerate}
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Be careful not to confuse the value of the Ricci curvature with that of the Gauss curvature!!!
	\end{tcolorbox}
	
	\subsection{Einstein Tensor}\label{einstein tensor}
	Let us apply a contraction to the second Bianchi  identity (valid for recall with the "$+$" only if the metric is positive):
	
	Let us recall that $\nabla_\lambda g_{\alpha\mu}=0$ and similarly by extension that $\Delta_\lambda g^{\alpha\mu}=0$. So finally this leads us to write by the property of the derivatives (product in sum):
	
	and therefore to get:
	
	 Using the antisymmetry property of the Riemann-Christoffel tensor we write:
	
	What ultimately brings us to write from the definition of Ricci's tensor:
	
	This last relationship being named "\NewTerm{contracted Bianchi identity}".

	Let us contract this relation once more:
	 
	That which is identical to write using the properties of the Einstein summation (which allows to freely change the indices):
	
	Which is equivalent to:
	
	As $\nabla_\lambda=g_{\lambda}^\mu \nabla_\mu R$, we have:
	
	By raising the index $\lambda$ by multiplication with $g^{\nu\mu}$, we get the "\NewTerm{Einstein's identity}\index{Einstein's identity}":
	
	The "\NewTerm{Einstein tensor}\index{Enstein tensor}" (tensor of order two and contravariant in the present case) which is therefore a constant in a given Riemannian space is therefore defined by:
	
	And expresses in the shortest possible way, parallel transport under all assumptions seen so far.

	Identically, we can obtain the covariant form:
	
	The tensor is therefore constructed for a Riemannian metric only (which nevertheless makes a lot of possible spaces ...), and is automatically non-divergent:
	
	It must be remembered, however, that a large part of the latest developments consider a symmetrical metric. This is why some speak of "\NewTerm{symmetrical gravitational theory}" when we deal with General Relativity.

	We shall find this tensor naturally in the sectionof General Relativity when, by making use of the variational principle, we decompose the action into two terms:
	\begin{itemize}
		\item the action of mass in the gravitational field

		\item the action of the gravitational field in the absence of mass
	\end{itemize}
	By expressing the whole in a Riemannian space we will then get the no less famous Einstein field equations (without further explanations in this section):
	
	the details being given in the section of General Relativity.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	As we see, we can very well add a constant term to the expression of Einstein's tensor, without changing the nullity of its divergence. This fact, used in Astrophysics, makes it possible to construct models of particular Universes that we will deal with in the section of Cosmology.
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us calculate the order $2$ covariant Einstein tensor:
	
	based on the diagonal metricsurface of the 2-sphere $\mathcal{S}^2$ (without the temporal component):
	
	Since the metric is diagonal, we have proved earlier above and in detail by the example that:
	
	And as in the present case, we also have:
	
	It comes:
	
	So we have to focus only on two components:
	
	which confirms what we have said earlier above.
	\end{tcolorbox}
	The complexity of the Einstein Tensor expression can be shown using the formula for the Ricci tensor in terms of Christoffel symbols:
	
	where ${\displaystyle \delta _{\beta }^{\alpha }}$ is the Kronecker tensor and the Christoffel symbol $\Gamma ^{\alpha }{}_{\beta \gamma }$ is given for recall by:
	
	Before cancellations, this formula results in $2\times (6+6+9+9)=60$ individual terms. Cancellations bring this number down somewhat.
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{95} & \pbox{20cm}{\score{4}{5} \\ {\tiny 40 votes,  82.5\%}} 
	\end{tabular} 
	\end{flushright}
		
	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Spinor Calculus}\label{spinors}
	\lettrine[lines=4]{\color{BrickRed}A}s we will see first in relativistic quantum physics, spinors play a major role in quantum theory, and therefore in all contemporary physics (quantum field theory, standard model, string theory, etc.). The actual purpose of this section on the spinors is only to give the tools to the reader that are necessary to a deep understanding of what will be done in the chapter about Atomistic.
	
	It was starting 1927 that the physicists Pauli, and after Dirac introduced spinors  for the representation of the wave functions (\SeeChapter{see section Relativistic Quantum Physics page \pageref{spinor relativistic quantum physics}}). However, in their mathematical form, spinors were discovered by Elie Cartan in 1913 during his research on the representations of the groups following the general theory of Clifford spaces (introduced by the mathematician W.K. Clifford in 1876). He showed, as we will see it, that in spinors provide in fact a linear representation of the group of rotations of a space with any number of dimensions. Thus, spinors are closely related to geometry but they are often introduce in an abstract way without intuitive geometric meaning. Thus, we will try (as always on this book) in this section to introduce this tool in the most simple and intuitive possible way with a maximum of details.
	
	The spinor formalism is not interest only of interest for quantum physics and its related developments, among others, Roger Penrose showed that the spinor theory was an extremely fruitful approach to the theory of General Relativity. Even if the most commonly used tool for the treatment of General Relativity is the tensor calculus, Penrose seems to have shown that in the specific case of the four-dimensional space and in the metric Lorentz the formalism of two components spinors was more appropriate.
	
	The theory of spinors named, "\NewTerm{spinor calculus}\index{spinor calculus}" or sometimes "\NewTerm{spin geometry}\index{spin geometry}" is extremely broad but as we know this book aims to address the physicists and engineers therefore we will limit ourselves to spinors properties useful in quantum physics (at least actually).
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
We strongly recommend the reader to have previously read the subsubsection on quaternions (\SeeChapter{see section Numbers page \pageref{quaternions}}), the subsubsection on rotations in space (\SeeChapter{see section Euclidean Geometry page \pageref{rotation}}) and finally, if possible, for a physical practical example, the section on Relativistic Quantum Physics page \pageref{relativistic quantum physics}.
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Unit Spinor}
	
	We will give here a first simplified and special definition (or example) of spinors. Thus, we will show that it is possible from such a tool to represent a vector equation of a space $e^3$ of three components using a two-component spinor. The method is extremely simple and the reader who has already read the part of the section on Quantum Wave Physics dealing with the Dirac equation and the section on Quantum Computing will see a rather beautiful analogy.
	
	Consider to start the sphere of radius of the following equation (\SeeChapter{see section Analytic Geometry page \pageref{sphere}}):
	
	And consider the following figure:
\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/algebra/spinor_unit_sphere.jpg}
\caption[]{Unit sphere}
\end{figure}
Let us put on this sphere of center on O and unit radius a point $P$ of coordinates $(x,y,z)$  and denote by $N$ (north) and $S$ (south) the points of the sphere intersecting with the $Z$ axis.

The point $S$ will have by convention for coordinates:
	
We obtain a projection so-named "\NewTerm{stereographic projection} \index{stereographic projection}" $P'$ of the point $P$ by tracing the straight line $SP$ that passes through the complex equatorial plane $x\text{O}y$  (yes! we chose a $\mathbb{C}$ as plane!) at point $P'$ of coordinates $(x', y', z')$.

The similar triangles $SP'\text{O}$ and $SPQ$ (with $Q$ being the orthogonal projection onto the $Z$-axis of the point $P$) give us the following relations by simply applying Thales' theorem (\SeeChapter{see section Euclidean Geometry page \pageref{thales theorem}}):
	
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
The last two relation (ratios) are obtained by simply applying Thales' theorem (see section Euclidean Geometry) in the complex equatorial plane.
	\end{tcolorbox}
	
	Let us write now to simplify the notations:
	
	We have from the prior-previous relation that:
	
	Taking the squared modulus (see the study of complex numbers in the section on Numbers):
	
	and from  the equation of the sphere it follows:
	
	we finally get:
	
	Let us write now the complex number $\xi$ under the form:
	
	
	 where $\phi,\psi$ are two complex numbers that we can always impose verify the following condition of unitarity (nothing prohibits to do that but for theoretical physics purpose this choice suit us well...):
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		The following complex numbers satisfy for example (hazard!!??)  the above condition:
		
	\end{tcolorbox}	
	Remember before continuing that we have proved in our study of complex numbers (\SeeChapter{see section Numbers page \pageref{module ration complex numbers}}) that:
	
	Therefore it comes by injecting the last two relations in the equation given above:
	
	So we get:
	
	Rearranged:
	
	Therefore:
	
	Finally we have a simple expression for the $z$ coordinate of the point $P$ because in the last equality above you must remember that $\psi\bar{\psi}+\phi\bar{\phi}=1$ then:
	
	As we have:
	
	Then by summing and respectively by substracting the two above relations and using previous results we get:
	
	To resume for the point $P$ we have with our choices:
	
	Thus, for any point $P$ on the sphere of radius unity, we can match a pair of complex numbers satisfying the imposed unitary identity!
	
	\pagebreak
	Therefore in complete and explicit form we finally have using what we know about complex numbers (\SeeChapter{see section Numbers page \pageref{complex numbers}}) and remarkable trigonometric functions (\SeeChapter{see section Trigonometry page \pageref{remarkable trigonometric identities}}):
	
	We notice easily that the norm of this vector is equal to $1$.
	
	This last relation also indicates us that $2\alpha$ is the angle between $\text{O}z$ and $\overrightarrow{OP}$ (since the hypotenuse of vector's angle has a unit norm) and therefore by deduction $\gamma-\beta$ represents the angle between $\text{O}x$ and the plane $(\text{O}z, \overrightarrow{OP})$:
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/spinor_simple_rotation.jpg}
		\caption[]{Representation of the rotation}
	\end{figure}

	The pair of complex numbers:

	

	is by definition a "\NewTerm{unitary spinor}\index{unitary spinor}" (it contains also all the information about $z$). Thus, as we have seen, a unitary spinor can also be expressed in the form:
	
	As well any spinor can be written in the more general form:
	
	The spin is essentially measured from the oriented $z$-axis as we have seen it yet with the previous figure.
	
	The stereographic projection led us to represent certain vectors of the Euclidean space $\mathcal{E}^3$ with the elements of a complex two-dimensional vector space that is the "\NewTerm{space of spinors}\index{space of spinors}".
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
This representation is not unique because the arguments of complex number are (in trigonometric form) determined at a given offset constant!
	\end{tcolorbox}	
	
	The reader who has already read a little bit the section on Wave Quantum Physics  will probably noticed the strange (not innocent) similarity of the following identity and relations:
	
	compared to the de Broglie normalization condition (the integral over the entire space of the sum of the products of the complex wave functions and its conjugate is equal to the unit) and to the developments determining the continuity equation also in the section of Wave Quantum Physics .
	
	Let us now see that for future needs, we can find two new vectors $\vec{v}_1,\vec{v}_2$ of Euclidean space $\mathcal{E}^3$ associated with a unitary spinor $(\psi,\phi) $ determined on the unit sphere. These vectors will have to be orthogonal to each other and with unit norm, each orthogonal to the vector $\overrightarrow{OP}$.	
	
	To simplify the notations let us write  $\vec{v}_3=\overrightarrow{OP}$ and $\vec{v}_i=(x_i,y_i,z_i),i\in \left\lbrace 1, 2 ,3\right\rbrace$.
	
	The $\vec{v}_1,\vec{v}_2,\vec{v}_3$ vectors are of course bounded by the cross product (\SeeChapter{see section Vector Calculus page \pageref{cross product}}):
	
	hence taking into account the expression of $\overrightarrow{OP}$ components based on its associated spinor, and the fact that $\psi\bar{\psi}+\phi\bar{\phi}=1$, we obtain:
	
	Writing the orthogonality of vectors we get them obviously six additional equations. However the orientation of vectors $\vec{v}_1,\vec{v}_2$ being not fixed, there is some uncertainty in the values of their components. Let us select values such that:
	
	Taking the complex conjugate quantities of previous relation and summing, to have only real parts we have to write:
	
	We can control the norm is equal to the unit. Just check with the squared norm:
	
	In the same way we get:
	
	We can easily check that these values restore well the relations of the vector cross product of $\vec{v}_2$. At any unitary spinor $(\psi,\phi)$ we can therefore associate three vectors $\vec{v}_1,\vec{v}_2,\vec{v}_3$. We can directly check that the vectors thus calculated are mutually orthogonal and with unit norm.
	
	A reader has make us the request to show in much details as possible this affirmation for $x_x$. So let's go:
	 
	
	\subsection{Geometric Properties}
	We will study the transformations of vectors associated to a spinor to derive the corresponding properties of spinor transformation. As we know some special (trivial) rotations in space can always be expressed as the product of two plane symmetries, therefore we begin by studying these latter.
	
	\subsubsection{Plane Symmetries}
	Let us consider first the plan symmetry of a vector:
	
	During a symmetry relative to a plane $P$, any vector $\overrightarrow{OM}$ is transformed into a vector $\overrightarrow{OM'}$. Let us determine a matrix $S$ representing this symmetry with respect to this plane! 
	
	Given $\overrightarrow{\text{O}A}$ a unit vector normal to the plane $\mathcal{P}$ and $H$  the root of the perpendicular projection from any given point $M$ of space on the plane $\mathcal{P}$:
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/plane_symmetry.jpg}
		\caption[]{Plane symmetry of a vector relatively to a plane}
	\end{figure}
	Let $M'$ be the symmetric point $M$ with respect to the plane $\mathcal{P}$, we have:
	
	Given $a_1,a_2,a_3$ the cartesian components of $\overrightarrow{\text{O}A}$ and $(x,y,z),(x',y',z')$ the respective components of the vectors $\overrightarrow{\text{O}M},\overrightarrow{\text{O}M'}$, the above equation gives us the linear relations:
	
	The matrix $S$ that take us from vector $\vec{X}(x,y,z)$ to the vector $\vec{X}'(x,y,z)$ has therefore the following expression:
	
	We keep in mind this result and let us now consider two vectors $(\vec{X}_1,\vec{X}_2)$ orthogonal to each other and unitary, defining as we have above  seen a unitary spinor $(\psi,\phi)$ (we used the notation $\vec{v}_1,\vec{v}_2$ before). A symmetry with respect to a plane $\mathcal{P}$ transforms the vectors $\vec{X}_1,\vec{X}_2$ into vectors $\vec{X'}_1,\vec{X'}_2$ which are associated the spinor $(\psi',\phi')$. 
	
	\begin{theorem}
	We will now show that the following transformation of the spinor $(\psi,\phi)$ into spinor $(\psi',\phi')=(x'_3,y'_3,z'_3)$ is:
	
	\end{theorem}
	and transforms the vectors $\vec{X}_1,\vec{X}_2$ into vectors $\vec{X}_1^{\prime},\vec{X}_2^{\prime}$, these vectors being deduced respectively - as we will just show it - from each other by a single plane symmetry and the matrix $\mathcal{A}$ represents well the sought transformation.
	
	The previous relation gives us therefore:
	
	In all we have so far the previous set of relations and:
	
	Therefore we can deduce:
	
	After, using the fact that $\|\vec{A}\|=a_1^2+a_2^2+a_3^2=1$, we get:
	
	So we fall well back on the symmetry matrix:
	
	Thus, the matrix that we will see again in the section of Relativistic Quantum Physics:
	
	$\mathcal{A}$ therefore generates the transformation of a spinor $(\psi,\phi)$ into a spinor $(\psi',\phi')$ such that the associated vectors $(\vec{X}_1,\vec{X}_2)$ can be deduced respectively from $(\vec{X}_1^{\prime},\vec{X}_2^{\prime})$  by a planar symmetry.
	
	\subsubsection{Rotations}
	As we have saw it in the section of Euclidean Geometry, it is possible to rotate a vector in the plane or in space using matrices. Similarly, by extension, it is clear that the multiplication of two rotations is a rotation (that is the elementary linear algebra - at least we consider it as is).
	
	Consider therefore two planes $P, Q$ whose intersection generates a line $L$ and let us denote $\vec{A}(a_1,a_2,a_3)$ and $\vec{B}(b_1,b_2,b_2)$ the unit vectors carried by the respective normal vectors (\SeeChapter{see section Vector Calculus page \pageref{normal vector}}) to these two intersecting planes in $L$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/spinor_intersecting_planes.jpg}
		\caption[]{Illustrated intersection of two planes}
	\end{figure}
	Let us denote by $\theta/2$ the angle of the vectors $\vec{A},\vec{B}$ between them (the reason for this notation comes from our study of quaternions (\SeeChapter{see section Numbers page \pageref{quaternions}})). Given $\vec{L}$ the unit direction vector carried by the line $L$ resulting from the intersection of planes $P, Q$ and such that:
	
	Explanations: $\vec{A},\vec{B}$ are unitary but not necessarily perpendicular and we still need to ensure that $\vec{L}$ is a unit vector (the norm equal to unity!). Therefore, the above relation ensures that:
	
	The previous vector product gives us for the components of $\vec{L}$:
	
	On the other hand, the scalar product can be written:	
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We will use these two planes as symmetry planes for our rotations.
	\end{tcolorbox}	
	As we have noticed previously, a rotation in $\mathcal{E}^3$ can always be done with more than two plane symmetries. Thus, a rotation can be denoted by the application (multiplication) of two matrices of symmetry according to the results obtained previously:
	
	Developing the product of these two matrices and taking into account the relations arising from vector and dot product we get:
	
	Thus, we can write the transformation of a spinor $(\psi,\phi)$ and a spinor $(\psi',\phi')$ with a matrix of the form:
	
	whose parameters are named "\NewTerm{Cayley-Klein parameters}\index{Cayley-Klein parameters}".
	
	The matrix $R\left(\vec{L},\frac{\theta}{2}\right)$ can be written in another form if we do a limited development for infinitely small rotations $\theta/2=\varepsilon/2$ (that's where the physics comes back...):
	
	Using only the first order terms, the rotation matrix is finally written:
	
	This matrix is the limited development of the matrix of rotation in the neighborhood of the identity matrix, the latter obviously corresponding to the zero rotation. We note also the latter in the form:
	
	where the matrix $\sigma_0$ is the identity matrix of order two and $\chi(\vec{L})$ is named the "\NewTerm{infinitesimal rotation matrix}\index{infinitesimal rotation matrix}". Now, if we put $L_1=1,L_2=L_3=0$ in $\chi(\vec{L})$ we get:
	
	How to interpret this result? Well it's quite simple, choose $L_1=1,L_2=L_3=0$ gives us a collinear vector $\vec{L}$ to the axis $\text{O}x$. Therefore, we can very well imagine the planes generating the axis $\text{O}x$ that carries $\vec{L}$. As $\varepsilon/2$ (verbatim $\theta/2$) is generated by the vectors $\vec{A},\vec{B}$ perpendicular to $\vec{L}$ and thus to $\text{O}x$, then the angle $\varepsilon/2$ (or its variation) represents a variation of the direction of the normal planes to $\vec{A},\vec{B}$ which by symmetry are used to construct the rotation (recall that $\vec{A},\vec{B}$are not necessarily mutually orthogonal). So by extension, having  $L_1=1,L_2=L_3=0$ allows us then only to make rotations (symmetries) around the $x$-axis.
	
	Similarly, a rotation about the $y$-axis corresponds to $L_2=1,L_1=L_3=0$, which gives:
	
	and the same with $L_3=1,L_1=L_2=0$ we finally have:
	
	The three matrices:
	
	are rotation matrices in the space of "\NewTerm{two-dimensional spinors}\index{two-dimensional spinors}". Physicists and mathematicians say that these matrices are an irreducible representation of dimension two of the group "$\NewTerm{\text{SU} (2)}$" or named  "\NewTerm{special group of spatial rotations $\text{SU}(2)$}\index{special group of spatial rotations}" (\SeeChapter{see section Set Algebra page \pageref{special unitary group}}).
	
	The previous infinitesimal matrices therefore show us in a skillful way the following matrices:
	
	These matrices are named "\NewTerm{Pauli matrices}\index{Pauli matrices}\label{pauli matrices origin}" and we will find them again in the section of Relativistic Quantum Physics as part of the study of the Dirac equation and the determination of its explicit solutions (using spinors).
	
	Using Pauli matrices, the infinitesimal rotations matrix can finally be written:
	
	Let us define a vector $\vec{\sigma}$, named "\NewTerm{Pauli vector}\index{Pauli vector}", whose components are the Pauli matrices:
	
	The expression $L_1\sigma_1+L_2\sigma_2+L_3\sigma_3$ can be written as a sort of dot product which represents a sum of matrices\label{spinor dot product} (the arrow above the sigma is sometimes omitted if no confusion is possible):
	
	The limited development is then written:
	
	The rotations matrix:
	
	can using Pauli matrices be written in the remarkable form under the assumption of small angles:
	
	Expression that we will a lot in the section of Quantum Computing to express the $R$ matrices explicitly and also in the section of Set Algebra page \pageref{set algebra}.
	
	What is written sometimes:
	
	Which can also be written in an extensive form:
	
	which has the form of a quaternion of angle $\theta$ (don't remember that this is only for small angles!) and of axis $\vec{L}$. Hence the reason that  have from the beginning chosen the notation $\varepsilon/2$.
	
	It is clear, so that the analogy with quaternions to be stronger, that the $2\times 2$ Pauli matrices are a set of four linearly independent matrices! As the canonical basis for quaternions!
	
	If we denote by $\vec{L}(L_1,L_2,L_3)=\vec{X}(x,y,z)$  then the "\NewTerm{spinor product}\index{spinor product}" is finally defined by:
	
	This matrix constitutes as we have already mentioned, to the limited development of the rotation matrix in the neighborhood of the identity matrix, the components of $\vec{x}$ being associated with a spinor whose rotation is through the double symmetry defined by two planes whose intersection is defined by the vector $\vec{X}$.
	
	We can also notice the interesting consequence that a rotation of $2\pi$ ($360^\circ$) rotation does not restore the object to its original position!!!
	
	Indeed:
	
	Therefore we need a rotation of $4\pi$ ($720^\circ$) to make a full turn! This corresponds to the spin of ${}^1{\mskip -5mu/\mskip -3mu}_2$. It takes two turns to find that the object reappears equivalently (this is counter intuitive and can make thing we are dealing with object having higher dimensions that what we exepect!). We then say that the representation of rotations is "bivaluated".
	
	Schematically this can be represented as:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/full_rotation.jpg}
		\caption{Spinor full rotation (special example)}
	\end{figure}
	Or you can consider the analogy that is to hold a tea cup on the palm of your hand and you turn your hand by maintaining it flat to regain its (the hand, not the cup!) original position. You will see that your hand has to do two laps!
	
	\pagebreak
	\subsubsection{Properties of Pauli Matrices}
	The reader can easily check (if this is not the case he can always contact us and we will write the details) the following main properties of the Pauli matrices, some of which will be used in the section of Relativistic Quantum Physics:
	\begin{enumerate}
		\item[P1.] Unitarity:
		

		\item[P2.] Anticommutativity:
		
		or $i\neq j$ and $i,j=1,2,3$.

		The last two properties gives us:
		
		with $i,j=1,2,3$.

		\item[P3.] Cyclicity:
		

		\item[P4.] Commutativity:
		
		
		\item[P5.] Vector product:
		
		Given the square of the different $\sigma$ by noting abusively by "$1$" the unitary matrix (we change the indices to give you the habit to use other common notations):
		
		Leading us to write that (square norm of the Pauli vector):
		
		Let us consider now the following products:
		
		Let us consider now the following products:
		
		All these relation can be summarized into a unique one (!):
		
		where for recall (\SeeChapter{see section Tensor Calculus page \pageref{kronecker symbol}}) the Kronecker symbol is defined by:
		
		and the antisymmetric symbol by:
		In three dimensions, the Levi-Civita symbol is defined as follows:
		
		i.e.  $\varepsilon_{ijk}$  is $1$ if $(i, j, k)$ is an even permutation of $(1,2,3)$ or in the natural order $(1,2,3)$, $-1$ if it is an odd permutation, and $0$ if any index is repeated. In three dimensions only, the cyclic permutations of $(1,2,3)$ are all even permutations, similarly the anticyclic permutations are all odd permutations. This means in 3D it is sufficient to take cyclic or anticyclic permutations of $(1,2,3)$ and easily obtain all the even or odd permutations.
		
		We also have:
		
		We fall back here on the components of the vector product:
		
		Now let us develop an important spinor identity which will be useful to us in the section of Relativistic Quantum Physics:
		
		But we also have:
		
		So finally:	
		
	
		\item[P6.] We note that these matrices are also hermitian (let us recall that a Hermitian matrix is a transposed matrix followed by its complex conjugate according to what we saw in the section of Linear Algebra) such that:
		
		It is therefore in the language of quantum physics: Hermitian operators!!!
	\end{enumerate}
	Let us now see what are the eigenvectors and eigenvalues of the Pauli matrices because this result is very useful for the section of Relativistic Quantum Physics and of Quantum Computing!

	Let us recall that when a transformation (application of a matrix) act on a vector, it changes the direction of the vector except for specific matrices that have eigenvalues. In this case, the direction is conserved but not their length. This property is exploited in quantum mechanics (and not only as we will see in many other sections of this book).
	
	Let us determine in a first time, the associated eigenvectors and eigenvalues (\SeeChapter{see section Linear Algebra page \pageref{eigenvector}}) using the most common method:
	
	The eigenvalues equation (\SeeChapter{see section Linear Algebra page \pageref{eigenvalue equations}}) is thus written:
	
	Which gives us as characteristic equation:
	
	hence the eigenvalues $\lambda=\pm 1$. Which gives us the possibility to determine the eigenvectors as following:
	
	Therefore for $\lambda=1$:
	
	This impose that $y=x$. The eigenvector is therefore:
	
	whatever is the value of $x$.
	
	Conclusion: The proper direction of the vector is conserved but not its length (norm) because it depends on the value of $x$.
	
	For $\lambda=-1$:
	
	This impose that $y=-x$ and therefore that the eigenvector is  equal to:
	
	The previous eigenvectors written with the Dirac formalism (\SeeChapter{see section  Relativistic Quantum Physics page \pageref{dirac formalism}}) give for $\lambda=1$:
	
	with a norm ($1$ since we formalize to the unit):
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In the formalism of Dirac $\langle v |$ is the is named a "Bra" and $|v \rangle$ a "Ket".
	\end{tcolorbox}
	This being only valid for components that are real numbers. The normalized eigenvector has therefore for expression:
	
	For $\lambda=-1$, we have:
	
	and:
	
	and the normalized eigenvector has thus for expression:
	
	Let us now determine the eigenvectors and eigenvalues associated to $\sigma_y$ by following the same procedure:
	
	So we have for the eigenvalues:
	
	The eigenvectors are determined as following:
	
	and therefore for $\lambda=1$:
	
	The eigenvector is therefore:
	and therefore for $\lambda=1$:
	
	The associated norm:
	
	The normalized vector is therefore expressed as:
	
	Let us now determine the eigenvectors and eigenvalues associated with $\sigma_z$ by doing the same again.
	
	We have therefore:
	
	The eigenvectors are then for $\lambda=1$:
	
	Which makes problem to us for say anything ... the only possibility is to choose $y=0$ and therefore:
	
	and the associated norm:
	
	The normalized vector has then for expression:
	
	and for $\lambda=-1$ we will have the same choice to do by choosing this time $x=0$ and therefore:
	
	hence the associated norm:
	
	The normalized eigenvector has then for expression:
	
	Therefore the normalized eigenvectors of $\sigma_z$ are on the directions of the Cartesian coordinate axes. It is for this particular reason that the eigenvectors of $\sigma_z$ are denoted in quantum computing by:
	
	and the reader should also know that we write also:
	
	
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{100} & \pbox{20cm}{\score{4}{5} \\ {\tiny 18 votes,  84.44\%}} 
	\end{tabular} 
	\end{flushright}